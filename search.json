[
  {
    "objectID": "chapters/interactingAPIs/request-methods.html",
    "href": "chapters/interactingAPIs/request-methods.html",
    "title": "Request Methods",
    "section": "",
    "text": "In our previous example, we sent a request to The Cat API using this endpoint:\nhttps://api.thecatapi.com/v1/images/search?size=small&mime_types=gif&limit=5\nIf you paste that URL in your browser, you’ll see a response similar to this:\nCode\nasync function getCatData() {\n  const url = `https://api.thecatapi.com/v1/images/search?size=small&mime_types=gif&limit=5`;\n  const response = await fetch(url);\n  if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);\n  const data = await response.json();\n  return data;\n}\n\ncatData = await getCatData();\n\nviewof catDataString = {\n  const pre = html`&lt;pre style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;\"&gt;`;\n  const code = html`&lt;code style=\"color: #333;\"&gt;`;\n  \n  // Formatted JSON with syntax highlighting\n  const formatted = JSON.stringify(catData, null, 2)\n  .replace(/&/g, '&amp;')\n  .replace(/&lt;/g, '&lt;')\n  .replace(/&gt;/g, '&gt;')\n  .replace(/\"([^\"]+)\":/g, '&lt;span style=\"color: #a31515;\"&gt;\"$1\"&lt;/span&gt;:') \n  .replace(/: \"([^\"]+)\"/g, ': &lt;span style=\"color: #008000;\"&gt;\"$1\"&lt;/span&gt;')  \n  .replace(/: ([0-9]+)/g, ': &lt;span style=\"color: #0000FF;\"&gt;$1&lt;/span&gt;')  \n  .replace(/\\b(true|false|null)\\b/g, '&lt;span style=\"color: #FF0000;\"&gt;$1&lt;/span&gt;');\n  \n  code.innerHTML = formatted;\n  pre.appendChild(code);\n  return pre;\n}\nEven though it feels like just clicking a link, what’s actually happening is that your browser (the client) sends a request to the server, with information like:\nIn everyday browsing, we don’t think about these request components because the browser handles them automatically. But when working directly with APIs we control these details ourselves, and that opens up a lot of power and flexibility.\nIn this section, we’re going to explore some common request methods that allow us to create or modify data on a server, not just read from it.",
    "crumbs": [
      "Interacting with APIs",
      "Request Methods"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/request-methods.html#request-methods",
    "href": "chapters/interactingAPIs/request-methods.html#request-methods",
    "title": "Request Methods",
    "section": "Request Methods",
    "text": "Request Methods\nA request method indicates the purpose of an HTTP request and the kind of response we expect. These methods act as semantic instructions for the server, which can respond with data, an error message, or a status update depending on whether the action is valid and allowed.\nThe most common HTTP methods are:\n\nGET\nAs we’ve already seen, the GET method is used to retrieve data from a server or resource. It doesn’t alter anything on the server, it just requests information.\nThe parameters included in the request URL help define the response. For example, a GET request can return a single item (e.g., images/{image_id}), or a list of items, as in the example we used at the beginning of this section.\n\n\n\nGET request\n\n\n\n\nPOST\nThe POST method is used to send data to a server. For example, when you fill out a form and click the “Submit” button, your browser typically sends a POST request containing that data. The server processes it—often creating a new resource or triggering an action.\nData for a POST request goes in the body (not the URL) and can be:\n\nform-data\n\nbinary (e.g., an image)\n\nraw text (e.g., JSON, XML)\n\nLater on, we’ll explore how to configure and send these request bodies using practical tools.\n\n\n\nPOST request\n\n\n\n\nPUT\nThe PUT method is used to update an existing resource. It looks similar to POST, but there’s an important difference: PUT requires the resource to already exist, and it typically replaces the entire object with the new data provided.\nIn other words, while POST is used to create something new, PUT is used to overwrite an existing record at a specific location (e.g., /images/{image_id}).\n\n\n\n\n\n\nIf you don’t include all fields in a PUT request, some servers may interpret that as a request to remove the missing fields.\n\n\n\n\n\n\nPUT request\n\n\n\n\nPATCH\nThe PATCH method can be understood as a way to edit an existing record. Unlike PUT, which typically replaces the entire resource, PATCH allows you to update only specific fields.\nThe values to be updated are included in the body of the request and will only overwrite the specified elements, leaving the rest of the record unchanged.\nFor example, sending the following body to /images/{image_id}:\n{\n  \"pending\": 1,\n  \"approved\": 0\n}\nwill update only those two fields in the existing record.\n\n\n\n\n\n\nIn many cases, using PATCH is safer than PUT—especially when we want to avoid unintentionally removing fields that weren’t included in the update. For that reason, PATCH is more commonly supported than PUT in modern APIs.\n\n\n\n\n\n\nPATCH request\n\n\n\n\nDELETE\nThe DELETE method is fairly self-explanatory. It takes a specific record identifier and removes that resource from the server.\nTypically, the response is a success status code (such as 200 OK or 204 No Content) confirming that the item has been deleted.\nBecause of its potential impact, the use of this method is usually restricted to authenticated users and requires proper authorization or ownership. It’s rarely, if ever, available to anonymous users.\n\n\n\nDELETE request\n\n\n\n\nHTTP Request Methods at a Glance\nWe can visualize the main difference between these methods using the following table.\n\n\n\n\n\n\n\n\n\n\n\nMethod\nPurpose\nRequires Existing Resource?\nTypical Use Case\nData Sent In\nCommon Response\n\n\n\n\nGET\nRetrieve data\n❌ No\nFetching a list or a single item\nURL (query params)\n200 OK, JSON data\n\n\nPOST\nCreate a new resource\n❌ No\nSubmitting a form, creating an object\nBody\n201 Created\n\n\nPUT\nReplace an existing resource\n✅ Yes\nOverwriting an entire object\nBody\n200 OK or 204 No Content\n\n\nPATCH\nUpdate specific fields\n✅ Yes\nEditing part of an existing record\nBody\n200 OK or 204 No Content\n\n\nDELETE\nRemove a resource\n✅ Yes\nDeleting a specific item\nURL\n204 No Content",
    "crumbs": [
      "Interacting with APIs",
      "Request Methods"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/request-methods.html#wrapping-up",
    "href": "chapters/interactingAPIs/request-methods.html#wrapping-up",
    "title": "Request Methods",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nIn this section, we’ve explored the five most common HTTP request methods and how each one defines a different type of interaction with a server.\nThese request methods form the foundation of working with RESTful APIs, and you’ll use them frequently as you explore or build your own data workflows.\nIn the next section, you’ll see how to interact with APIs using the GET method directly from this notebook.",
    "crumbs": [
      "Interacting with APIs",
      "Request Methods"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/exercise-calling-data.html",
    "href": "chapters/interactingAPIs/exercise-calling-data.html",
    "title": "Exercise: Calling Data from an API",
    "section": "",
    "text": "A query is basically a question, so, let’s try to answer this question: How many ‘knights’ are in the department of Medieval Art of the Metropolitan Museum of Art? To answer that question, we have to segment it into multiple parts:\n\nWhat is required to search within a specific department in the API?\nWhat endpoint do we need to use?\nWhat parameters do we need to use?\nWhat’s the previous information we need to retrieve before doing the query?\nWhat is the query we need to use?\nHow do we get the specific information we need?\n\nSeems like a lot of steps, but it’s not too complex as it might seem. Let’s start with the first step: looking at the API documentation: Metropolitan Museum of Art Collection API\nBecause the first question is about searching within a specific scope, let’s start with the search endpoint. As we can see, there is a list of paremeters, the first one is q, which is the one that helps us to search by term (like ‘knight’); and scrolling down a little, we can see the departmentId parameter, which requires a numeric (integer) identifier corresponding to the department we want to search within:\n\n\n\n\n\n\n\n\nParameter\nFormat\nNotes\n\n\n\n\nq\nSearch term e.g. sunflowers\nReturns a listing of all Object IDs for objects that contain the search query within the object’s data\n\n\ndepartmentId\nInteger\nReturns objects that are a part of a specific department. For a list of departments and department IDs, refer to our /department endpoint: https://collectionapi.metmuseum.org/public/collection/v1/departments \n\n\n\nAs you can see, documentation is the best starting point to interact with an API. With that information, we have answered not only the first question, but also what endpoint, parameters, and previous information we need to retrieve before doing the query.\nBefore proceeding with the next question, let’s get the list of departments and their IDs to select the one corresponding to the ‘Medieval Art’ department. To do that, just paste the ‘/department’ endpoint as is pointed out in the documentation (if you get lost, follow the hints ;) ).\nScroll down to see the list of departments and their departmentId. Copy the value of the departmentId for the ‘Medieval Art’ department (the number, not the name). We’ve underlined the value in the list for you.\n\n\nCode\nviewof departmentListEndpoint = Inputs.text({\n  label: \"Department list\",\n  placeholder: \"\",\n  value: \"\",\n  attributes: {\n    class: \"form-control mb-3\"\n  }\n})\n\n/**\n * The purpose of this function is to validate the endpoint\n * for students. Have this in mind if want to replicate the\n * code in other exercises.\n */\nasync function validateEndpoint(endpoint, query, params) {\n  const checks = [\n    {\n    pattern: /^https:\\/\\//,\n    message: \"URL must start with 'https://'\"\n    },\n    {\n    pattern: /collectionapi\\.metmuseum\\.org/,\n    message: \"URL must contain 'collectionapi.metmuseum.org'\"\n    },\n    {\n    pattern: /\\/public\\//,\n    message: \"Missing '/public/' in the path\"\n    },\n    {\n    pattern: /\\/collection\\//,\n    message: \"Missing '/collection/' in the path\"\n    },\n    {\n    pattern: /\\/v1\\//,\n    message: \"Missing '/v1/' in the path\"\n    }\n  ];\n\n  if (params) {\n    const paramsList = params.split(\"&\");\n    checks.push(\n    {\n      pattern: new RegExp(`${query}\\\\?`),\n      message: `URL must contain '${query}' before parameters`\n    },\n    {\n      pattern: /[?]/g,\n      message: \"Query and parameters must be separated by '?'\"\n    },\n    ...paramsList.map(param =&gt; ({\n      pattern: new RegExp(param),\n      message: `Missing parameter: ${param}`\n    }))\n    );\n  } else {\n    checks.push(\n    {\n      pattern: new RegExp(`${query}$`),\n      message: `URL must end with '${query}'`\n    }\n    )\n  }\n\n  if (/[^:]\\/\\//.test(endpoint)) {\n    return \"Invalid URL: Contains double slashes (//)\";\n  }\n\n  const segments = ['collectionapi.metmuseum.org', 'public', 'collection', 'v1', query];\n  for (let i = 0; i &lt; segments.length - 1; i++) {\n    const pattern = new RegExp(`${segments[i]}[^/]+${segments[i + 1]}`);\n    if (pattern.test(endpoint)) {\n    return `Missing slash between '${segments[i]}' and '${segments[i + 1]}'`;\n    }\n  }\n\n  const basePattern = new RegExp(`^https:\\/\\/collectionapi\\.metmuseum\\.org\\/public\\/collection\\/v1\\/${query}`);\n  if (!basePattern.test(endpoint.split('?')[0])) {\n    return `Base URL is incorrect. Should start with: https://collectionapi.metmuseum.org/public/collection/v1/${query}`;\n  }\n\n  for (const check of checks) {\n    if (!check.pattern.test(endpoint)) {\n    return check.message;\n    }\n  }\n\n  return null;\n}\n\nasync function fetchDepartmentList(endpoint) {\n  try {\n    const response = await fetch(endpoint);\n    const status = {\n    code: response.status,\n    ok: response.ok,\n    text: response.statusText\n    };\n\n    if (!response.ok) {\n    throw new Error(`HTTP error! status: ${response.status}`);\n    }\n\n    const data = await response.json();\n    return { data, status};\n  } catch (error) {\n    return {\n    data: { \"Message\": `Error: ${error.message}` },\n    status: {\n      code: 400,\n      ok: false,\n      text: \"Bad Request\"\n    }\n    };\n  }\n}\n\ndepartmentList = {\n  if (departmentListEndpoint) {\n    const validation = await validateEndpoint(departmentListEndpoint, \"departments\", \"\");\n    if (validation) {\n    return {\n      data: { \"Message\": validation },\n      status: {\n        code: 400,\n        ok: false,\n        text: \"Bad Request\"\n      }\n    };\n    }\n    const result = await fetchDepartmentList(departmentListEndpoint);\n    return result;\n  } else {\n    return {\n    data: { \"Message\": \"You need to enter a valid endpoint.\" },\n    status: {\n      code: 400,\n      ok: false,\n      text: \"Bad Request\"\n    }\n    };\n  }\n}\n\nprettyDepartmentList = {\n  let jsonString;\n  if (typeof departmentList === 'string') {\n    jsonString = JSON.stringify(JSON.parse(departmentList), null, 2);\n  } else {\n    jsonString = JSON.stringify(departmentList, null, 2);\n  }\n  \n  // Highlight the entire Medieval Art object\n  return jsonString.replace(\n    /(\\{[^\\}]*\"displayName\":\\s*\"Medieval Art\"[^\\}]*\\})/g, \n    '&lt;span style=\"background-color: #fff3cd; display: inline-block; width: 100%;\"&gt;$1&lt;/span&gt;'\n  );\n}  \n\nviewof prettyDepartmentListContainer = {\n  let content;\n  if (departmentList.data.Message) {\n    content = html`&lt;div class=\"alert alert-warning m-0\"&gt;${departmentList.data.Message}&lt;/div&gt;`;\n  } else {\n    content = html`&lt;pre class=\"card-body m-0\" style=\"background-color: #f8f9fa; max-height: 400px; overflow-y: auto;\"&gt;${prettyDepartmentList}&lt;/pre&gt;`;\n  }\n  \n  const badgeClass = departmentList.status.ok ? \"bg-success\" : \"bg-danger\";\n  \n  const container = html`&lt;div class=\"card\"&gt;\n    &lt;div class=\"card-header d-flex justify-content-between align-items-center\"&gt;\n    &lt;span&gt;Department list&lt;/span&gt;\n    &lt;span class=\"badge ${badgeClass}\"&gt;${departmentList.status.code} ${departmentList.status.text}&lt;/span&gt;\n    &lt;/div&gt;\n    ${content}\n  &lt;/div&gt;`;\n  return container;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe next step will consist of answering What is the query we need to use to search for ‘knights’ in the department of Medieval Art?\nAs we’ve seen in the documentation, we will require two parameters: q and departmentId. The first one is the search term ‘knight’, and the second one is the departmentId we got from the previous step.\nTherefore, write here the query that will retrive all the objects that contain the term ‘knight’ in the department of Medieval Art.\n\n\nCode\nviewof knightsQuery = Inputs.text({\n  label: \"Query\",\n  placeholder: \"\",\n  value: \"\",\n  attributes: {\n    class: \"form-control mb-3\"\n  }\n});\n\nasync function fetchQuery(endpoint) {\n  try {\n    const response = await fetch(endpoint);\n    const status = {\n    code: response.status,\n    ok: response.ok,\n    text: response.statusText\n    };\n\n    if (!response.ok) {\n    throw new Error(`HTTP error! status: ${response.status}`);\n    }\n\n    const data = await response.json();\n    return { data, status };\n  } catch (error) {\n    return {\n    data: { \"Message\": `Error: ${error.message}` },\n    status: {\n      code: 400,\n      ok: false,\n      text: \"Bad Request\"\n    }\n    };\n  }\n}\n\nqueryResult = {\n  if (knightsQuery) {\n    const validation = await validateEndpoint(knightsQuery, \"search\", \"q=knight&departmentId=17\");\n    if (validation) {\n    return {\n      data: { \"Message\": validation },\n      status: {\n        code: 400,\n        ok: false,\n        text: \"Bad Request\"\n      }\n    };\n    }\n    const result = await fetchQuery(knightsQuery);\n    return result;\n  } else {\n    return {\n    data: { \"Message\": \"You need to enter a valid endpoint.\" },\n    status: {\n      code: 400,\n      ok: false,\n      text: \"Bad Request\"\n    }\n    };\n  }\n}\n\nprettyQueryResult = {\n  let jsonString;\n  if (typeof queryResult === 'string') {\n    jsonString = JSON.stringify(JSON.parse(queryResult), null, 2);\n  } else {\n    jsonString = JSON.stringify(queryResult, null, 2);\n  }\n  return jsonString;\n}\n\nviewof prettyQueryResultContainer = {\n  let content;\n  if (queryResult.data.Message) {\n    content = html`&lt;div class=\"alert alert-warning m-0\"&gt;${queryResult.data.Message}&lt;/div&gt;`;\n  } else {\n    content = html`&lt;pre class=\"card-body m-0\" style=\"background-color: #f8f9fa; max-height: 400px; overflow-y: auto;\"&gt;${prettyQueryResult}&lt;/pre&gt;`;\n  }\n  \n  const badgeClass = queryResult.status.ok ? \"bg-success\" : \"bg-danger\";\n  \n  const container = html`&lt;div class=\"card\"&gt;\n    &lt;div class=\"card-header d-flex justify-content-between align-items-center\"&gt;\n    &lt;span&gt;Query result&lt;/span&gt;\n    &lt;span class=\"badge ${badgeClass}\"&gt;${queryResult.status.code} ${queryResult.status.text}&lt;/span&gt;\n    &lt;/div&gt;\n    ${content}\n  &lt;/div&gt;`;\n  return container;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHaving the query result, the only thing left to do is to get the specific information we need. In this case, you can see that the answer is in the same response under the key total. Just to validate that we have the same answer, write in the next cell the total number of objects that contain the term ‘knight’ in the department of Medieval Art.\n\n\nCode\nviewof knightsTotal = Inputs.number({\n  label: \"Total number of knights\",\n  placeholder: \"\",\n  value: 0,\n  attributes: {\n    class: \"form-control mb-3\"\n  }\n});\n\nviewof validationResult = {\n  const container = html`&lt;div&gt;&lt;/div&gt;`;\n  \n  if (!queryResult.status.ok || queryResult.data.Message) {\n    if (knightsTotal !== 0) {  // Only show warning if user has entered a number\n    container.innerHTML = `\n      &lt;div class=\"alert alert-warning\"&gt;\n        Please enter a valid query first before submitting your answer!\n      &lt;/div&gt;`;\n    }\n  } else if (queryResult.data.total !== undefined) {\n    if (knightsTotal === queryResult.data.total) {\n    container.innerHTML = `\n      &lt;div class=\"alert alert-success\"&gt;\n        Correct! There are ${queryResult.data.total} objects that contain the term 'knight' in the Medieval Art department.\n      &lt;/div&gt;`;\n    } else {\n    container.innerHTML = `\n      &lt;div class=\"alert alert-danger\"&gt;\n        That's not correct. Try again!\n      &lt;/div&gt;`;\n    }\n  }\n  \n  return container;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGreat! Now you know the basics stepts to get data from an API. In the next chapter, we will show you how to get data that can be tabulated and analyzed.",
    "crumbs": [
      "Interacting with APIs",
      "Exercise: Calling Data"
    ]
  },
  {
    "objectID": "chapters/curl/curl.html",
    "href": "chapters/curl/curl.html",
    "title": "cURL - APIs from the command line",
    "section": "",
    "text": "In the previous chapters we have seen how to interact with APIs directly from the web browser, and by the help of software testing applications like Postman. However, there are more direct ways to interact with APIs. One of the most common is by using the command line interface (CLI) of your computer.\ncURL (Client URL) is a powerful, Open Source, command line tool used for transfering data to and from servers using multiple protocols, including HTTP, HTTPS, FTP, and more. It is widely used for testing APIs, for automating tasks, and for debugging network issues. In its origins, cURL was developed for Unix-based systems, but its popularity has led to being packaged for Windows and macOS as well."
  },
  {
    "objectID": "chapters/coding/python-query-analysis.html",
    "href": "chapters/coding/python-query-analysis.html",
    "title": "Query and Analyze API Data with Python",
    "section": "",
    "text": "All right, enough setup! Now we can start making requests and exploring the data.",
    "crumbs": [
      "Getting Data Programmatically",
      "Queries and Analysis"
    ]
  },
  {
    "objectID": "chapters/coding/python-query-analysis.html#the-shape-of-data",
    "href": "chapters/coding/python-query-analysis.html#the-shape-of-data",
    "title": "Query and Analyze API Data with Python",
    "section": "The shape of data",
    "text": "The shape of data\nThe first thing we want to do is get a sense of the shape of the data we’re getting from the API. One interesting question is: how are items distributed across time? There are multiple ways to explore this, but perhaps the most straightforward is to use facets to get the distribution of items by year.\nAdditionally, we’ll filter the results to only include titles, descriptions, and dates. This makes the data easier to work with and reduces the amount we need to process.\n\n# Defining the fields, facets, and filters outside the function call for better readability\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\"\n}\n\ntry:\n    ai_search = search_items(\n        \"artificial AND intelligence\", # search query\n        fields=_join_list(fields, sep=\",\"), # fields to include in the response\n        facets=\"sourceResource.date.begin\", # facets to include in the response\n        **dotted_fields, # additional parameters (e.g. filters),\n        page_size=5, # items per page\n        sort_by=\"sourceResource.date.begin\", # sort by date\n        sort_order=\"asc\", # oldest to newest\n        page=1, # page number to retrieve\n        verbose=True # print the request URL for debugging purposes\n        )\n    \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n    r = httpx.get(f\"{FALLBACK_DATA_URL}dpla_search_results.json\")\n    r.raise_for_status()\n    ai_search = r.json()\n\n\n# download the preloaded data for the next steps\nprint(f\"{ai_search.get('count')} results found.\") if isinstance(ai_search, dict) else print(f\"{len(ai_search)} results found.\")\n\nRequest URL [redacted]: https://api.dp.la/v2/items?q=artificial+AND+intelligence&fields=sourceResource.title%2CsourceResource.description%2CsourceResource.date.begin%2CsourceResource.date.end&facets=sourceResource.date.begin&sourceResource.subject.name=artificial+intelligence&page_size=5&sort_by=sourceResource.date.begin&sort_order=asc&page=1\n728 results found.\n\n\nIf the request was successful, you should see a redacted version of the request URL and the number of results found. If there was an error (e.g. invalid API key, network error, etc.), you should see an error message and the preloaded data will be used instead.\n\n\n\n\n\n\nNote how the result count is significantly higher than the result from the web interface. The exact reason isn’t fully documented, but one possibility is that the web interface and API handle field matching differently (perhaps with different case sensitivity or tokenization). This is a good example of how different interfaces to the same data can yield different results, and why it’s important to understand the underlying data and how it’s indexed.\n\n\n\n\nLet’s play with the facets!\nSomething that stands out in the response is the facet counts. Facets can be understood as a way to get aggregated counts of items based on specific fields. In this case, we’re getting the count of items by year (based on the sourceResource.date.begin field).\nTo explore the facet counts, we can extract the facet information from the response and plot it using a bar chart. This will let us visualize the distribution of items across time.\nThe first thing we need to do is reach the facet information nested in the response. To know how, we need to explore the documentation and understand the object structure.\nAccording to the documentation, the way to reach our facet information is following this structure:\nai_search\n├── facets (dict)\n│   └── sourceResource.date.begin (dict)\n│       └── entries (list)\n│           ├── count (int)\n│           └── label (str)\nTherefore, if we want to get the entries of the sourceResource.date.begin facet, we can do it with the following code:\n\nfacets_entries = ai_search.get(\"facets\", {}).get(\"sourceResource.date.begin\", {}).get(\"entries\", [])\n\n# Print a sample (we use 'time' because that's the label for date facets)\nfor entry in facets_entries[:5]:\n    print(f\"Year: {entry.get('time')}, Count: {entry.get('count')}\")\n\nYear: 2026, Count: 4\nYear: 2025, Count: 119\nYear: 2024, Count: 95\nYear: 2023, Count: 77\nYear: 2022, Count: 23\n\n\nNow that we understand better the structure of the facet information, we can visualize it using a bar chart. We will use the seaborn library to create the bar chart, but you can use any other library you prefer (e.g. matplotlib, plotly, etc.).\n\n# Extract the year and count information from the facet entries\nyears = [entry.get(\"time\") for entry in facets_entries][::-1] # We use [::-1] to reverse the order\ncounts = [entry.get(\"count\") for entry in facets_entries][::-1]\n\n# Create a bar chart using seaborn\nsns.barplot(x=years, y=counts)\nplt.xlabel('Year')\nplt.ylabel('Number of Items')\nplt.title('Items about Artificial Intelligence by Year')\nplt.xticks(range(0, len(years), 5), [years[i] for i in range(0, len(years), 5)], rotation=45)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis graph helps us understand the distribution of items about artificial intelligence across different years. We can see that between 2018 and 2022 there is a peak in the number of items, and a significant increase after 2023. Before that, the number of items per year is relatively low (fewer than 10 per year).",
    "crumbs": [
      "Getting Data Programmatically",
      "Queries and Analysis"
    ]
  },
  {
    "objectID": "chapters/coding/python-query-analysis.html#further-exploration",
    "href": "chapters/coding/python-query-analysis.html#further-exploration",
    "title": "Query and Analyze API Data with Python",
    "section": "Further exploration",
    "text": "Further exploration\nWith this information, we can establish three periods of interest: before 2018, between 2018 and 2022, and after 2023. We’ll set up a list with these periods and use it to filter items for further analysis:\n\nperiods = [\n    (\"preCovid\", 1844, 2018),\n    (\"Covid\", 2019, 2021),\n    (\"postCovid\", 2022, 2026),\n]\n\nLet’s create a pool of items for each period using the search_all_items function we defined earlier. We’ll use the sourceResource.date.begin field to filter the items by year:\n\nai_results = {}\n\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\",\n}\n\ntry:\n    for period_name, start_date, end_date in tqdm(periods):\n        ai_results[period_name] = search_all_items(\n            \"artificial AND intelligence\", # search query\n            max_items=400, # maximum number of items to retrieve for each period\n            fields=_join_list(fields, sep=\",\"), # fields to include in the response\n            facets=\"sourceResource.date.begin\", # Retrieve facets for date ranges\n            page_size=100, # items per page\n            **dotted_fields, # filter to ensure results are about AI, not just using AI in metadata\n            **{\"sourceResource.date.after\": str(start_date)}, # Between year\n            **{\"sourceResource.date.before\": str(end_date)}, # and Year\n            sort_by=\"sourceResource.date.begin\", # sort by date\n            sort_order=\"asc\", # oldest to newest\n            verbose=False \n        )\n        \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n    r = httpx.get(f\"{FALLBACK_DATA_URL}ai_results_by_wave.json\")\n    r.raise_for_status()\n    ai_results = r.json()\n\n  0%|          | 0/3 [00:00&lt;?, ?it/s] 33%|███▎      | 1/3 [00:01&lt;00:02,  1.09s/it] 67%|██████▋   | 2/3 [00:02&lt;00:01,  1.07s/it]100%|██████████| 3/3 [00:05&lt;00:00,  2.15s/it]100%|██████████| 3/3 [00:05&lt;00:00,  1.86s/it]\n\n\nThis code will create a dictionary called ai_results where the keys are the period names (e.g. “preCovid”, “Covid”, “postCovid”) and the values are lists of items that match the search query and date filters for each period. Let’s print a summary of the results to see how many items we got for each period:\n\nai_results_summary = {period: len(items) for period, items in ai_results.items()}\nprint(\"AI Results Summary by Period:\")\nfor period, count in ai_results_summary.items():\n    print(f\"{period}: {count} items\")\n\nAI Results Summary by Period:\npreCovid: 89 items\nCovid: 80 items\npostCovid: 318 items\n\n\nWe can see a significant increase in the number of items in the “postCovid” period compared to the earlier periods. This reflects the AI boom that has been happening in recent years—which was likely accelerated (though not solely caused) by the COVID-19 pandemic.",
    "crumbs": [
      "Getting Data Programmatically",
      "Queries and Analysis"
    ]
  },
  {
    "objectID": "chapters/coding/python-query-analysis.html#extracting-keywords-with-yake",
    "href": "chapters/coding/python-query-analysis.html#extracting-keywords-with-yake",
    "title": "Query and Analyze API Data with Python",
    "section": "Extracting keywords with YAKE",
    "text": "Extracting keywords with YAKE\nOur last step is to extract keywords from the titles and descriptions of items in each period using the YAKE library. YAKE (Yet Another Keyword Extractor) is a simple and effective unsupervised keyword extraction method.\nBut first, let’s explore the structure of the sourceResource field in our items:\n\n# Get a sample item to explore the structure of the sourceResource field\nai_results.get(\"postCovid\")[:5]\n\n[{'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['In scope of the U.S. Government Publishing Office Cataloging and Indexing Program (C&I) and Federal Depository Library Program (FDLP).',\n   'Description based on online resource; title from publication (Agency website, viewed September 14, 2022).'],\n  'sourceResource.title': 'Artificial intelligence strategic plan: fiscal years 2023-2027, draft report for comment'},\n {'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['\"Published Fall 2022.\"',\n   '\"Ann Caracristi Institute for Intelligence Research\"--Cover.',\n   'In scope of the U.S. Government Publishing Office Cataloging and Indexing Program (C&I) and Federal Depository Library Program (FDLP).',\n   'Includes bibliographical references (pages 69-72).',\n   'Description based on online resource; title from PDF title page (NIU, viewed July 18, 2023).'],\n  'sourceResource.title': 'Perceptions of artificial intelligence / machine learning in the intelligence community : a systematic review of the literature'},\n {'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['In scope of the U.S. Government Publishing Office Cataloging and Indexing Program (C&I) and Federal Depository Library Program (FDLP).',\n   '\"FHWA-HRT-21-081.\"',\n   'Description based on online resource; title from publication (Agency website, viewed December 22, 2023).'],\n  'sourceResource.title': 'Effectiveness of TMC AI applications in case studies'},\n {'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['Access ID (govinfo): CHRG-117hhrg46195.',\n   '\"Serial no. 117-52.\"',\n   'Includes bibliographical references.',\n   'Hearing witnesses: Broussard, Meredith, Associate Professor, Arthur L. Carter Journalism Institute of New York University; Cooper, Aaron, Vice President, Global Policy, BSA--The Software Alliance; King, Meg, Director, Science and Technology Innovation Program, The Wilson Center; Vogel, Miriam, President and CEO, EqualAI; Yong, Jeffery, Principal Advisor, Financial Stability Institute, Bank for International Settlements.',\n   'Date of hearing: 2021-10-13.',\n   'Description based on online resource; title from PDF title page (govinfo, viewed Jan. 19, 2022).'],\n  'sourceResource.title': 'Beyond I, robot : ethics, artificial intelligence, and the digital age : viritual hearing before the Task Force on Artificial Intelligence of the Committee on Financial Services, U.S. House of Representatives, One Hundred Seventeenth Congress, first session, October 13, 2021'},\n {'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['Access ID (govinfo): CHRG-117hhrg47880.',\n   'Hearing witnesses: Greenfield, Kevin, Deputy Comptroller for Operational Risk Policy, Office of the Comptroller of the Currency (OCC); Hall, Melanie, Commissioner, Division of Banking and Financial Institutions, State of Montana; and Chair, Board of Directors, Conference of State Bank Supervisors (CSBS); Lay, Kelly J., Director, Office of Examination and Insurance, National Credit Union Administration (NCUA); Rusu, Jessica, Chief Data, Information and Intelligence Officer, Financial Conduct Authority (FCA), United Kingdom.',\n   'Date of hearing: 2022-05-13.',\n   'Includes bibliographical references.',\n   '\"Serial no. 117-85.\"',\n   'Description based on online resource; title from PDF title page (GovInfo, viewed Aug. 2, 2022).'],\n  'sourceResource.title': 'Keeping up with the codes : using AI for effective RegTech : hybrid hearing before the Task Force on Artificial Intelligence of the Committee on Financial Services, U.S. House of Representatives, One Hundred Seventeenth Congress, second session, May 13, 2022'}]\n\n\nGreat! Each item is a dictionary with the following keys:\n\nsourceResource.date.begin (str): The start date of the item (e.g. “2015-01-01”)\nsourceResource.date.end (str): The end date of the item (e.g. “2015-12-31”)\nsourceResource.description (str | list): A description of the item\nsourceResource.title (str | list): The title of the item\n\nNow we can use YAKE to extract keywords from the titles of the items in each period. We’ll create a function that takes a list of items and returns a dictionary with keyword counts for each period:\n\ndef extract_keywords(items, skip=None, ngram=2, max_keywords=5, language=\"en\"):\n    \"\"\"Extract keywords from a list of items using YAKE.\"\"\"\n\n    ai_keywords = {}\n\n    kw_extractor = yake.KeywordExtractor(lan=language, n=ngram, top=max_keywords)\n\n    if skip:\n        skip_keywords = set(skip)\n\n    for period, items in tqdm(items.items(), desc=\"Extracting keywords\"):\n        period_keywords = {}\n        for item in items:\n            text = _join_list(item.get(\"sourceResource.title\", \"\")).lower()\n\n            keywords = kw_extractor.extract_keywords(text)\n            for kw, score in keywords:\n                if skip and kw in skip_keywords:\n                    continue\n                period_keywords[kw] = period_keywords.get(kw, 0) + 1\n                \n        ai_keywords[period] = period_keywords\n    \n    return ai_keywords\n\nNow, we can iterate over the loaded items and extract the keywords for each period:\n\n# Define a list of common words to skip (optional)\nskip_words = [\"artificial intelligence\", \"ai\", \"intelligence\", \"artificial\", \"bibliographical references\", \"includes bibliographical\", \"online resource\", \"bibliography\", \"references\", \"bibliographical\", \"reference\"]\n\ntop = 10\n\nai_keywords = extract_keywords(ai_results, skip=skip_words, ngram=2, max_keywords=top)\n\nfor period, keywords in ai_keywords.items():\n    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:top]\n    print(f\"Top {top} keywords for {period}:\")\n    for kw, count in sorted_keywords:\n        print(f\"  {kw}: {count}\")\n    print()\n\nExtracting keywords:   0%|          | 0/3 [00:00&lt;?, ?it/s]Extracting keywords: 100%|██████████| 3/3 [00:00&lt;00:00, 10.40it/s]Extracting keywords: 100%|██████████| 3/3 [00:00&lt;00:00, 10.38it/s]\n\n\nTop 10 keywords for preCovid:\n  machine: 9\n  learning: 8\n  systems: 7\n  system: 6\n  proceedings: 6\n  machine intelligence: 5\n  report: 5\n  robotics: 5\n  space: 5\n  computer: 4\n\nTop 10 keywords for Covid:\n  sixteenth congress: 14\n  hundred sixteenth: 13\n  learning: 9\n  report: 8\n  financial services: 8\n  task force: 8\n  national security: 6\n  national: 6\n  security: 5\n  united states: 5\n\nTop 10 keywords for postCovid:\n  eighteenth congress: 59\n  united states: 57\n  hundred eighteenth: 54\n  states senate: 52\n  hearing: 36\n  learning: 26\n  machine learning: 22\n  report: 19\n  nineteenth congress: 19\n  hundred nineteenth: 19\n\n\n\n\n\n\nFinally, we can visualize the top keywords for each period using a bar chart:\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 5), sharex=False)\n\nfor ax, (period_name, start_year, end_year) in zip(axes, periods):\n    data = _top_n(ai_keywords.get(period_name, {}), 10)\n    terms = list(data.keys())\n    counts = list(data.values())\n\n    sns.barplot(x=counts, y=terms, ax=ax, palette=\"viridis\", hue=counts)\n    ax.set_title(f\"{period_name} ({start_year}–{end_year})\")\n    ax.set_xlabel(\"Frequency\")\n\nplt.suptitle(\"How 'Artificial Intelligence' appears across time in DPLA\", fontsize=14)\nplt.tight_layout()\nplt.show()",
    "crumbs": [
      "Getting Data Programmatically",
      "Queries and Analysis"
    ]
  },
  {
    "objectID": "chapters/coding/python-query-analysis.html#wrapping-up",
    "href": "chapters/coding/python-query-analysis.html#wrapping-up",
    "title": "Query and Analyze API Data with Python",
    "section": "Wrapping up",
    "text": "Wrapping up\nAnd that’s it! You’ve successfully queried the DPLA API, filtered and paginated through results, visualized temporal distributions using facets, and extracted keywords from metadata across different time periods. Along the way, you’ve seen how APIs let you access much richer datasets than web interfaces alone—and how a little code can unlock powerful ways to explore and analyze cultural heritage collections.\nThe techniques you’ve learned here—building requests programmatically, handling pagination, working with nested JSON structures, and combining API data with text analysis—are transferable to many other APIs. Whether you’re exploring museum collections, scientific datasets, or social media archives, the same patterns apply: understand the documentation, experiment with parameters, and iterate on your queries.\nKeep exploring, and remember: every API is a doorway to data that’s waiting to be discovered.",
    "crumbs": [
      "Getting Data Programmatically",
      "Queries and Analysis"
    ]
  },
  {
    "objectID": "chapters/coding/python-functions.html",
    "href": "chapters/coding/python-functions.html",
    "title": "Helping Functions",
    "section": "",
    "text": "One of the most useful features of programming languages is the ability to work with functions: reusable blocks of code that perform a specific task. In Python, functions are defined using the def keyword. They can take inputs (arguments) and return outputs (values).\nFor this exercise, we’ll write a few small helper functions (to avoid repeating ourselves) and two main functions that query the DPLA API and return responses in a clean, “Pythonic” format.",
    "crumbs": [
      "Getting Data Programmatically",
      "Helping Functions"
    ]
  },
  {
    "objectID": "chapters/coding/python-functions.html#helper-functions",
    "href": "chapters/coding/python-functions.html#helper-functions",
    "title": "Helping Functions",
    "section": "Helper functions",
    "text": "Helper functions\nThese helper functions aren’t strictly necessary, but they keep the rest of the notebook cleaner. You can copy/paste them into your notebook and come back to the details later:\n\ndef _join_list(x, sep=\"; \", keep_first_only=False):\n    \"\"\"\n    Helper function to join a list of values into a single string. If the input is not a list, it will return the string representation of the input. If the input is None, it will return an empty string.\n    \"\"\"\n    if isinstance(x, list):\n        if keep_first_only and len(x) &gt; 0:\n            return str(x[0])\n        return sep.join(str(v) for v in x if v is not None)\n    return \"\" if x is None else str(x)\n\ndef _top_n(d, n=10):\n    \"\"\"Helper function to return the top n items from a dictionary, sorted by value in descending order.\"\"\"\n    return dict(sorted(d.items(), key=lambda x: x[1], reverse=True)[:n])\n\ndef _redact_request_url(url):\n    \"\"\"Remove the api_key parameter from the URL for display purposes.\"\"\"\n    parsed_url = urlsplit(str(url))  # Convert httpx.URL to string\n    query_params = parse_qsl(parsed_url.query)\n    filtered_params = [(name, value) for name, value in query_params if name != \"api_key\"]\n    redacted_query = urlencode(filtered_params)\n    redacted_url = parsed_url._replace(query=redacted_query)\n    return urlunsplit(redacted_url)",
    "crumbs": [
      "Getting Data Programmatically",
      "Helping Functions"
    ]
  },
  {
    "objectID": "chapters/coding/python-functions.html#main-functions",
    "href": "chapters/coding/python-functions.html#main-functions",
    "title": "Helping Functions",
    "section": "Main functions",
    "text": "Main functions\nLet’s take some time to understand our first main function. The goal is simple: make a DPLA request and get back a JSON response as a Python dictionary.\nIf we take a look at the DPLA API documentation, we can see that a request is built from a base URL (https://api.dp.la/v2), a resource type (items or collections), query parameters (e.g. q=artificial+intelligence), and your API key.\nAdditionally, you can request for specific facets (e.g. facets=sourceResource.type), specific fields to be returned (e.g. fields=sourceResource.title,sourceResource.description), and you can also specify the page number (e.g. page=2) and the number of results per page (e.g. page_size=20).\nIf we tried to handle all of that ad hoc every time, our notebook would get long and repetitive fast. Instead, we’ll write one function that builds the request and sends it for us.\nOur function will be called search_items. It takes a query, a resource_type (usually items), and any additional DPLA parameters via **parameters.\nOur resulting function will return a dictionary from the JSON response.\n\ndef search_items(query, resource_type='items', verbose=False, timeout=30.0, **parameters):\n    \"\"\"\n    Search DPLA items with given query and parameters.\n    \n    Args:\n        query (str): The search query string. It's possible to use logical operators (AND, OR, NOT). Additionally, you can use wildcards (*) for partial matches.\n        resource_type (str): The type of resource to search for. Default is 'items'.\n        verbose (bool): If True, prints the request URL. Default is False.\n        timeout (float): The timeout for the HTTP request in seconds. Default is 30.0.\n        **parameters: Facets and filter parameters from the DPLA API documentation: https://pro.dp.la/developers/requests\n                    Dotted keywords and values can be passed using dictionary unpacking. For example, to filter by sourceResource.title, you can pass:\n                    **{\"sourceResource.title\": \"example title\"}\n    Returns:\n        dict: The JSON response from the DPLA API as a Python dictionary.\n    \"\"\"\n    \n    # Build the request URL and minimal parameters\n    base_url = f\"{API_BASE_URL}{resource_type}\"\n    params = {\n        \"q\": query,\n        \"api_key\": os.getenv(ENV_VAR_NAME),\n    }\n    \n    # Add additional parameters if any\n    for key, value in parameters.items():\n        params[key] = value\n        \n    # Make the request\n    with httpx.Client(timeout=timeout) as client:\n        response = client.get(base_url, params=params)\n    \n    if verbose:\n        print(f\"Request URL [redacted]: {_redact_request_url(response.url)}\")\n    \n    response.raise_for_status() \n    return response.json()\n\n\n\n\n\n\n\nIn a proper application, we would want to handle the exceptions that may occur during the request (e.g. network errors, invalid API key, etc.), but for the sake of this exercise we will keep it simple and just raise any exceptions that occur.\n\n\n\nLet’s test the function with a simple query:\n\nresponse = search_items(\n    \"artificial AND intelligence\",\n    page_size=5,\n    fields=\"sourceResource.title,sourceResource.description\",\n    facets=\"sourceResource.date.begin,sourceResource.date.end\",\n    verbose=True\n)\n\nprint(json.dumps(response, indent=2))\n\nRequest URL [redacted]: https://api.dp.la/v2/items?q=artificial+AND+intelligence&page_size=5&fields=sourceResource.title%2CsourceResource.description&facets=sourceResource.date.begin%2CsourceResource.date.end\n{\n  \"count\": 3379,\n  \"docs\": [\n    {\n      \"sourceResource.title\": \"Artificial Intelligence\"\n    },\n    {\n      \"sourceResource.description\": [\n        \"\\\"January 1986.\\\"\",\n        \"Caption title.\"\n      ],\n      \"sourceResource.title\": \"Artificial intelligence\"\n    },\n    {\n      \"sourceResource.description\": [\n        \"In scope of the U.S. Government Publishing Office Cataloging and Indexing Program (C&I) and Federal Depository Library Program (FDLP).\",\n        \"Includes bibliographical references.\",\n        \"Online resource; title from PDF cover (USMC website, viewed July 18, 2024).\"\n      ],\n      \"sourceResource.title\": \"Artificial intelligence strategy\"\n    },\n    {\n      \"sourceResource.title\": \"Intelligence and the Computer - Artificial Intelligence\"\n    },\n    {\n      \"sourceResource.description\": \"Recording of the session titled \\\"Artificial intelligence and micros\\\" held at the fifth West Coast Computer Faire in San Francisco. The following papers were presented at this session: \\\"Microcomputers and the design of contelligent systems,\\\" presented by Dean Gengle. \\\"Artificial intelligence as applied to input and output in the office - or - making computers read and speak,\\\" presented by Art Derfall. \\\"Solving the Shooting Stars Puzzle,\\\" presented by Joel Shprentz.Additional Descriptive Notes: Saturday, 4:30\",\n      \"sourceResource.title\": \"Artificial intelligence and micros\"\n    }\n  ],\n  \"facets\": {\n    \"sourceResource.date.begin\": {\n      \"_type\": \"date_histogram\",\n      \"entries\": [\n        {\n          \"count\": 4,\n          \"time\": \"2026\"\n        },\n        {\n          \"count\": 128,\n          \"time\": \"2025\"\n        },\n        {\n          \"count\": 173,\n          \"time\": \"2024\"\n        },\n        {\n          \"count\": 100,\n          \"time\": \"2023\"\n        },\n        {\n          \"count\": 34,\n          \"time\": \"2022\"\n        },\n        {\n          \"count\": 34,\n          \"time\": \"2021\"\n        },\n        {\n          \"count\": 33,\n          \"time\": \"2020\"\n        },\n        {\n          \"count\": 23,\n          \"time\": \"2019\"\n        },\n        {\n          \"count\": 15,\n          \"time\": \"2018\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"2017\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"2016\"\n        },\n        {\n          \"count\": 6,\n          \"time\": \"2015\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"2014\"\n        },\n        {\n          \"count\": 26,\n          \"time\": \"2013\"\n        },\n        {\n          \"count\": 2,\n          \"time\": \"2012\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"2011\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"2010\"\n        },\n        {\n          \"count\": 6,\n          \"time\": \"2008\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"2007\"\n        },\n        {\n          \"count\": 31,\n          \"time\": \"2006\"\n        },\n        {\n          \"count\": 27,\n          \"time\": \"2005\"\n        },\n        {\n          \"count\": 71,\n          \"time\": \"2004\"\n        },\n        {\n          \"count\": 89,\n          \"time\": \"2003\"\n        },\n        {\n          \"count\": 67,\n          \"time\": \"2002\"\n        },\n        {\n          \"count\": 122,\n          \"time\": \"2001\"\n        },\n        {\n          \"count\": 327,\n          \"time\": \"2000\"\n        },\n        {\n          \"count\": 95,\n          \"time\": \"1999\"\n        },\n        {\n          \"count\": 171,\n          \"time\": \"1998\"\n        },\n        {\n          \"count\": 52,\n          \"time\": \"1997\"\n        },\n        {\n          \"count\": 97,\n          \"time\": \"1996\"\n        },\n        {\n          \"count\": 18,\n          \"time\": \"1995\"\n        },\n        {\n          \"count\": 22,\n          \"time\": \"1994\"\n        },\n        {\n          \"count\": 87,\n          \"time\": \"1993\"\n        },\n        {\n          \"count\": 17,\n          \"time\": \"1992\"\n        },\n        {\n          \"count\": 40,\n          \"time\": \"1991\"\n        },\n        {\n          \"count\": 8,\n          \"time\": \"1990\"\n        },\n        {\n          \"count\": 14,\n          \"time\": \"1989\"\n        },\n        {\n          \"count\": 9,\n          \"time\": \"1988\"\n        },\n        {\n          \"count\": 8,\n          \"time\": \"1987\"\n        },\n        {\n          \"count\": 30,\n          \"time\": \"1986\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1985\"\n        },\n        {\n          \"count\": 12,\n          \"time\": \"1984\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1983\"\n        },\n        {\n          \"count\": 13,\n          \"time\": \"1982\"\n        },\n        {\n          \"count\": 27,\n          \"time\": \"1981\"\n        },\n        {\n          \"count\": 33,\n          \"time\": \"1980\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"1979\"\n        },\n        {\n          \"count\": 5,\n          \"time\": \"1978\"\n        },\n        {\n          \"count\": 9,\n          \"time\": \"1977\"\n        },\n        {\n          \"count\": 7,\n          \"time\": \"1976\"\n        },\n        {\n          \"count\": 5,\n          \"time\": \"1975\"\n        },\n        {\n          \"count\": 10,\n          \"time\": \"1974\"\n        },\n        {\n          \"count\": 9,\n          \"time\": \"1973\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1972\"\n        },\n        {\n          \"count\": 2,\n          \"time\": \"1971\"\n        },\n        {\n          \"count\": 19,\n          \"time\": \"1970\"\n        },\n        {\n          \"count\": 13,\n          \"time\": \"1969\"\n        },\n        {\n          \"count\": 16,\n          \"time\": \"1968\"\n        },\n        {\n          \"count\": 7,\n          \"time\": \"1967\"\n        },\n        {\n          \"count\": 30,\n          \"time\": \"1966\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"1965\"\n        },\n        {\n          \"count\": 25,\n          \"time\": \"1964\"\n        },\n        {\n          \"count\": 84,\n          \"time\": \"1963\"\n        },\n        {\n          \"count\": 62,\n          \"time\": \"1962\"\n        },\n        {\n          \"count\": 53,\n          \"time\": \"1961\"\n        },\n        {\n          \"count\": 10,\n          \"time\": \"1960\"\n        },\n        {\n          \"count\": 24,\n          \"time\": \"1959\"\n        },\n        {\n          \"count\": 13,\n          \"time\": \"1958\"\n        },\n        {\n          \"count\": 16,\n          \"time\": \"1957\"\n        },\n        {\n          \"count\": 17,\n          \"time\": \"1956\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1955\"\n        },\n        {\n          \"count\": 16,\n          \"time\": \"1954\"\n        },\n        {\n          \"count\": 16,\n          \"time\": \"1952\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1951\"\n        },\n        {\n          \"count\": 13,\n          \"time\": \"1950\"\n        },\n        {\n          \"count\": 2,\n          \"time\": \"1949\"\n        },\n        {\n          \"count\": 2,\n          \"time\": \"1947\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"1946\"\n        },\n        {\n          \"count\": 6,\n          \"time\": \"1944\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"1942\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1938\"\n        },\n        {\n          \"count\": 2,\n          \"time\": \"1930\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1927\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1926\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1925\"\n        },\n        {\n          \"count\": 12,\n          \"time\": \"1924\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1923\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1914\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1885\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1871\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1844\"\n        }\n      ]\n    },\n    \"sourceResource.date.end\": {\n      \"_type\": \"date_histogram\",\n      \"entries\": [\n        {\n          \"count\": 4,\n          \"time\": \"2026\"\n        },\n        {\n          \"count\": 129,\n          \"time\": \"2025\"\n        },\n        {\n          \"count\": 173,\n          \"time\": \"2024\"\n        },\n        {\n          \"count\": 99,\n          \"time\": \"2023\"\n        },\n        {\n          \"count\": 34,\n          \"time\": \"2022\"\n        },\n        {\n          \"count\": 34,\n          \"time\": \"2021\"\n        },\n        {\n          \"count\": 33,\n          \"time\": \"2020\"\n        },\n        {\n          \"count\": 23,\n          \"time\": \"2019\"\n        },\n        {\n          \"count\": 15,\n          \"time\": \"2018\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"2017\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"2016\"\n        },\n        {\n          \"count\": 6,\n          \"time\": \"2015\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"2014\"\n        },\n        {\n          \"count\": 26,\n          \"time\": \"2013\"\n        },\n        {\n          \"count\": 2,\n          \"time\": \"2012\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"2011\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"2010\"\n        },\n        {\n          \"count\": 6,\n          \"time\": \"2008\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"2007\"\n        },\n        {\n          \"count\": 31,\n          \"time\": \"2006\"\n        },\n        {\n          \"count\": 27,\n          \"time\": \"2005\"\n        },\n        {\n          \"count\": 71,\n          \"time\": \"2004\"\n        },\n        {\n          \"count\": 89,\n          \"time\": \"2003\"\n        },\n        {\n          \"count\": 67,\n          \"time\": \"2002\"\n        },\n        {\n          \"count\": 122,\n          \"time\": \"2001\"\n        },\n        {\n          \"count\": 327,\n          \"time\": \"2000\"\n        },\n        {\n          \"count\": 95,\n          \"time\": \"1999\"\n        },\n        {\n          \"count\": 171,\n          \"time\": \"1998\"\n        },\n        {\n          \"count\": 52,\n          \"time\": \"1997\"\n        },\n        {\n          \"count\": 97,\n          \"time\": \"1996\"\n        },\n        {\n          \"count\": 18,\n          \"time\": \"1995\"\n        },\n        {\n          \"count\": 22,\n          \"time\": \"1994\"\n        },\n        {\n          \"count\": 87,\n          \"time\": \"1993\"\n        },\n        {\n          \"count\": 17,\n          \"time\": \"1992\"\n        },\n        {\n          \"count\": 40,\n          \"time\": \"1991\"\n        },\n        {\n          \"count\": 8,\n          \"time\": \"1990\"\n        },\n        {\n          \"count\": 14,\n          \"time\": \"1989\"\n        },\n        {\n          \"count\": 9,\n          \"time\": \"1988\"\n        },\n        {\n          \"count\": 8,\n          \"time\": \"1987\"\n        },\n        {\n          \"count\": 30,\n          \"time\": \"1986\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1985\"\n        },\n        {\n          \"count\": 12,\n          \"time\": \"1984\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1983\"\n        },\n        {\n          \"count\": 13,\n          \"time\": \"1982\"\n        },\n        {\n          \"count\": 28,\n          \"time\": \"1981\"\n        },\n        {\n          \"count\": 33,\n          \"time\": \"1980\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"1979\"\n        },\n        {\n          \"count\": 5,\n          \"time\": \"1978\"\n        },\n        {\n          \"count\": 9,\n          \"time\": \"1977\"\n        },\n        {\n          \"count\": 7,\n          \"time\": \"1976\"\n        },\n        {\n          \"count\": 5,\n          \"time\": \"1975\"\n        },\n        {\n          \"count\": 10,\n          \"time\": \"1974\"\n        },\n        {\n          \"count\": 9,\n          \"time\": \"1973\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1972\"\n        },\n        {\n          \"count\": 2,\n          \"time\": \"1971\"\n        },\n        {\n          \"count\": 19,\n          \"time\": \"1970\"\n        },\n        {\n          \"count\": 13,\n          \"time\": \"1969\"\n        },\n        {\n          \"count\": 16,\n          \"time\": \"1968\"\n        },\n        {\n          \"count\": 7,\n          \"time\": \"1967\"\n        },\n        {\n          \"count\": 30,\n          \"time\": \"1966\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"1965\"\n        },\n        {\n          \"count\": 25,\n          \"time\": \"1964\"\n        },\n        {\n          \"count\": 84,\n          \"time\": \"1963\"\n        },\n        {\n          \"count\": 62,\n          \"time\": \"1962\"\n        },\n        {\n          \"count\": 53,\n          \"time\": \"1961\"\n        },\n        {\n          \"count\": 10,\n          \"time\": \"1960\"\n        },\n        {\n          \"count\": 24,\n          \"time\": \"1959\"\n        },\n        {\n          \"count\": 13,\n          \"time\": \"1958\"\n        },\n        {\n          \"count\": 16,\n          \"time\": \"1957\"\n        },\n        {\n          \"count\": 17,\n          \"time\": \"1956\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1955\"\n        },\n        {\n          \"count\": 16,\n          \"time\": \"1954\"\n        },\n        {\n          \"count\": 16,\n          \"time\": \"1952\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1951\"\n        },\n        {\n          \"count\": 13,\n          \"time\": \"1950\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1949\"\n        },\n        {\n          \"count\": 2,\n          \"time\": \"1947\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"1946\"\n        },\n        {\n          \"count\": 6,\n          \"time\": \"1944\"\n        },\n        {\n          \"count\": 3,\n          \"time\": \"1942\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1938\"\n        },\n        {\n          \"count\": 2,\n          \"time\": \"1930\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1927\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1926\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1925\"\n        },\n        {\n          \"count\": 12,\n          \"time\": \"1924\"\n        },\n        {\n          \"count\": 4,\n          \"time\": \"1923\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1914\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1885\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1871\"\n        },\n        {\n          \"count\": 1,\n          \"time\": \"1844\"\n        }\n      ]\n    }\n  },\n  \"limit\": 5,\n  \"start\": 1\n}\n\n\nHooray 🎉! we’ve successfully queried the DPLA API and got a well-formatted response. However, this function only returns a single page of results. The maximum number of results per page is 100, so if we want more than one page, we need to handle pagination.\nOur second function, search_all_items, takes almost the same parameters as search_items, but it fetches multiple pages and returns a single list of items (up to a maximum you set).\nSome specific parameters for this function will allow us to control the maximum number of items to fetch (max_items), the sleep time between requests to avoid hitting rate limits (sleep), and the timeout for the HTTP requests (timeout).\n\ndef search_all_items(query, resource_type='items', max_items=100, sleep=0.5, verbose=False, timeout=30.0, **parameters):\n    \"\"\"\n    Collect up to max_items across pages.\n    \n    Args:\n        query (str): The search query string. It's possible to use logical operators \n            (AND, OR, NOT). Additionally, you can use wildcards (*) for partial matches.\n        max_items (int): Maximum number of items to retrieve. For number of elements per page, \n            use the page_size parameter in **parameters.\n        sleep (float): Time to wait between requests to avoid hitting rate limits.\n        **parameters: Facets and filter parameters from the DPLA API documentation: https://pro.dp.la/developers/requests\n    \"\"\"\n    all_docs = []\n    page = 1\n    page_size = int(parameters.get(\"page_size\", 100))\n    if page_size &gt; 100:\n        page_size = 100\n        print(\"page_size cannot exceed 100. Setting to 100.\")\n        \n    while len(all_docs) &lt; max_items:\n        parameters['page'] = page\n        data = search_items(\n            query,\n            resource_type=resource_type,\n            verbose=verbose,\n            timeout=timeout,\n            **parameters\n        )\n        docs = data.get('docs', [])\n        if not docs:\n            break  # No more results\n        all_docs.extend(docs)\n        \n        # stop if we've reached max_items\n        if len(all_docs) &gt;= max_items:\n            break\n        \n        page += 1\n        time.sleep(sleep)\n        \n    return all_docs[:max_items]\n\nAgain, this function can be improved with proper error handling and a better pagination logic, but we want to keep it simple for now.\nSame as with the previous function, we can test it with a simple query:\n\nresults = search_all_items(\n    \"artificial AND intelligence\",\n    page_size=50,\n    max_items=150,\n    fields=\"sourceResource.title,sourceResource.description\",\n    facets=\"sourceResource.date.begin,sourceResource.date.end\",\n)\n\nprint(f\"Total items retrieved: {len(results)}\")\nprint(json.dumps(results[-5:], indent=2))  # Print the last 5 results\n\nTotal items retrieved: 150\n[\n  {\n    \"sourceResource.description\": [\n      \"\\\"May 2023.\\\"\",\n      \"Includes bibliographical references (pages 40-47).\",\n      \"\\\"A report by the Select Committee on Artificial Intelligence of the National Science and Technology Council.\\\"\",\n      \"Description based on online resource; title from PDF cover (White House, viewed Nov. 7, 2023).\"\n    ],\n    \"sourceResource.title\": \"The national artificial intelligence research and development strategic plan: 2023 update\"\n  },\n  {\n    \"sourceResource.description\": [\n      \"Updated irregularly\",\n      \"The CRS web page provides access to all versions published since 2018 in accordance with P.L. 115-141.\",\n      \"In scope of the U.S. Government Publishing Office Cataloging and Indexing Program (C&I) and Federal Depository Library Program (FDLP).\",\n      \"Report includes bibliographical references.\",\n      \"Contents viewed on December 7, 2023; title from CRS web page.\"\n    ],\n    \"sourceResource.title\": \"Artificial intelligence : overview, recent advances, and considerations for the 118th Congress\"\n  },\n  {\n    \"sourceResource.description\": \"Bibliography: leaves [107]-[108]\",\n    \"sourceResource.title\": \"The meaning and mechanics of intelligence\"\n  },\n  {\n    \"sourceResource.description\": [\n      \"Updated irregularly\",\n      \"Batch processed record.\",\n      \"This record provides access to the versions of this bill or resolution published in the United States Government Publishing Office's (GPO) GovInfo system. To see the versions, use the GPO PURL. To see the actions related to this bill or resolution, use the Congress.gov URL.\",\n      \"At head of title: 118th Congress, 2d session.\",\n      \"Sponsor(s): Representative Lisa Blunt Rochester.\",\n      \"Cosponsor(s): Representative Marcus J. Molinaro.\",\n      \"Referred committee(s): House Committee on Energy and Commerce.\",\n      \"Date of introduction: \\\"September 19, 2024.\\\"\",\n      \"United States Congress chamber(s): United States House of Representatives.\",\n      \"United States federal government branch: Legislative branch.\",\n      \"Access ID (GovInfo): BILLS-118hr9673ih.\",\n      \"Pagination at time of introduction: 16 pages.\",\n      \"In scope of the U.S. Government Publishing Office Cataloging and Indexing Program (C&I) and Federal Depository Library Program (FDLP).\",\n      \"GovInfo.gov metadata; title from caption of the introduced version (GovInfo, September 19, 2024).\"\n    ],\n    \"sourceResource.title\": \"To Direct the Secretary of Commerce to Develop a National Strategy Regarding Artificial Intelligence Consumer Literacy and Conduct a National Artificial Intelligence Consumer Literacy Campaign\"\n  },\n  {\n    \"sourceResource.description\": \"Deep learning has rapidly emerged as a transformative technology that permeates all modern software, from autonomous driving systems to medical-diagnosis and malware-detection tools. Considering the critical role of this software in our technologies, it must behave as intended. The complexity introduced by deep-learning components complicates formal reasoning about the behavior of such software, frequently resulting in solutions that offer only empirical or no guarantees.This thesis contributes techniques and algorithms that increase the robustness of deep-learning-powered software by providing strong provable guarantees across the components existing in the entire deep learning pipeline. By leveraging the power of abstract interpretation, a well-established theory for program analysis and verification, this thesis enables the verification of robustness properties across the deep learning pipeline. The thesis focuses on four critical aspects of robustness: (1) preventing numerical bugs\",\n    \"sourceResource.title\": \"Towards Robust Artificial-Intelligence-Powered Software: Provable Guarantees via Abstract Interpretation\"\n  }\n]\n\n\nGreat! We have successfully retrieved multiple pages of results from the DPLA API, and we can control the number of items we want to fetch with the max_items parameter. Now we are ready to do some queries and explore the data!",
    "crumbs": [
      "Getting Data Programmatically",
      "Helping Functions"
    ]
  },
  {
    "objectID": "chapters/coding/coding-intro.html",
    "href": "chapters/coding/coding-intro.html",
    "title": "Getting Started with APIs and Coding",
    "section": "",
    "text": "Tools like Postman and Bruno are great for exploring and testing APIs. But if you want to pull data from multiple endpoints, repeat requests, automate workflows, or clean and analyze results, you’ll quickly want to use code.\nMost high-level languages used in data science and digital humanities (Python, R, JavaScript, etc.) have libraries for working with web APIs. Those libraries handle the basics—sending requests, parsing responses, and (often) authentication—so you can focus on what data you need and how to use it. The hardest part is usually the API itself: its authentication requirements, endpoint structure, and response format.\nIn this section, we’ll use Python with the Digital Public Library of America (DPLA) API. DPLA is a good learning API because it offers a lot of public data and has a relatively straightforward structure and authentication process.",
    "crumbs": [
      "Getting Data Programmatically",
      "Getting Started"
    ]
  },
  {
    "objectID": "chapters/coding/coding-intro.html#our-goal",
    "href": "chapters/coding/coding-intro.html#our-goal",
    "title": "Getting Started with APIs and Coding",
    "section": "Our Goal",
    "text": "Our Goal\nIn this exercise, we’ll use DPLA data to ask a research-style question: How does “artificial intelligence” show up in cultural heritage metadata over time?\nWe’ll start from this public DPLA search:\nhttps://dp.la/search?q=artificial+AND+intelligence&subject=%22Artificial+intelligence%22&page=1\n\n\n\nSearch results for “artificial AND intelligence” in the DPLA\n\n\nAs you can see, the search is filtered to include only items with the subject “Artificial intelligence”. This is because not all matches in the simple query are actually about this topic but, for instance, disclaimers about using AI to generate or enhance metadata.\nThis is a useful example for our lesson because you can’t bulk download all of these filtered results from the DPLA website, and the website’s subject filter is case-sensitive (small changes can lead to very different counts). The API behaves differently, so we can run a more consistent search and work with a larger (but still manageable) set of results.\nFrom there, we’ll use the API to fetch metadata, then use facets and dates to get a first “trend” view. We’ll also compare three time windows—preCovid (1844–2019), Covid (2019–2022), and postCovid (2022–2026)—and extract keywords from titles and descriptions (excluding obvious phrases) to see what stands out in each period.\nThis gives us a high-level view of how “artificial intelligence” appears in a large cultural heritage collection across time, and shows how adjusting parameters can refine results into a better pool of data for analysis.",
    "crumbs": [
      "Getting Data Programmatically",
      "Getting Started"
    ]
  },
  {
    "objectID": "chapters/coding/coding-intro.html#where-to-start",
    "href": "chapters/coding/coding-intro.html#where-to-start",
    "title": "Getting Started with APIs and Coding",
    "section": "Where to Start?",
    "text": "Where to Start?\nBefore writing code, it helps to understand what the API expects (requests) and what it returns (responses). The best place to start is the documentation—in this case, the DPLA API Codex. The DPLA team provides a short “how to use it” guide that boils things down to two steps:\n\nRequest an API key from DPLA: To get a key, send a request with your email address. DPLA will email you a 32-character API key.\n\nTo request an API key, open your terminal and use the curl command:\ncurl \"https://api.dp.la/v2/api_key/YOUR_EMAIL@example.com\"\nReplace YOUR_EMAIL@example.com with your actual email address. The DPLA will send you a 32-character API key to your email inbox.\n\n\n\n\n\n\nKeep your API key secure and do not share it publicly. Your API key is like a password tied to your email address, and it can be revoked if it’s abused. See the DPLA API policies for details.\n\n\n\n\nMake a request to the API: With your API key, you’re ready to test a request and confirm that you can retrieve data.\n\n\n\nCode\nviewof method = Inputs.select([\"GET\"], {\n  label: \"HTTP Method\",\n  attributes: {\n  class: \"form-select mb-3\"\n  }\n})\n\nviewof endpoint = Inputs.text({\n  label: \"Endpoint path\", \n  placeholder: \"/v2/items\",\n  value: \"/v2/items\",\n  attributes: {\n  class: \"form-control mb-3\"\n  }\n})\n\nviewof query = Inputs.text({\n    label: \"Search: \",\n    placeholder: \"weasel, cat, dog, etc.\",\n    value: \"cat\",\n    attributes: {\n    class: \"form-control mb-3\"\n    }\n})\n\nviewof apikey = Inputs.password({\n  label: \"API Key\",\n  placeholder: \"Your DPLA API key\",\n  value: \"\",\n  attributes: {\n  class: \"form-control mb-3\"\n  }\n})\n\n// Function to make the API request\nasync function fetchFromApi(method, path, q, api_key) {\n  const baseUrl = \"https://api.dp.la\";\n  const targetUrl = `${baseUrl}${path}?q=${q}&page_size=1&api_key=${api_key}`;\n  // Using All Origins proxy to bypass CORS issues\n  // See the documentation in https://allorigins.win/\n  const proxyUrl = `https://api.allorigins.win/get?url=${encodeURIComponent(targetUrl)}`;\n\n  try {\n    const response = await fetch(proxyUrl);\n    if (!response.ok) throw new Error(\"Proxy server is down.\");\n\n    const wrapper = await response.json();\n    const dplaStatus = wrapper.status.http_code; \n    const data = JSON.parse(wrapper.contents);\n\n    // Check for authentication errors first (403 or \"Unauthorized\" message)\n    if (dplaStatus === 403 || data.message === \"Unauthorized\") {\n      return {\n        data: { \"Message\": \"🔑 Invalid API Key. Please check your key and try again.\" },\n        status: { code: 403, ok: false, text: \"Unauthorized\" }\n      };\n    }\n\n    // Check for other DPLA errors (like 404 or 400)\n    if (dplaStatus &gt;= 400) {\n      return {\n        data: data,\n        status: { code: dplaStatus, ok: false, text: \"DPLA Error\" }\n      };\n    }\n\n    return { data, status: { code: 200, ok: true, text: \"OK\" } };\n\n  } catch (error) {\n    // Network or Parsing errors\n    return {\n      data: { \"Message\": `🌐 Connection Error: ${error.message}` },\n      status: { code: 500, ok: false, text: \"Network Error\" }\n    };\n  }\n}\n\nresponse = {\n  const result = await fetchFromApi(method, endpoint, query, apikey);\n  return result;\n}\n\nviewof prettyResponse = {\n  let content;\n\n  if (response.data.Message) {\n    content = html`&lt;div class=\"alert alert-warning m-0\"&gt;${response.data.Message}&lt;/div&gt;`;\n  } else {\n    content = html`&lt;pre class=\"card-body m-0\" style=\"background-color: #f8f9fa; max-height: 400px; overflow-y: auto;\"&gt;${JSON.stringify(response.data, null, 2)}&lt;/pre&gt;`;\n  }\n\n  const badgeClass = response.status.ok ? \"bg-success\" : \"bg-danger\";\n\n  const container = html`&lt;div class=\"card\"&gt;\n  &lt;div class=\"card-header d-flex justify-content-between align-items-center\"&gt;\n  &lt;span&gt;Response&lt;/span&gt;\n  &lt;span class=\"badge ${badgeClass}\"&gt;${response.status.code} ${response.status.text}&lt;/span&gt;\n  &lt;/div&gt;\n  ${content}\n  &lt;/div&gt;`;\n  return container;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNoteAPI Key Privacy\n\n\n\n\n\nThe API key is not stored anywhere in this book, and it’s used exclusively to make the request to the DPLA API. Once you close the browser tab or refresh the page, the API key will be lost.",
    "crumbs": [
      "Getting Data Programmatically",
      "Getting Started"
    ]
  },
  {
    "objectID": "chapters/coding/coding-intro.html#understanding-the-response",
    "href": "chapters/coding/coding-intro.html#understanding-the-response",
    "title": "Getting Started with APIs and Coding",
    "section": "Understanding the Response",
    "text": "Understanding the Response\nFor the sake of simplicity, we limit the response to one item. Even then, the response is large and not very human-readable at first. Here are a few fields that are especially useful when you’re getting started:\n\ncount, which indicates the total number of items that match the query.\n\n\n\nCode\nviewof countInfo = {\n    let count;\n\n    if (response.data.count !== undefined ){\n    count = html`&lt;div class=\"alert alert-info\"&gt;Total items matching the query: &lt;strong&gt;${response.data.count}&lt;/strong&gt;&lt;/div&gt;`\n    } else {\n    count = html`&lt;div class=\"alert alert-secondary\"&gt;Waiting for the response&lt;/div&gt;`\n    }\n\n    return count;\n}\n\n\n\n\n\n\n\n\ndocs, which contains the actual data of the items retrieved. It’s possible to access specific fields at this level, for example, the ingestDate field, which indicates the date when the item was ingested into the DPLA.\n\n\n\nCode\nviewof ingestDateInfo = {\n    let ingestDate;\n\n    if (response.data.docs && response.data.docs[0] && response.data.docs[0].ingestDate) {\n    ingestDate = html`&lt;div class=\"alert alert-info\"&gt;Ingest Date of the first item: &lt;strong&gt;${response.data.docs[0].ingestDate}&lt;/strong&gt;&lt;/div&gt;`\n    } else {\n    ingestDate = html`&lt;div class=\"alert alert-secondary\"&gt;Waiting for the response&lt;/div&gt;`\n    }\n\n    return ingestDate;\n}\n\n\n\n\n\n\n\n\nsourceResource, which contains the metadata of the item, including fields like title, creator, date, description, etc.\n\n\n\nCode\nviewof source = {\n    let title, creator, date;\n    if (response.data.docs && response.data.docs[0] && response.data.docs[0].sourceResource) {\n    const source = response.data.docs[0].sourceResource;\n    title = source.title ? source.title[0] : \"N/A\";\n    creator = source.creator ? source.creator[0] : \"N/A\";\n    date = source.date ? source.date[0].displayDate : \"N/A\";\n    return html`&lt;div class=\"card\"&gt;\n    &lt;div class=\"card-body\"&gt;\n    &lt;h5 class=\"card-title\"&gt;${title}&lt;/h5&gt;\n    &lt;h6 class=\"card-subtitle mb-2 text-muted\"&gt;Creator: ${creator}&lt;/h6&gt;\n    &lt;p class=\"card-text\"&gt;Date: ${date}&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;/div&gt;`;\n    } else {\n    return html`&lt;div class=\"alert alert-secondary\"&gt;Waiting for the response&lt;/div&gt;`\n    }\n}\n\n\n\n\n\n\n\n\nprovider, which indicates the institution that provided the item to the DPLA.\n\n\n\nCode\nviewof providerInfo = {\n    let provider;\n    if (response.data.docs && response.data.docs[0] && response.data.docs[0].provider) {\n    provider = response.data.docs[0].provider.name || \"N/A\";\n    return html`&lt;div class=\"alert alert-info\"&gt;Provider: &lt;strong&gt;${provider}&lt;/strong&gt;&lt;/div&gt;`;\n    } else {\n    return html`&lt;div class=\"alert alert-secondary\"&gt;Waiting for the response&lt;/div&gt;`\n    }\n}\n\n\n\n\n\n\n\nThese are just a few examples. To go further, see the DPLA documentation on requests and responses.\n\n\n\n\n\n\nWarningBulk downloads\n\n\n\n\n\nThe API is great for searching and sampling, but it’s not meant for downloading large volumes of data. If you need a full dataset, use DPLA’s bulk download files instead.",
    "crumbs": [
      "Getting Data Programmatically",
      "Getting Started"
    ]
  },
  {
    "objectID": "chapters/dataAnalysis/collecting-data-for-analysis.html",
    "href": "chapters/dataAnalysis/collecting-data-for-analysis.html",
    "title": "Collecting Data for Analysis",
    "section": "",
    "text": "During the previous chapters, we learned how to interact with APIs to retrieve data. We also practiced retrieving the total number of objects that contain a specific term in a particular department of the Metropolitan Museum of Art.\nIn this chapter, we will take the next step: collecting and analyzing data using the Metropolitan Museum of Art API. Specifically, we will explore the question: What thematic areas can we identify from the terms associated with objects in the Medieval Art collection?\nBy analyzing how pairs of terms co-occur, we aim to uncover thematic areas and explore whether these topics reveal insights about Medieval Art — at least from the perspective of the Metropolitan Museum of Art’s metadata.\nThis chapter showcases how computational approaches can reveal hidden patterns in cultural data. By analyzing co-occurring terms in museum metadata, we aim to uncover thematic areas that define Medieval Art, helping us explore the implicit narratives within museum collections.",
    "crumbs": [
      "Data Analysis",
      "Collecting Data for Analysis"
    ]
  },
  {
    "objectID": "chapters/dataAnalysis/collecting-data-for-analysis.html#preparing-the-data-collection-and-preservation",
    "href": "chapters/dataAnalysis/collecting-data-for-analysis.html#preparing-the-data-collection-and-preservation",
    "title": "Collecting Data for Analysis",
    "section": "Preparing the data collection and preservation",
    "text": "Preparing the data collection and preservation\nBefore starting to collect data, it’s important to think ahead and plan how the data will be collected, stored, and preserved for future use. Although this is a small project, we can follow some best practices to ensure data integrity and reproducibility.\nOur first step is to create a directory structure to store the data and file naming conventions. We will create a directory called data to store both the raw and processed data. We will also create a directory to store the code used to collect data, and a directory to store the results of the analysis. Additionally, we will create a README.md file to document the project and a LICENSE file to specify the license under which the data is made available.\nThe project directory structure would look like this:\nproject/\n├── data/\n│   ├── raw/ # Original, unprocessed data\n│   ├── processed/ # Cleaned and transformed data for analysis\n├── code/ # Scripts used for data collection and processing\n├── README.md # Project documentation\n├── LICENSE # Licensing information\nTo ensure consistency, we will use clear and descriptive names for our files. For example:\n\nraw_medieval_art_tags.csv: The original data collected from the API.\nprocessed_term_pairs.csv: The processed dataset containing term pairs with weights.\n\nVisit the project repository on GitHub to explore the full directory structure, example files, and scripts.\n\n\n\n\n\n\nTipAbout Data Use\n\n\n\n\n\nIt’s important to check the data source documentation to identify the terms of use and any possible restrictions. In this case, the Metropolitan Museum of Art has published the data under a Creative Commons Zero (CC0) license. This means the data is free to use and distribute without restrictions. For more details, visit the API documentation.",
    "crumbs": [
      "Data Analysis",
      "Collecting Data for Analysis"
    ]
  },
  {
    "objectID": "chapters/dataAnalysis/collecting-data-for-analysis.html#what-data-to-collect",
    "href": "chapters/dataAnalysis/collecting-data-for-analysis.html#what-data-to-collect",
    "title": "Collecting Data for Analysis",
    "section": "What data to collect?",
    "text": "What data to collect?\nBefore starting to collect data, we need to explore what data is available and how it can help answer our research question.\nThe first step is to examine a random object from the Medieval Art collection to identify useful fields for our analysis. We can use the endpoint https://collectionapi.metmuseum.org/public/collection/v1/objects?departmentIds=17 to retrieve a list of object IDs from the Medieval Art collection. Simply paste the full endpoint into your browser to see a response like this:\n\n{\n     \"total\": 7136,\n     \"objectIDs\": [\n        32830, 32831, 32832, 32833, 32834,\n        32835, 32836, 32837, 32838, 32839,\n        // ... many more IDs ...\n        818469, 818574, 831188\n        ]\n}\nOnce we have an object ID, we can use it to get detailed information about that object by appending the ID to the endpoint https://collectionapi.metmuseum.org/public/collection/v1/objects/{objectID}. For example, the object with ID 32830 gives the following detailed response:\n{\n  \"objectID\": 32830,\n  \"isHighlight\": false,\n  \"accessionNumber\": \"23.21.2\",\n  \"accessionYear\": \"1923\",\n  \"isPublicDomain\": true,\n  \"primaryImage\": \"https://images.metmuseum.org/CRDImages/md/original/DP164978.jpg\",\n  \"primaryImageSmall\": \"https://images.metmuseum.org/CRDImages/md/web-large/DP164978.jpg\",\n  \"additionalImages\": [\n    \"https://images.metmuseum.org/CRDImages/md/original/DP164979.jpg\"\n  ],\n  \"constituents\": null,\n  \"department\": \"Medieval Art\",\n  \"objectName\": \"Manuscript cutting from a Dominican antiphonary\",\n  \"title\": \"Initial P with the Martyrdom of Saint Peter Martyr\",\n  \"culture\": \"Italian\",\n  \"period\": \"\",\n  \"dynasty\": \"\",\n  \"reign\": \"\",\n  \"portfolio\": \"\",\n  \"artistRole\": \"\",\n  \"artistPrefix\": \"\",\n  \"artistDisplayName\": \"\",\n  \"artistDisplayBio\": \"\",\n  \"artistSuffix\": \"\",\n  \"artistAlphaSort\": \"\",\n  \"artistNationality\": \"\",\n  \"artistBeginDate\": \"\",\n  \"artistEndDate\": \"\",\n  \"artistGender\": \"\",\n  \"artistWikidata_URL\": \"\",\n  \"artistULAN_URL\": \"\",\n  \"objectDate\": \"second half 13th century\",\n  \"objectBeginDate\": 1350,\n  \"objectEndDate\": 1400,\n  \"medium\": \"Tempera and ink on parchment\",\n  \"dimensions\": \"3 1/8 x 2 13/16 in. (7.9 x 7.1 cm)\",\n  \"measurements\": null,\n  \"creditLine\": \"Gift of Bashford Dean, 1923\",\n  \"geographyType\": \"Made in\",\n  \"city\": \"Bologna\",\n  \"state\": \"\",\n  \"county\": \"\",\n  \"country\": \"Italy\",\n  \"region\": \"\",\n  \"subregion\": \"\",\n  \"locale\": \"\",\n  \"locus\": \"\",\n  \"excavation\": \"\",\n  \"river\": \"\",\n  \"classification\": \"Manuscripts and Illuminations\",\n  \"rightsAndReproduction\": \"\",\n  \"linkResource\": \"\",\n  \"metadataDate\": \"2024-10-03T04:53:53.567Z\",\n  \"repository\": \"Metropolitan Museum of Art, New York, NY\",\n  \"objectURL\": \"https://www.metmuseum.org/art/collection/search/32830\",\n  \"tags\": [\n    {\n      \"term\": \"Saint Peter\",\n      \"AAT_URL\": \"http://vocab.getty.edu/page/ia/901000056\",\n      \"Wikidata_URL\": \"https://www.wikidata.org/wiki/Q33923\"\n    },\n    {\n      \"term\": \"Men\",\n      \"AAT_URL\": \"http://vocab.getty.edu/page/aat/300025928\",\n      \"Wikidata_URL\": \"https://www.wikidata.org/wiki/Q8441\"\n    }\n  ],\n  \"objectWikidata_URL\": \"\",\n  \"isTimelineWork\": true,\n  \"GalleryNumber\": \"\"\n}\nBy exploring the object data, we can identify several fields of interest:\n\n\n\nField\nDescription\n\n\n\n\ntitle\nName of the object\n\n\nobjectName\nType of object (e.g., Manuscript cutting)\n\n\nmedium\nMaterials used (e.g., Tempera on parchment)\n\n\nculture\nCultural origin (e.g., Italian)\n\n\nobjectDate\nDate or estimated time period\n\n\ntags\nControlled vocabulary terms (AAT & Wikidata)\n\n\n\nFor our analysis, we will focus on the tags field, which provides a list of terms associated with the object. The tags field is particularly valuable because it uses controlled vocabularies like the Art and Architecture Thesaurus (AAT) and Wikidata, ensuring consistency across the dataset. These terms capture key themes and concepts, making them ideal for analyzing thematic areas in Medieval Art.\nDocumenting the focus of our analysis in a README.md file is a critical step in ensuring transparency and reproducibility. The README.md file provides future researchers (or your future self!) with a clear understanding of the dataset, its purpose, and how it was used. Here’s an example of how you can document this information:\nHaving identified the tags field as the focus of our analysis, we can include that information in our README.md file to document the project. This is an example of how we can include that information in the README.md file:\n\n\n\n\n\n\nNoteMedieval Art Collection Analysis\n\n\n\n\n\nThis project aims to analyze the Medieval Art collection of the Metropolitan Museum of Art. The focus of this analysis is on the tags field, which provides a list of terms associated with each object, using controlled vocabularies like the Art and Architecture Thesaurus (AAT) and Wikidata. With that information, we aim to identify thematic areas and explore whether these topics reveal insights about Medieval Art — at least from the perspective of the Metropolitan Museum of Art’s metadata, and propose a method to analyze arbitrary terms associated with museum collections.\nLicense and right of use\nThe data provided by the Metropolitan Museum of Art is published under a Creative Commons Zero (CC0) license. This means that the data is in the public domain and can be used for any purpose without restriction. The derived data and code written for this project is published under the same license.",
    "crumbs": [
      "Data Analysis",
      "Collecting Data for Analysis"
    ]
  },
  {
    "objectID": "chapters/dataAnalysis/collecting-data-for-analysis.html#collecting-data",
    "href": "chapters/dataAnalysis/collecting-data-for-analysis.html#collecting-data",
    "title": "Collecting Data for Analysis",
    "section": "Collecting data",
    "text": "Collecting data\nNow that we have identified the tags field as the focus of our analysis, we can start collecting data. To do this, we need to extract the specific information we need and store it in a format suitable for analysis.\nTypically, this involves isolating the necessary information and storing it in a structured format like CSV (Comma-Separated Values). For our purpose, the CSV file will have two columns: objectID and tags. An example of the data is shown below:\nobjectID,terms\n462990,\"['Entombment', 'Christ', 'Virgin Mary']\"\n462991,['Women']\n462992,\"['Men', 'Cross', 'Christ']\"\n462994,['Men']\n462995,['Men']\n462997,\"['Cross', 'Christ']\"\n462998,\"['Animals', 'Birds']\"\n463015,\"['Men', 'Portraits']\"\n463023,['Deer']\n463024,['Eagles']\n463032,\"['Entombment', 'Christ']\"\n463036,\"['Illness', 'Men', 'Women']\"\n463037,\"['Kings', 'Women', 'David', 'Eve']\"\n463039,\"['Kings', 'Baptismal Fonts', 'Saints']\"\n463040,\"['Infants', 'Women', 'Baptismal Fonts']\"\n463052,['Deer']\n463081,\"['Animals', 'Cross']\"\n463083,['Human Figures']\n463084,\"['Human Figures', 'Angels']\"\nThis data was collected from the first 100 objects in the Medieval Art collection. It’s important to note that not all objects have tags, and some only have one term. Exploring the data is crucial to understanding its completeness and whether it can answer our research question.\nInstead of treating tags as separate labels, we analyze co-occurrence—when two terms appear together in the same object. By counting these relationships, we can visualize which themes are closely related across different artworks. Even with incomplete data, we can identify thematic areas. For example, we can extract term pairs that co-occur within the same object. For object ID 462990, the terms Entombment, Christ, and Virgin Mary co-occur, which can be represented like this:\nSource,Target\nEntombment,Christ\nEntombment,Virgin Mary\nChrist,Virgin Mary\nWhen we notice that some pairs repeat across different objects, we can add weights to these pairs. For example, the pair Men and Cross appears in both 462992 and 462997, giving it a weight of 2. The table becomes:\nSource,Target,Weight\nEntombment,Christ,1\nEntombment,Virgin Mary,1\nChrist,Virgin Mary,1\nMen,Cross,2\nMen,Christ,1\nCross,Christ,1\nUsing this small sample, we can already start drafting a simple network graph to visualize these relationships.\n\n\nCode\nlibrary(igraph)\n\nedges &lt;- data.frame(\n  Source = c(\n  \"Entombment\", \"Entombment\", \"Christ\", \"Men\", \"Men\", \"Cross\", \n  \"Animals\", \"Men\", \"Illness\", \"Illness\", \"Men\", \"Kings\", \n  \"Kings\", \"Kings\", \"Women\", \"Women\", \"David\"\n),\n  Target = c(\n  \"Christ\", \"Virgin Mary\", \"Virgin Mary\", \"Cross\", \"Christ\", \"Christ\", \n  \"Birds\", \"Portraits\", \"Men\", \"Women\", \"Women\", \"Women\", \n  \"David\", \"Eve\", \"David\", \"Eve\", \"Eve\"\n),\n  Weight = c(\n  2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1\n)\n\n)\n\nnetwork &lt;- graph_from_data_frame(edges, directed = FALSE)\n\nplot(\n  network,\n  vertex.size = 30,\n  vertex.label.cex = 0.8,\n  edge.width = E(network)$Weight * 2,\n  edge.color = \"gray50\",\n  vertex.color = \"lightblue\",\n  main = \"Network Graph\"\n)\n\n\n\n\n\n\n\n\n\nThis simple network graph provides a glimpse of the relationships between terms. For instance, we can see how the network is highly centered around the term “Men,” which connects with other groups associated with “Christ.” In contrast, “Women” is part of a smaller group tied to terms like “Eve,” “David,” and “Kings.” While this is just a small sample, it demonstrates how network analysis can reveal thematic patterns.\nWith this proof of concept, we’re ready to scale up to the entire Medieval Art collection. The complete data includes term pairs from 3695 objects, resulting in 1725 unique pairs. The dataset is stored in a CSV file, which can be accessed here: Medieval Art Data - Tags Pairs. Below is a preview of the first 20 rows:\n\n\nCode\nterms_collection &lt;- read.csv(\"https://raw.githubusercontent.com/jairomelo/intro2APIs-examples/refs/heads/main/data/processed/processed_medieval_art_tags_pairs.csv\")\nhead(terms_collection, 20)\n\n\n                  Source                     Target Weight\n1               Achilles                   Centaurs      1\n2               Achilles                     Horses      1\n3               Achilles                        Men      1\n4               Achilles                      Tents      1\n5               Achilles                 Trojan War      1\n6                   Adam                     Angels      1\n7                   Adam                     Christ      2\n8                   Adam                Crucifixion      2\n9                   Adam                        Eve      6\n10                  Adam                   Nativity      1\n11                  Adam                Virgin Mary      1\n12                  Adam                    Working      1\n13 Adoration of the Magi Adoration of the Shepherds      2\n14 Adoration of the Magi                     Angels      4\n15 Adoration of the Magi               Annunciation      5\n16 Adoration of the Magi                       Beds      1\n17 Adoration of the Magi                     Christ      8\n18 Adoration of the Magi                Crucifixion      6\n19 Adoration of the Magi                       Dogs      1\n20 Adoration of the Magi                      Jesus      6\n\n\n\n\n\n\n\n\nBoth raw and processed data are stored in the project repository. You can download the data directly from the repository to use it in your own analysis.\n\n\n\nIn the next chapter, we’ll use this full dataset to build a comprehensive network graph, analyze its structure, and uncover thematic clusters that provide deeper insights into the Medieval Art collection.\n\n\n\n\n\n\nTipA Note About Automation\n\n\n\n\n\nManually collecting data from over 7,000+ objects would be time-consuming and error-prone. By automating the retrieval and processing of metadata, we ensure accuracy, reproducibility, and scalability—allowing us to apply this method to other museum collections in the future.\nTo collect the data for this project, we used a Python script to automate the process. The script performs the following steps:\n\nRetrieves object IDs from the API.\nFetches tags for each object.\nGenerates term pairs and calculates weights.\nSaves the results in a CSV file for analysis.\n\nCode is available in the project repository. If you’re familiar with Python, you can experiment with it to collect data from other departments or customize it to fit your needs",
    "crumbs": [
      "Data Analysis",
      "Collecting Data for Analysis"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-1.html",
    "href": "chapters/postman/exercise-github-api-1.html",
    "title": "Exercise 1: Getting Information from GitHub",
    "section": "",
    "text": "Now, instead of walking through GitHub’s many endpoints one by one, you’re going to solve a few challenges that will help reinforce the concepts we’ve explored in the previous lessons.\nBut don’t worry — before you begin, let’s review how to navigate GitHub’s API documentation to build requests in Postman.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 1: Getting Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-1.html#the-get-octocat-endpoint",
    "href": "chapters/postman/exercise-github-api-1.html#the-get-octocat-endpoint",
    "title": "Exercise 1: Getting Information from GitHub",
    "section": "The “Get Octocat” Endpoint",
    "text": "The “Get Octocat” Endpoint\nThis endpoint is part of GitHub’s Meta category.\nTo find it, go to the GitHub API documentation:\nhttps://docs.github.com/en/rest?apiVersion=2022-11-28\nIn the left sidebar, scroll down to:\nMeta\n|- Meta\n|  |- Get Octocat\n\nGitHub’s documentation may seem intimidating at first, but with a closer look, it’s not that different from The Cat API’s documentation.\n\n\n\nGet Octocat documentation\n\n\nHere’s how to read it:\n\nThe left column explains what the endpoint does and what type of token it accepts.\n\nIn our case, we’re using a Personal Access Token (classic), which gives us access to repositories and endpoints we have permission for.\nFine-grained tokens are another type of GitHub token with more limited scopes. If a note says “This endpoint works with fine-grained tokens,” it means you can use it, but you may encounter restrictions.\n\nThe Parameters section (still in the left column) lists:\n\nRequired headers\nOptional query parameters\nPath parameters\nBody fields (for POST/PATCH requests)\n\nThe right column shows example requests in cURL, JavaScript, and GitHub CLI.\nEven if you’re not using those, they can help you understand what values and formats are expected.\nAt the very top, you’ll see the actual API endpoint. For this one, it’s: GET/octocat\nAt the bottom, you’ll find sample responses and schemas — great for comparing against your own requests.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 1: Getting Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-1.html#translating-github-docs-to-postman",
    "href": "chapters/postman/exercise-github-api-1.html#translating-github-docs-to-postman",
    "title": "Exercise 1: Getting Information from GitHub",
    "section": "Translating GitHub Docs to Postman",
    "text": "Translating GitHub Docs to Postman\nLet’s build the request for /octocat in Postman:\n\nCreate a new GET request inside your GitHub collection.\nSet the URL to: {{gh_URL}}/octocat\nUnder the Headers tab, add:\n\n\nKey: accept\nValue: application/vnd.github+json\n\n\nUnder the Params tab, add:\n\n\nKey: s\nValue: (any message you want — e.g., This is me, interacting with GitHub)\n\n\nMake sure your GitHub environment is selected, then click Send.\n\nYou should see the Octocat in your response window!\n\n\n\nThe Octocat",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 1: Getting Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-1.html#search-repositories",
    "href": "chapters/postman/exercise-github-api-1.html#search-repositories",
    "title": "Exercise 1: Getting Information from GitHub",
    "section": "Search Repositories",
    "text": "Search Repositories\nNow it’s your turn!\nIn this exercise, you’ll use Postman to send a request that searches for public GitHub repositories matching specific criteria.\nYou can find the endpoint documentation by navigating to:\nSearch\n|- Search\n|  |- Search repositories\nOnce there, follow the documentation to write a request that:\n\nSearches for repositories matching the query:\n\ntetris+language:python\n\nSorts results by: stars\nOrders them in: descending order (desc)\n\nOptional, but recommended:\n\nReturn 30 results per page\nGet results from page 3\n\nUse the information in the documentation to set the correct query parameters in Postman.\nWhen you’re done, explore the response — it will include metadata like repository names, descriptions, star counts, and links to the original repos.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 1: Getting Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-1.html#check-your-work",
    "href": "chapters/postman/exercise-github-api-1.html#check-your-work",
    "title": "Exercise 1: Getting Information from GitHub",
    "section": "Check your work",
    "text": "Check your work\nIf everything is set up correctly, your request should look something like this:\n\nURL\n{gh_URL}/search/repositories\n\n\nQuery Parameters\n\n\n\nKey\nValue\n\n\n\n\nq\ntetris+language:python\n\n\nsort\nstars\n\n\norder\ndesc\n\n\nper_page\n30\n\n\npage\n3\n\n\n\n\n\nHeaders (optionals)\n\n\n\nKey\nValue\n\n\n\n\naccept\napplication/vnd.github+json\n\n\nAuthorization\nBearer {{gh_token}}\n\n\n\nYour response should include a JSON object with a key named items — this is a list of repositories that match your search. You can scan through the names, star counts, and descriptions to explore what’s trending in the world of Python + Tetris 😄\nIf your response returns an error or doesn’t look like what’s expected:\n\nDouble-check your query parameters\nMake sure you’re using GET as the request method\nConfirm your token is still valid and your environment is active",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 1: Getting Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/postman.html",
    "href": "chapters/postman/postman.html",
    "title": "What’s Postman",
    "section": "",
    "text": "Postman is a widely used platform that facilitates interaction with APIs. Originally designed for testing APIs in development environments, it has become a powerful tool for exploring and managing API requests in both individual and collaborative workflows.\nOne of Postman’s major strengths is that it enables users without programming experience to interact with API services in a comprehensive and structured way. Users can send requests to read, write, update, and delete information from an API, all through a visual interface. This includes managing authentication credentials securely (even OAuth), defining public and secret variables, configuring headers, adding parameters, and submitting body content in various formats (form-data, binary, raw JSON, GraphQL, etc.).",
    "crumbs": [
      "Using Postman",
      "Postman"
    ]
  },
  {
    "objectID": "chapters/postman/postman.html#postman-applications",
    "href": "chapters/postman/postman.html#postman-applications",
    "title": "What’s Postman",
    "section": "Postman Applications",
    "text": "Postman Applications\nPostman is available as a desktop application and as a web platform. While both offer powerful features, the combination of the two provides the best experience:\n\nThe desktop app handles sensitive operations such as managing secrets and authentication tokens locally.\nThe web interface supports real-time collaboration, making it ideal for team-based API design, testing, and documentation.\n\nWhen used together, Postman offers a seamless way to experiment with, document, and share API workflows, and without writing a single line of code.\n\n\n\n\n\n\nWarningWhy secrets and credentials are not stored in the web interface\n\n\n\n\n\nThis is largely a design decision, but it’s also an important security feature. By keeping secrets and credentials on the desktop client, rather than the web interface, Postman ensures that each user interacts with an API using their own credentials, rather than sharing a common set of keys.\nThis approach enhances security and allows for granular control over user actions, enabling organizations to manage permissions more effectively and reduce the risk of unauthorized access.",
    "crumbs": [
      "Using Postman",
      "Postman"
    ]
  },
  {
    "objectID": "chapters/postman/postman.html#do-i-have-to-use-postman-to-work-with-apis",
    "href": "chapters/postman/postman.html#do-i-have-to-use-postman-to-work-with-apis",
    "title": "What’s Postman",
    "section": "Do I Have to Use Postman to Work with APIs?",
    "text": "Do I Have to Use Postman to Work with APIs?\nPostman is a popular, user-friendly tool, but it’s not the only way to interact with APIs. In fact, open-source alternatives like Bruno and HTTPie may be better suited for some users, especially those who prefer tools that don’t rely on web synchronization or who want more control over their workflows.\nWe’ve chosen Postman for this workshop because of its flexibility, feature set, and beginner-friendly interface. However, the core concepts and workflows (such as sending requests, handling authentication, and organizing collections) are largely shared across platforms. Once you understand how APIs work, you’ll be able to adapt to other tools with ease.",
    "crumbs": [
      "Using Postman",
      "Postman"
    ]
  },
  {
    "objectID": "chapters/postman/postman-environment.html",
    "href": "chapters/postman/postman-environment.html",
    "title": "Managing Environment Variables with Postman",
    "section": "",
    "text": "Now that we’ve learned how to use collections in Postman to send requests to an API, it’s time to explore environment variables.\nAs Postman defines it:\nEach environment variable stores a value. For example, you might create a variable named url with the value https://api.thecatapi.com/v1. You can then use {url} in your request instead of typing the full URL each time.\nEnvironment variables can also store sensitive information, such as passwords or API tokens, which need to be included in requests securely.",
    "crumbs": [
      "Using Postman",
      "Environment Variables"
    ]
  },
  {
    "objectID": "chapters/postman/postman-environment.html#setting-up-an-environment",
    "href": "chapters/postman/postman-environment.html#setting-up-an-environment",
    "title": "Managing Environment Variables with Postman",
    "section": "Setting Up an Environment",
    "text": "Setting Up an Environment\nTo set up an environment in Postman, follow these steps:\n\nOpen Postman and click the Environments tab in the left sidebar.\nClick the + New Environment button.\nGive your environment a name — for example: CatAPI.\n\n\n\nCreating a Default Variable\nYou’ll see a new window where you can add your variables. Let’s begin by setting a default variable for the base URL.\nFill out the following fields:\n\nVariable: url\n\nType: default\n\nInitial value: https://api.thecatapi.com/v1\n\nCurrent value: (filled automatically)\n\nClick the checkbox to activate the variable, then click Save in the upper-right corner.\n\n\n\nPostman default variable\n\n\n\n\n\nCreating a Secret Variable\nNow let’s add a secret variable to store your API token.\nFirst, sign up for an API key at https://thecatapi.com/signup.\nYou’ll receive an email within a few seconds containing a token that looks something like this:\nlive_Wi95wPeQHTsi0d4jH91jA51wjATQANFz7pqJpnQPrr2YpJ7T9sFJuyHYQXz5MMem\n(This is a fake example — never share your real API tokens publicly.)\nOnce you have your token, add it to the same environment you created earlier:\n\nVariable: api_token\n\nType: secret\n\nInitial value: your_api_token_here\n\nCurrent value: (filled automatically)\n\nCheck the box to activate the variable, then click Save.\n\n\n\nPostman secret variable",
    "crumbs": [
      "Using Postman",
      "Environment Variables"
    ]
  },
  {
    "objectID": "chapters/postman/postman-environment.html#using-variables-in-a-request",
    "href": "chapters/postman/postman-environment.html#using-variables-in-a-request",
    "title": "Managing Environment Variables with Postman",
    "section": "Using Variables in a Request",
    "text": "Using Variables in a Request\nNow that we’ve saved our variables, we can include them in a request. Let’s go back to the Collections tab and open the List Breeds request.\nTo use your environment variables, you first need to activate the CatAPI environment. In the top-right corner of Postman, you’ll see a button labeled “No Environment”. Click it and select CatAPI from the dropdown menu:\n\nNow, go to the request URL field. Carefully remove https://api.thecatapi.com/v1 from the beginning — but be sure to leave the trailing slash in /breeds intact.\nType { in the field, and you’ll see a dropdown menu showing available variables from the CatAPI environment, along with global variables provided by Postman. If you hover over a variable name, Postman will display its value.\n\nSelect the url variable. Your request should now look like this:\n{{url}}/breeds\nTo confirm that everything is working correctly, click Send again. You should see the same response as before — now powered by your environment variable!\n\n\n\nPostman - Request with default variable",
    "crumbs": [
      "Using Postman",
      "Environment Variables"
    ]
  },
  {
    "objectID": "chapters/postman/postman-environment.html#using-secrets-in-a-request",
    "href": "chapters/postman/postman-environment.html#using-secrets-in-a-request",
    "title": "Managing Environment Variables with Postman",
    "section": "Using Secrets in a Request",
    "text": "Using Secrets in a Request\nThe most common way to send authenticated requests is by including a token in the request headers. Let’s try that using our Search Cat Images request.\nFirst, replace the URL with your {{url}} default variable, just like we did in the previous exercise.\nNext, click on the Headers tab, located just below the URL field. You’ll see a table where you can add header keys, values, and optional descriptions. HTTP headers are metadata that provide additional context about the request and help the server prepare an appropriate response.\nTo learn how to include the API token, let’s refer to the Cat API documentation:\n\nTo get more than 10, and additional fields then be sure to use your API Key from the welcome email as the ‘x-api-key’ header, or ?api_key= query string parameter to access all the images and data.\n\n\n\n\n\n\n\nYou may have noticed that it’s also possible to send the API key as a query parameter in the URL. While this works, it’s not recommended. Even though Postman keeps the token secret in your environment, placing it in the URL exposes it in the browser or server logs when the request is made.\n\n\n\nTo add the header, use the following values:\n\nKey: x-api-key\n\nValue: {{api_token}}\n\nNow run the request again. You’ll see that the limit=3 parameter works as expected — and you can even increase it (e.g., to 30) without any issue. Because the request is now authenticated, the API allows access to features that aren’t available to unauthenticated users.",
    "crumbs": [
      "Using Postman",
      "Environment Variables"
    ]
  },
  {
    "objectID": "chapters/postman/postman-environment.html#wrapping-up",
    "href": "chapters/postman/postman-environment.html#wrapping-up",
    "title": "Managing Environment Variables with Postman",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nIn this section, we’ve learned how to use environment variables and secrets in Postman to simplify and secure our requests. By storing values like the base URL and API token in a reusable environment, we can write cleaner, safer, and more flexible requests.\nWe also explored how to pass secrets through request headers, a common and recommended practice when working with APIs that require authentication.\nIn the next section, we’ll go a step further and explore how to use POST and DELETE methods with The Cat API to interact with user-specific data, such as favorites or image votes.\nAfter that, we’ll begin working with the GitHub API, where we’ll apply all these concepts in a more complex and widely used platform.",
    "crumbs": [
      "Using Postman",
      "Environment Variables"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api.html",
    "href": "chapters/postman/exercise-github-api.html",
    "title": "Working with the GitHub API",
    "section": "",
    "text": "The GitHub API is a powerful tool for developers, researchers, and data analysts. It allows you to programmatically interact with almost every feature available on GitHub’s website — including repositories, issues, users, and more.\nIn this section, we’ll explore a curated set of endpoints from the GitHub REST API using Postman. We’ll build on what we learned in previous chapters: setting headers, using variables, and making GET, POST, PATCH, and DELETE requests.\nThe only requirement to access most of GitHub’s API is an authorization token, which you can generate in your GitHub settings.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Working with the GitHub API"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api.html#getting-your-github-authorization-token",
    "href": "chapters/postman/exercise-github-api.html#getting-your-github-authorization-token",
    "title": "Working with the GitHub API",
    "section": "Getting Your GitHub Authorization Token",
    "text": "Getting Your GitHub Authorization Token\nTo access private endpoints or perform actions like creating issues or repositories, GitHub requires an authorization token. This token acts like a password for your API requests, so keep it secret and secure.\nHere’s how to generate one:\n\nGo to GitHub and log into your account.\nNavigate to your Developer settings:\n\n\nClick your profile photo (top right) → Settings\nScroll down to Developer settings\n\n\nIn the left sidebar, select Personal access tokens → then Tokens (classic)\nClick the Generate new token (classic) button.\nFill out the form:\n\n\nNote: Give your token a descriptive name, e.g. Postman Workshop Token\nExpiration: Choose any short duration (e.g. 7 or 30 days)\nScopes:\n\nCheck the box for repo → this includes issues, pull requests, and repo access\nYou can also select read:user and user:email if you want user info\n\n\n\nClick Generate token\n\n\n\n\n\n\n\nGitHub will only show the token once. Copy it immediately and save it somewhere secure (e.g., directly into Postman).\n\n\n\n\n\n\nGithub Token Form\n\n\n\nAdd It to Postman\n\nCreate a new environment named GitHub Env.\nAdd a secret variable:\n\n\nVariable: gh_token\nType: secret\nInitial value: (paste your token here)\nCurrent value: will be filled automatically\n\n\nSave the environment.\n\nYou’ll now be able to authorize your GitHub requests by including this header in each request:\nAuthorization: Bearer {{gh_token}}\nPostman will automatically substitute {gh_token} with your real token, keeping it secure.\nThis is also a good moment to set a default variable for the GitHub base URL.\nWe’ll use the variable gh_URL to store: https://api.github.com.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Working with the GitHub API"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api.html#testing-the-authorization-token",
    "href": "chapters/postman/exercise-github-api.html#testing-the-authorization-token",
    "title": "Working with the GitHub API",
    "section": "Testing the Authorization Token",
    "text": "Testing the Authorization Token\nLet’s confirm that your GitHub token is working correctly by sending a simple authenticated GET request.\nWe’ll use this endpoint:\nGET {{gh_URL}}/user\nThis returns information about the authenticated user — meaning: you!\n\nSteps\n\nIn your GitHub collection in Postman, create a new request called “Get Authenticated User”.\nSet the method to GET.\nIn the URL field, enter: {gh_URL}/user\nIn the Headers tab, add:\n\n\nKey: Authorization\nValue: Bearer {{gh_token}}\n(Don’t forget to include Bearer before the token!)\n\n\nMake sure the GitHub environment is selected.\nClick Send.\n\nIf everything is working, you’ll get a response that includes your GitHub user information:\n\nlogin (your GitHub username)\nid\npublic_repos\ncreated_at, etc.\n\n\n\n\nExample of a successful GitHub user response in Postman\n\n\nIf you receive a 401 Unauthorized error, double-check:\n\nYour token is correct and hasn’t expired\nYou added Bearer before {{gh_token}}\nThe header name is exactly: Authorization",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Working with the GitHub API"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api.html#ready-to-explore-more",
    "href": "chapters/postman/exercise-github-api.html#ready-to-explore-more",
    "title": "Working with the GitHub API",
    "section": "Ready to Explore More?",
    "text": "Ready to Explore More?\nWith your environment set up and your authentication tested, you’re now ready to explore a wide range of endpoints in the GitHub API.\nFrom retrieving repository data to creating issues or managing user activity, the GitHub API gives you programmatic access to one of the world’s largest collaborative platforms, and you’ve already taken the most important step: connecting securely.\nIn the next section, we’ll explore a few real-world use cases with the GitHub API to practice sending GET, POST, PATCH, and DEL requests.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Working with the GitHub API"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Unlocking Data",
    "section": "",
    "text": "This workbook accompanies the workshop series Unlocking Data: A Gentle Introduction to APIs.\nThese workshops are designed for people with no programming experience who want to start working with APIs, particularly web APIs. While developed with humanities researchers in mind, the content is equally valuable for information professionals in archives and libraries, as well as anyone beginning their journey into the world of APIs.\nThese workshops are designed to be self-taught, allowing you to learn at your own pace without installing any specific software. We’ve included interactive examples so you can practice as you learn. We also offer in-person workshops to guide participants through the basics and provide hands-on support.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#what-youll-learn",
    "href": "index.html#what-youll-learn",
    "title": "Welcome to Unlocking Data",
    "section": "What You’ll Learn",
    "text": "What You’ll Learn\n\n\n\n No Coding Required\nStart with zero programming experience. Learn API fundamentals through interactive examples in your browser.\n\n\n\n\n Practical Tools\nMaster Postman (and open-source alternatives like Bruno) for API testing, then progress to Python for automation.\n\n\n\n\n Real-World Data\nWork with cultural heritage APIs from institutions like the Metropolitan Museum, the Digital Public Library of America, and more.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "index.html#workshop-series",
    "href": "index.html#workshop-series",
    "title": "Welcome to Unlocking Data",
    "section": "Workshop Series",
    "text": "Workshop Series\nEach workshop in this series can be followed independently, depending on your interests, previous knowledge, and goals. Each workshop includes guided explanations of how to interact with different API providers, followed by hands-on exercises to practice the concepts.\n\n\n\n\n\n\n\n\nWhat are APIs?\n\n\nLearn the fundamentals of APIs, what they are, and why they exist.\n\n\nStart Learning\n\n\n\n\n\n\n\n\n\n\n\nInteracting with APIs\n\n\nDiscover how to interact with APIs using endpoints, request methods, and practical exercises.\n\n\nExplore\n\n\n\n\n\n\n\n\n\n\n\nUsing Postman\n\n\nMaster API testing with Postman, including GET, POST, and DELETE requests with hands-on GitHub API exercises.\n\n\nGet Started\n\n\n\n\n\n\n\n\n\n\n\nProgrammatic Access\n\n\nLearn how to interact with APIs programmatically using Python for automated data collection.\n\n\nCode\n\n\n\n\n\n\n\n\n\n\n\nData Analysis\n\n\nApply your API knowledge to a complete research project: collect and analyze Medieval Art metadata from the Met Museum to uncover thematic patterns.\n\n\nAnalyze\n\n\n\n\n\n\n\n\n\n\n\nAPI Catalog\n\n\nExplore APIs from major cultural institutions including the Metropolitan Museum, Library of Congress, Smithsonian, and international collections like Europeana and Rijksmuseum.\n\n\nBrowse\n\n\n\n\n\n\nLet’s get started! 🚀",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "CHANGELOG.html",
    "href": "CHANGELOG.html",
    "title": "Changelog",
    "section": "",
    "text": "All notable changes to this project will be documented in this file.\nThe format is based on Keep a Changelog,\nand this project adheres to Semantic Versioning.\n\n\n\n\n\n\nRequest Methods episode to Interacting with APIs chapter.\nIntroduction to APIs using Postman and The Cat API\nHands-on chapters with GET, POST, PATCH, DELETE requests\nEnvironment variable setup with secrets\nNew section on working with the GitHub REST API, including:\n\nEnvironment variable setup (gh_token, gh_URL, owner, repo)\nToken-based authentication via headers\nInteractive exercises for:\n\nRetrieving user info (GET /user)\nSearching repositories (GET /search/repositories)\nCreating, updating, listing, and unlocking issues (POST, PATCH, GET, DELETE)\n\n\nNew instructional content on reading and translating GitHub API documentation into Postman requests\nExample visuals for Postman interface, GitHub responses, and Octocat endpoint\n\n\n\n\n\nClarified explanations and formatting in earlier chapters (e.g. Cat API section)\nImproved consistency in variable usage (url, gh_URL, api_token, etc.)\n\n\n\n\n\nMinor typos and grammatical improvements throughout\nCorrected outdated references in Postman version and screenshots\n\n\n\n\n\n\n\n\n\nMinor typo corrections and formatting tweaks for clarity\n\n\n\n\n\n\n\n\nInitial public release\nWhat are APIs and why do they exists\nInteracting with APIs (GET requests) using the Web interface.\nAPIs for Data Analysis\nA Catalog of APIs"
  },
  {
    "objectID": "CHANGELOG.html#section",
    "href": "CHANGELOG.html#section",
    "title": "Changelog",
    "section": "",
    "text": "Request Methods episode to Interacting with APIs chapter.\nIntroduction to APIs using Postman and The Cat API\nHands-on chapters with GET, POST, PATCH, DELETE requests\nEnvironment variable setup with secrets\nNew section on working with the GitHub REST API, including:\n\nEnvironment variable setup (gh_token, gh_URL, owner, repo)\nToken-based authentication via headers\nInteractive exercises for:\n\nRetrieving user info (GET /user)\nSearching repositories (GET /search/repositories)\nCreating, updating, listing, and unlocking issues (POST, PATCH, GET, DELETE)\n\n\nNew instructional content on reading and translating GitHub API documentation into Postman requests\nExample visuals for Postman interface, GitHub responses, and Octocat endpoint\n\n\n\n\n\nClarified explanations and formatting in earlier chapters (e.g. Cat API section)\nImproved consistency in variable usage (url, gh_URL, api_token, etc.)\n\n\n\n\n\nMinor typos and grammatical improvements throughout\nCorrected outdated references in Postman version and screenshots"
  },
  {
    "objectID": "CHANGELOG.html#section-1",
    "href": "CHANGELOG.html#section-1",
    "title": "Changelog",
    "section": "",
    "text": "Minor typo corrections and formatting tweaks for clarity"
  },
  {
    "objectID": "CHANGELOG.html#section-2",
    "href": "CHANGELOG.html#section-2",
    "title": "Changelog",
    "section": "",
    "text": "Initial public release\nWhat are APIs and why do they exists\nInteracting with APIs (GET requests) using the Web interface.\nAPIs for Data Analysis\nA Catalog of APIs"
  },
  {
    "objectID": "resources/catalog.html",
    "href": "resources/catalog.html",
    "title": "A Catalog of APIs",
    "section": "",
    "text": "This is a non-exhaustive list of APIs that can be used to retrieve data from cultural institutions, and other sources, that can be used for research, teaching, or event to build or integrate with applications.",
    "crumbs": [
      "A Catalog of APIs"
    ]
  },
  {
    "objectID": "resources/catalog.html#us-based-apis",
    "href": "resources/catalog.html#us-based-apis",
    "title": "A Catalog of APIs",
    "section": "US Based APIs",
    "text": "US Based APIs\n\n\n\nAPI\nVersion\nDescription\nLink to Documentation\nRequires Authentication\nStatus of Documentation\n\n\n\n\nInternet Archive\nN/A\nAPI for searching and retrieving data from public collections from the Internet Archive\nIA Tools and APIs\nYes\nWell documented with examples and code snippets\n\n\nLibrary of Congress\nN/A\nProvides structured data about Library of Congress collections\nAPIs for LoC.gov\nNo\nWell documented. Some collections have their own documentation. See Additional APIs\n\n\nGetty Museum\nN/A\nAPI designed for tasks like getting records, tracking changes, and querying the collection\nGetty API Documentation\nNo\n“Work in progress”\n\n\nMetropolitan Museum of Art\nv1\nProvides access to Open Access data and public domain high-resolution images\nThe Met Collection API\nNo\nWell documented with examples and code snippets\n\n\nArt Institute of Chicago\nv1\nREST-style service to explore and integrate the museum’s public data\nArt Institute of Chicago API\nNo\nWell documented with examples and code snippets\n\n\nHarvard Art Museums\nN/A\nREST-style service for integrating museum collections in projects\nHarvard Art Museums API\nYes\nDetailed and well-documented. See GitHub Docs\n\n\nThe Cleveland Museum of Art\n4.0.0\nProvides access to the museum’s collection data in JSON format\nCleveland Museum of Art API\nNo\nWell documented but not very user-friendly\n\n\nSmithsonian Institution\nN/A\nOpen access to Smithsonian collections and research datasets\nSmithsonian API\nNo\nComprehensive documentation with examples\n\n\nHathiTrust\nN/A\nAPI for accessing and managing HathiTrust collections and data\nHathiTrust Data API\nYes\nLimited and somewhat outdated documentation\n\n\nWalters Art Museum Collections\nN/A\nAccess to Walters Art Museum’s public data and metadata\nWalters API\nNo\nLimited documentation available on GitHub\n\n\nNational Archives Catalog\nv2.0\nAccess to the National Archives collection metadata\nNational Archives API\nNo\nWell documented with examples\n\n\nThe New York Times Archive\nv3\nProvides access to NYT articles, archives, and multimedia\nNYT APIs\nYes\nComprehensive documentation with examples\n\n\nDigital Public Library of America\nN/A\nProvides access to metadata and content from partner libraries\nDPLA API Codex\nYes\nDetailed documentation available",
    "crumbs": [
      "A Catalog of APIs"
    ]
  },
  {
    "objectID": "resources/catalog.html#non-us-apis",
    "href": "resources/catalog.html#non-us-apis",
    "title": "A Catalog of APIs",
    "section": "Non-US APIs",
    "text": "Non-US APIs\n\n\n\n\n\n\nPolicies for using APIs from non-US institutions may be different from those in the US. It is important to check the API documentation for each institution to be aware of usage limits and other restrictions.\n\n\n\n\n\n\nAPI\nVersion\nDescription\nLink to Documentation\nRequires Authentication\nStatus of Documentation\n\n\n\n\nEuropeana\nv2.0\nAPI to search and retrieve European cultural heritage metadata\nEuropeana API\nYes\nComprehensive documentation with code examples\n\n\nRijksmuseum\nN/A\nProvides access to Rijksmuseum collection data and images\nRijksmuseum API\nYes\nModerately documented with examples\n\n\nV&A Collections\nN/A\nAPI for accessing the Victoria and Albert Museum’s collection data and images\nV&A API\nYes\nWell-documented with examples\n\n\nScience Museum Collections\nN/A\nAPI for accessing the Science Museum’s collection data and images\nScience Museum API\nYes\nWell documented but not very user-friendly",
    "crumbs": [
      "A Catalog of APIs"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-2.html",
    "href": "chapters/postman/exercise-github-api-2.html",
    "title": "Exercise 2: Working with Issues in the GitHub API",
    "section": "",
    "text": "Now it’s time to interact with one of the most common features of GitHub: issues. You’ll create, update, list, and (sort of) delete an issue — all using the GitHub API.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 2: Adding and Removing Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-2.html#preparing-your-repository",
    "href": "chapters/postman/exercise-github-api-2.html#preparing-your-repository",
    "title": "Exercise 2: Working with Issues in the GitHub API",
    "section": "Preparing Your Repository",
    "text": "Preparing Your Repository\nTo complete this exercise, you’ll need a repository that you own (not one you just contribute to). If you don’t already have one, go to GitHub and create a new, empty repository.\nYou can name it something like api-test-repo or postman-playground.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 2: Adding and Removing Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-2.html#setting-environment-variables",
    "href": "chapters/postman/exercise-github-api-2.html#setting-environment-variables",
    "title": "Exercise 2: Working with Issues in the GitHub API",
    "section": "Setting Environment Variables",
    "text": "Setting Environment Variables\nLet’s make this exercise easier to repeat and reuse. In your GitHub Postman environment, add two default variables:\n\nowner → your GitHub username\n\nrepo → the name of your target repository\n\nNow you can reuse them like this:\n{gh_URL}/repos/{{owner}}/{{repo}}/issues",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 2: Adding and Removing Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-2.html#create-a-new-issue",
    "href": "chapters/postman/exercise-github-api-2.html#create-a-new-issue",
    "title": "Exercise 2: Working with Issues in the GitHub API",
    "section": "Create a New Issue",
    "text": "Create a New Issue\n\nMethod: POST\n\nEndpoint:\n{{gh_URL}}/repos/{{owner}}/{{repo}}/issues\nHeaders:\n\nAuthorization: Bearer {{gh_token}}\naccept: application/vnd.github+json\n\nBody (raw, JSON):\n{\n\"title\": \"API-created issue\",\n\"body\": \"This issue was created using Postman!\"\n}\n\nClick Send. If successful, you’ll receive a 201 Created response with the new issue’s details.\n📌 Save the number of the issue from the response — you’ll need it in the next step.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 2: Adding and Removing Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-2.html#update-the-issue",
    "href": "chapters/postman/exercise-github-api-2.html#update-the-issue",
    "title": "Exercise 2: Working with Issues in the GitHub API",
    "section": "Update the Issue",
    "text": "Update the Issue\n\nMethod: PATCH\nEndpoint:\n{{gh_URL}}/repos/{{owner}}/{{repo}}/issues/{issue_number}\nReplace {issue_number} with the number from Step 1.\nHeaders: same as before\nBody (raw, JSON):\n{\n\"title\": \"Updated title from Postman\",\n\"body\": \"This issue has been updated via PATCH request!\"\n}\n\nYou should receive a 200 OK response with the updated issue content.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 2: Adding and Removing Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-2.html#list-issues-in-your-repository",
    "href": "chapters/postman/exercise-github-api-2.html#list-issues-in-your-repository",
    "title": "Exercise 2: Working with Issues in the GitHub API",
    "section": "List Issues in Your Repository",
    "text": "List Issues in Your Repository\n\nMethod: GET\nEndpoint:\n{{gh_URL}}/repos/{{owner}}/{{repo}}/issues\n\nThis request returns all open issues in your repository, including the one you just created.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 2: Adding and Removing Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-2.html#delete-the-issue-unlock",
    "href": "chapters/postman/exercise-github-api-2.html#delete-the-issue-unlock",
    "title": "Exercise 2: Working with Issues in the GitHub API",
    "section": "“Delete” the Issue (Unlock)",
    "text": "“Delete” the Issue (Unlock)\nGitHub doesn’t allow true issue deletion via the API. However, you can simulate deletion in two ways: - Close the issue (optional) - Unlock it — which removes any restriction on further edits\nLet’s unlock it:\n\nMethod: DELETE\nEndpoint:\n{{gh_URL}}/repos/{{owner}}/{{repo}}/issues/{issue_number}/lock\n\nIf successful, you’ll receive a 204 No Content response.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 2: Adding and Removing Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/exercise-github-api-2.html#summary",
    "href": "chapters/postman/exercise-github-api-2.html#summary",
    "title": "Exercise 2: Working with Issues in the GitHub API",
    "section": "Summary",
    "text": "Summary\nBy completing these steps, you’ve now:\n\nCreated and updated GitHub content using POST and PATCH\nRetrieved structured issue data with GET\nPerformed a partial deletion (unlock) with DELETE\n\nYou’re now ready to apply what you’ve learned to real project workflows, automation tasks, or your next API-powered integration.",
    "crumbs": [
      "Using Postman",
      "Exercise: The GitHub API",
      "Exercise 2: Adding and Removing Information from GitHub"
    ]
  },
  {
    "objectID": "chapters/postman/postman-post.html",
    "href": "chapters/postman/postman-post.html",
    "title": "POST Requests in Postman",
    "section": "",
    "text": "One of the key advantages of using APIs is the ability to perform actions that modify data on a service — and to do so programmatically, without needing a graphical user interface.\nThis is especially useful when you’re:\nIn this section, we’ll explore how to use the POST method to send data to a server, using features from The Cat API such as adding favorite images.",
    "crumbs": [
      "Using Postman",
      "POST Requests in Postman"
    ]
  },
  {
    "objectID": "chapters/postman/postman-post.html#uploading-your-first-image",
    "href": "chapters/postman/postman-post.html#uploading-your-first-image",
    "title": "POST Requests in Postman",
    "section": "Uploading Your First Image",
    "text": "Uploading Your First Image\nSince we have an API key, we also have a user ID assigned. That means we can upload images to our “own” collection.\n\n\n\n\n\n\nKeep in mind that The Cat API is primarily a testing platform. It’s not intended to store personal information long-term.\nUploading pictures of your cats is totally fine, just don’t use it as a personal photo album! 😉\n\n\n\nLet’s test this by creating a new GET request called “myImages”.\n\nAdd it to your CatAPI collection.\nMake sure the CatAPI environment is selected.\nIn the URL field, enter:\n\n{{url}}/images/\nIf you run this request, you might see an error message like this:\nAUTHENTICATION_ERROR - the header \"x-api-key\" is invalid, check it then try again\nThis happens because we haven’t included our api_token secret in the request headers. Once you add the header (x-api-key: {{api_token}}), run the request again — you should now get an empty array in the response.\nThat means your personal image collection is accessible — and ready for uploads!\n\n\n\nPOSTMAN images collection\n\n\n\nWriting the POST Request\nTo understand how to upload an image to The Cat API using a POST request, let’s refer to their documentation ↗️.\nOnce there, navigate to the Images section and locate:\nPOST /images/upload\nThis page provides useful information for constructing our request. Focus on the “Request Body Schema” section. It lists three parameters:\n\n\n\n\n\n\n\n\nParameter\nType\nDescription\n\n\n\n\nfile\nstring &lt;binary&gt;\nThe binary file data of the image to be uploaded.\n\n\nsub_id\nstring (optional)\nA string used to segment your images (e.g., identify the user).\n\n\nbreeds_ids\nstring (optional)\nComma-separated list of breed IDs shown in the image.\n\n\n\nTo keep things simple, we’ll just use the required file parameter. As noted, it accepts a string of type binary, which refers to the actual image file you want to upload.\nLet’s walk through how to set up this POST request in Postman.\n\n\nIn your collection, create a new request and name it “Upload Cat Image”.\nChange the request type from GET to POST.\nEnsure the CatAPI environment is selected.\nIn the URL field, enter:\n\n{{url}}/images/upload\n\nAdd the API key\n\nGo to the Headers tab and add the following:\n\n\nKey: x-api-key\nValue: {{api_token}}\n\n\n\nConfigure the request body\n\nClick the Body tab. You’ll see several content type options:\n\n\n\n\n\n\n\n\nType\nDescription\n\n\n\n\nnone\nNo data will be sent with the request.\n\n\nform-data\nSimulates form submissions (key, data type, and value).\n\n\nx-www-form-urlencoded\nSends key-value pairs in URL-encoded format (similar to form-data).\n\n\nraw\nUsed to send raw text (typically JSON or XML).\n\n\nbinary\nUsed to send files like images, video, or audio.\n\n\nGraphQL\nFor structured GraphQL queries to compatible APIs.\n\n\n\nWe’ll use form-data for this request — even though the image is binary — as indicated in The Cat API documentation.\n\n\n\nSending Binary Content\nPostman makes it easy to upload files as part of a request. When using the desktop application, you can simply select the image file from your computer, and Postman will attach it as binary content in the body of the request.\n\n\n\n\n\n\nRemember that this is a public testing API. If you’re uploading a picture of your cat, make sure not to include any people, addresses, or personal information in the image or filename.\n\n\n\nNow, this part can be a little confusing: although the file itself is binary, we still need to use the form-data option — not the binary option.\n\n\n\n\n\n\nNoteWhy use form-data instead of binary?\n\n\n\n\n\nThe reason is that the API expects the image to be sent as part of a multipart/form-data request — just like a file upload in a browser form. The binary mode in Postman sends only the file content, without a field name or other metadata. But the Cat API expects a form-like structure with the field named file.\nIn simple terms: if we send the data as binary, the server won’t know what to do with it.\n\n\n\nLet’s attach our image by selecting form-data from the Body menu. Then, add the following:\n\nKey: file\n\nFrom the dropdown menu next to the key, select File instead of Text.\n\nValue: Click + Select File and choose a file from your local machine.\n\nYou should see something like this:\n\n\n\nPOSTMAN Body file-form attached file\n\n\nNow you can send the request. If it’s successful, you’ll see a response like this:\n\n\n\nPOSTMAN POST request success message\n\n\nThe warning icon before the filename and the cloud upload symbol are optional indicators — useful if you’re working in a shared workspace and want to sync files with your team.\nOnce uploaded, you can access the image directly from The Cat API using the ID it returns — in this case, 8ML9aFY8p.\n\n\nCode\nasync function getCatData(catId) {\n  const url = `https://api.thecatapi.com/v1/images/${catId}`;\n  const response = await fetch(url);\n  if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);\n  const data = await response.json();\n  return data;\n}\n\ncatData = await getCatData(\"8ML9aFY8p\")\n\nviewof catDataString = {\n  const pre = html`&lt;pre style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;\"&gt;`;\n  const code = html`&lt;code style=\"color: #333;\"&gt;`;\n  \n  // Formatted JSON with syntax highlighting\n  const formatted = JSON.stringify(catData, null, 2)\n  .replace(/&/g, '&amp;')\n  .replace(/&lt;/g, '&lt;')\n  .replace(/&gt;/g, '&gt;')\n  .replace(/\"([^\"]+)\":/g, '&lt;span style=\"color: #a31515;\"&gt;\"$1\"&lt;/span&gt;:') \n  .replace(/: \"([^\"]+)\"/g, ': &lt;span style=\"color: #008000;\"&gt;\"$1\"&lt;/span&gt;')  \n  .replace(/: ([0-9]+)/g, ': &lt;span style=\"color: #0000FF;\"&gt;$1&lt;/span&gt;')  \n  .replace(/\\b(true|false|null)\\b/g, '&lt;span style=\"color: #FF0000;\"&gt;$1&lt;/span&gt;');\n  \n  code.innerHTML = formatted;\n  pre.appendChild(code);\n  return pre;\n}\n\nfunction displayCat(data) {\n  const container = html`&lt;div class=\"card\" style=\"max-width: 300px; margin: 15px auto;\"&gt;\n    &lt;img src=\"${data.url}\" class=\"card-img-top\" alt=\"Silvestre\"&gt;\n    &lt;div class=\"card-body\"&gt;\n    &lt;h5 class=\"card-title\"&gt;Let me introduce you to Silvestre :)&lt;/h5&gt;\n    &lt;/div&gt;\n  &lt;/div&gt;`;\n  return container;\n}\n\ndisplayCat(catData);",
    "crumbs": [
      "Using Postman",
      "POST Requests in Postman"
    ]
  },
  {
    "objectID": "chapters/postman/postman-post.html#what-about-other-types-of-data",
    "href": "chapters/postman/postman-post.html#what-about-other-types-of-data",
    "title": "POST Requests in Postman",
    "section": "What About Other Types of Data?",
    "text": "What About Other Types of Data?\nIn this example, we only explored one type of request body — form-data. But APIs can require many different formats, and it’s up to the API designers to define how they expect data to be sent.\nThat’s why reading the documentation is essential. To know how to send a POST (or any other type of request), you first need to understand how the API expects the request to be structured — including parameters, headers, and body format.\nIn the next section, we’ll explore how to work with DELETE requests, and then move on to the more sophisticated GitHub REST API.",
    "crumbs": [
      "Using Postman",
      "POST Requests in Postman"
    ]
  },
  {
    "objectID": "chapters/postman/postman-delete.html",
    "href": "chapters/postman/postman-delete.html",
    "title": "DELETE Requests in Postman",
    "section": "",
    "text": "Now that we’ve learned how to send information to a server using an API endpoint, let’s explore how to delete content from it using a DELETE request.\nTo avoid deleting the image we uploaded earlier, we’ll start this section by uploading a new image — this one of a very festive (and slightly grumpy) cat:\nHere’s the response returned by the API when the image was uploaded:\nNow we’re ready to delete that image from our personal collection using its ID.",
    "crumbs": [
      "Using Postman",
      "DELETE Requests in Postman"
    ]
  },
  {
    "objectID": "chapters/postman/postman-delete.html#delete-request",
    "href": "chapters/postman/postman-delete.html#delete-request",
    "title": "DELETE Requests in Postman",
    "section": "DELETE Request",
    "text": "DELETE Request\nTo understand how to use the DELETE request in the context of The Cat API, let’s return to the documentation — specifically to:\nDEL /images/:image_id\nAccording to the docs, this request requires one path parameter:\n\nimage_id: string. Example: dMsUj1-nz\n\nAnd, as usual, two header parameters — including our already familiar x-api-key: {{api_token}}.\n\nLet’s set up the request:\n\nCreate a new request in your CatAPI collection and name it “Delete Cat Image”.\nChange the request type to DELETE.\nMake sure the CatAPI environment is selected.\nIn the URL field, enter the following base:\n\n{{url}}/images\n\nCopy the id value from your last successful image upload. In my case, it’s xsOtklMbs.\nAppend the ID to the URL like this:\n\n{{url}}/images/xsOtklMbs\n\nGo to the Headers tab and add your authentication header:\n\n\nKey: x-api-key\n\nValue: {api_token}\n\nNow you’re ready to send the request!\n\n\n\n\n\n\nYou won’t receive a confirmation or warning before deletion, so make sure you’re removing the image you intended to.\n\n\n\nIf the request is successful, you’ll receive a 204 No Content status — this means the image was deleted.\n\n\n\nPOSTMAN delete request\n\n\n\nAnd that’s it! ✅\nYou may have noticed that the endpoint used to GET or DELETE a record is the same:\n{{url}}/images/{image_id}\nThat’s one of the core principles of REST APIs:\nEach data entity — like images — is addressed by a consistent URL, while the HTTP method (GET, POST, DELETE, etc.) defines what action is performed on it.\n\nThe Cat API is a user-friendly tool for getting familiar with the basic concepts of REST APIs. It also helped us explore some key features of Postman, such as environments, headers, and request bodies.\nAs mentioned earlier, APIs can range from very simple to quite complex. The GitHub API falls somewhere in the middle. It’s highly comprehensive, but you can interact with many of its endpoints using just an authorization token.\nIn the next section, we’ll explore some of those endpoints through a series of hands-on exercises — introducing you to the use of APIs in a more “real-world” context.",
    "crumbs": [
      "Using Postman",
      "DELETE Requests in Postman"
    ]
  },
  {
    "objectID": "chapters/postman/postman-basic-requests.html",
    "href": "chapters/postman/postman-basic-requests.html",
    "title": "Basic Requests using Postman",
    "section": "",
    "text": "In this section, we’ll create a workspace in Postman and perform a few basic GET requests using The Cat API.",
    "crumbs": [
      "Using Postman",
      "Basic Request"
    ]
  },
  {
    "objectID": "chapters/postman/postman-basic-requests.html#starting-with-a-workspace",
    "href": "chapters/postman/postman-basic-requests.html#starting-with-a-workspace",
    "title": "Basic Requests using Postman",
    "section": "Starting with a Workspace",
    "text": "Starting with a Workspace\nWorkspaces help keep your requests organized and make it easier to collaborate with others. To create a new workspace, go to Workspaces → Create Workspace and choose a Blank workspace.\nYou’ll be prompted to enter a name, select a workspace type, and manage access settings.\nFor this exercise, we’ll use the following setup:\n\nName: My Requests\nType: Internal workspace\nAccess: Keep the default — everyone in your team can access it\n\n\n\n\nWorkspace creation form in Postman\n\n\nIf you haven’t set up a team in Postman yet, you’ll be prompted to fill out a simple form to create one. Just keep it straightforward: choose any name for your team, select Other for the question “What will your team work on?”, and confirm.\nOnce your team is created, you’ll be ready to add some collections.",
    "crumbs": [
      "Using Postman",
      "Basic Request"
    ]
  },
  {
    "objectID": "chapters/postman/postman-basic-requests.html#creating-a-collection",
    "href": "chapters/postman/postman-basic-requests.html#creating-a-collection",
    "title": "Basic Requests using Postman",
    "section": "Creating a Collection",
    "text": "Creating a Collection\nOnce your workspace and team are set up, the next step is to create a collection. Collections are folders that help you group and organize your API requests. They’re especially useful when working with multiple endpoints from the same service.\nTo create a new collection:\n\nIn the left sidebar, click the Collections tab.\nClick the + Blank Collection button.\nGive your collection a name. For this exercise, you can name it “The Cat API”.\n(Optional) Add a description to explain what this collection is for.\nClick Create.\n\nYou’ll now see your new collection listed in the sidebar. We’ll add our first GET request to it in the next step.\n\n\n\nCreating a collection in Postman\n\n\n\n\n\n\n\n\nWhen you create a new workspace, Postman may automatically add a template collection named “End-to-End Tests”. You can safely remove it to keep your workspace clean and focused.\nTo delete it:\nClick the three dots (⋮) next to the collection name, then select Delete.",
    "crumbs": [
      "Using Postman",
      "Basic Request"
    ]
  },
  {
    "objectID": "chapters/postman/postman-basic-requests.html#adding-your-first-get-request",
    "href": "chapters/postman/postman-basic-requests.html#adding-your-first-get-request",
    "title": "Basic Requests using Postman",
    "section": "Adding Your First GET Request",
    "text": "Adding Your First GET Request\nNow that you’ve created your collection, it’s time to add a request. We’ll start by adding a basic GET request to The Cat API to retrieve a list of cat breeds.\nTo do this:\n\nClick the name of your collection (The Cat API) in the left sidebar.\nClick the three dots (⋮) next to the collection name, then select Add request.\nGive your request a name — for example, List Breeds.\nIn the Request type dropdown, make sure GET is selected.\nIn the URL field, paste the following endpoint:\n\nhttps://api.thecatapi.com/v1/breeds\n\nClick Save (upper right) and choose Save to “The Cat API” collection.\n\nYou’re ready to send your first request! Click the Send button.\nYou should see a 200 OK response along with a list of cat breeds in the response body.\n\n\n\nAdding a GET request in Postman",
    "crumbs": [
      "Using Postman",
      "Basic Request"
    ]
  },
  {
    "objectID": "chapters/postman/postman-basic-requests.html#using-query-parameters",
    "href": "chapters/postman/postman-basic-requests.html#using-query-parameters",
    "title": "Basic Requests using Postman",
    "section": "Using Query Parameters",
    "text": "Using Query Parameters\nMany APIs allow you to filter or customize your results using query parameters. These are added to the end of the URL after a ? symbol and follow a key=value format.\nLet’s try an example using The Cat API’s search endpoint to retrieve only a few cat images in GIF format.\n\nIn your The Cat API collection, add a new request.\nName the request: Search Cat Images\nSet the request type to GET.\nIn the URL field, paste the following:\n\nhttps://api.thecatapi.com/v1/images/search\n\nSwitch to the Params tab (below the URL field).\nAdd the following query parameters:\n\n\n\n\nKey\nValue\n\n\n\n\nmime_types\ngif\n\n\nlimit\n3\n\n\nsize\nsmall\n\n\n\n\nClick Send to execute the request.\n\nYou should see a list of cat image objects, each with a url field pointing to a different cat GIF.\n\n\n\nQuery parameters in Postman\n\n\n\nIf you look carefully at the response, you’ll notice that it returns 10 elements instead of the 3 we specified in the limit parameter. This is because that parameter only works for authenticated users.\nThe reason is simple: while you can request fewer items (like 3), you can also ask for much larger amounts—such as 20 or even 100 images. If anyone could do this without limits (for example, a bot), The Cat API’s resources could be overwhelmed very quickly.\nThat’s why even seemingly harmless actions often require some form of access control to prevent abuse or unintended behavior.\nIn the next section, we’ll explore how authentication works and how Postman can help you manage your credentials securely.",
    "crumbs": [
      "Using Postman",
      "Basic Request"
    ]
  },
  {
    "objectID": "chapters/dataAnalysis/analyzing-data.html",
    "href": "chapters/dataAnalysis/analyzing-data.html",
    "title": "Analyzing Data",
    "section": "",
    "text": "Our final step is to analyze the data using network analysis to uncover thematic areas within the Medieval Art collection.\nWe have a dataset containing 1725 unique pairs of tags. By transforming this dataset into a network graph, we can visualize how terms relate to one another:\nWhile the full details of network construction are beyond the scope of this workshop, you can create and explore network graphs using tools like Gephi. The processed network data is available in the project repository as network_medieval_art_tags.gexf.",
    "crumbs": [
      "Data Analysis",
      "Analyzing Data"
    ]
  },
  {
    "objectID": "chapters/dataAnalysis/analyzing-data.html#visualizing-the-network",
    "href": "chapters/dataAnalysis/analyzing-data.html#visualizing-the-network",
    "title": "Analyzing Data",
    "section": "Visualizing the network",
    "text": "Visualizing the network\nTo better understand our network, we need to visualize how terms are connected. A graph layout allows us to represent nodes (tags) and edges (relationships) in a structured way, making it easier to explore clusters, central terms, and thematic areas.\nThere are multiple tools available for network visualization, ranging from desktop applications to web-based platforms:\n\nGephi (desktop | Gephi Lite) – A powerful open-source tool for network analysis.\nCytoscape (desktop) – Commonly used in bioinformatics and large-scale networks.\nRetina (web-based) – A browser-friendly tool that allows interactive exploration.\n\nFor this workshop, we will explore the network using Retina, which enables interactive exploration directly in your browser.\n\n\n\n\n\n\nNoteRetina as a Web API for Network Visualization\n\n\n\n\n\nUnlike traditional network visualization tools, Retina works dynamically through URL parameters, making it function similarly to a Web API. Instead of manually adjusting settings in a graphical interface, Retina allows users to modify the view using URL parameters. These parameters control layout, node size, color attributes, filtering, and focus points—all without needing a separate backend. Because the graph is rendered directly in the user’s browser, all changes happen client-side, just like API calls retrieving data.\nThis means we can construct Retina URLs dynamically to generate custom views of our network, making it a powerful tool for API-driven scientific research.\nBy tweaking the URL, you can reconfigure the network visualization dynamically, much like calling an API with different query parameters!\n\n\n\nThe layout for this visualization was prepared in Gephi and then exported to Retina, but you can also import the raw GEXF file into Retina and generate a similar graph from scratch.\n\n\nYou can also explore the network in a new tab by clicking here\nThe network is fully interactive, allowing you to zoom in and out to examine specific areas or use the search bar to locate a particular term. Each node represents a tag from the dataset, and its color and size reflect important network properties.\n\nColor is determined by the modularity class, which groups related terms into thematic communities. Terms that frequently appear together in the dataset will tend to cluster, forming distinct regions in the network.\nSize is based on Eigenvector Centrality, a measure of influence within the network. Larger nodes indicate terms that serve as key connectors, linking multiple themes together. For instance, if a term like “Christ” or “Virgin Mary” appears prominently, it suggests that it plays a central role in shaping the overall structure of the dataset.\n\nTo further customize your view, open the menu on the left and experiment with color and size settings to highlight different attributes. By adjusting these parameters, you can gain new insights into how terms interact and uncover unexpected thematic relationships within the Medieval Art collection.",
    "crumbs": [
      "Data Analysis",
      "Analyzing Data"
    ]
  },
  {
    "objectID": "chapters/dataAnalysis/analyzing-data.html#exploring-the-network",
    "href": "chapters/dataAnalysis/analyzing-data.html#exploring-the-network",
    "title": "Analyzing Data",
    "section": "Exploring the network",
    "text": "Exploring the network\nOne of the challenges of using networks as a data visualization tool is the risk of creating a “hairball” — a dense, cluttered graph with too many connections, making it difficult to interpret patterns or insights. To address this issue, Retina offers built-in features that allow us to explore the network in more detail and isolate key themes.\n\nIsolate communities\nSince each community in our network represents a thematic area, we can filter specific communities to explore the terms they contain and their connections to the broader network.\n\n\n\n\n\n\nTipHow to Isolate a Community in Retina\n\n\n\n\n\nA straightforward way to isolate communities is by using Retina’s “Explore” feature:\n\nOpen the Explore menu.\nClick on a community to highlight it.\nUse the filter options to hide other nodes and focus on a single thematic area.\n\n\n\n\nRetina Filter by Community\n\n\n\n\n\nOur analysis reveals 11 distinct communities, with three major groups containing most of the terms. Some communities are very small, consisting of just two or three nodes.\nTo explore this further, let’s isolate the largest community (Community 0) and analyze its structure in more detail:\n\n\nWith this filter applied, we can better interpret the network. Community 0 appears to represent “Animals, Plants, and Objects”, a broad category that connects mythical, religious, and historical figures with specific objects such as flowers, horses, and medallions.\n\n\nSpotlight on “Horses”\nTo further explore connections within Community 0, let’s isolate the term “Horses” using the search bar in Retina.\nThis reveals that “Horses” acts as a bridge between multiple thematic areas. It links to mythical and historical figures like Achilles, Alexander the Great, Andromache, Hercules, and Saint George. Additionally, it connects with other communities, including Community 4 (centered on “Men” and “Women”) and Community 3 (focused on “Christ” and “Virgin Mary”), highlighting its broad thematic relevance.\n\n\nBeyond connections, we can examine network metrics in the explore menu to better understand the role of “Horses” in the dataset. Below is a breakdown of key values and their meaning in the network:\n\n\n\n\n\n\n\n\nMetric\nValue (Horses)\nMeaning in Context\n\n\n\n\nEccentricity\n3\n“Horses” is relatively central, as its longest shortest path to another node is just 3 steps\n\n\nCloseness Centrality\n0.56\nA higher closeness value means “Horses” is well positioned, with short paths to many other nodes.\n\n\nHarmonic Closeness Centrality\n0.622\nAccounts for disconnected components, reinforcing the influence of “Horses” in the network.\n\n\nBetweenness Centrality\n0.041\nLow betweenness suggests that while “Horses” connects communities, it is not a major bridge.\n\n\nWeighted Degree\n174\n“Horses” has an exceptionally high number of connections, making it a key thematic term.\n\n\nModularity Class\n0\nIt belongs to the largest thematic cluster in the network.\n\n\nStat Inf Class\n14\nBased on Statistical Inference, this classification suggests “Horses” belongs to a statistically significant thematic grouping.\n\n\nClustering Coefficient\n0.167\nA low coefficient indicates “Horses” is not densely interconnected but still plays a linking role.\n\n\nTriangles\n381\nThe node appears in many tightly interconnected groups, suggesting strong thematic associations.\n\n\nEigenvector Centrality\n0.584\nHigh influence, indicating strong connections to other important terms.\n\n\n\nThis approach can be applied to any term in the network—by isolating a node and interpreting its metrics and connections, we can uncover hidden patterns and thematic relationships within the collection.",
    "crumbs": [
      "Data Analysis",
      "Analyzing Data"
    ]
  },
  {
    "objectID": "chapters/dataAnalysis/analyzing-data.html#some-preliminary-insights",
    "href": "chapters/dataAnalysis/analyzing-data.html#some-preliminary-insights",
    "title": "Analyzing Data",
    "section": "Some preliminary insights",
    "text": "Some preliminary insights\nAfter this brief exploration, we can draw several preliminary conclusions about the structure of thematic areas in the Medieval Art collection. These insights not only help us understand how objects are categorized but also highlight broader patterns in cultural and religious representation.\n\n1. Thematic Areas Are Well-Defined but Overlapping\nThe network is clustered into 11 distinct communities, each representing a thematic area. From those, we can identify three major groups:\n\nCommunity 0 (Animals, Plants, and Objects) is a broad category that links religious, mythical, and historical figures to specific objects like horses, flowers, and medallions.\nCommunity 4 (Men & Women) represents human figures and social roles, and overlaps with religious themes.\nCommunity 3 (Christ & Virgin Mary) is highly centered on religious iconography.\n\nOther communities are highly specialized and relatively isolated from the rest of the network. Community 6, for example, is composed of just three nodes—Greek, Coptic, Documents—suggesting a connection to textual traditions rather than visual themes. Similarly, Community 8 consists only of Hands and Feet, likely indicating specific iconographic elements, while Community 9 pairs Dolphins with Female Nudes, pointing to a connection between aquatic imagery and representations of the human body.\n\n\n2. Certain Terms Act as “Bridges” Between Communities\nWhile the network is divided into distinct thematic areas, some terms serve as key connectors, linking multiple communities and acting as conceptual bridges.\nOne clear example is “Horses”, which appears in different contexts—connecting historical figures such as Alexander the Great, Achilles, and Saint George, while also maintaining links to broader thematic groups like Men and Women and Religious Figures. Similarly, Christ and Virgin Mary are central nodes within religious iconography, reinforcing strong intra-community ties. On the other hand, Men and Women function as structural connectors, bridging religious, mythological, and historical figures, reinforcing their role as central elements in medieval art representation.\n\n\n3. The Network Reveals How Art Metadata Is Structured\nThis exploration is not just about historical themes—it also shows how the Met Museum categorizes its objects. The fact that certain terms cluster together may reflect institutional categorization practices, rather than just organic historical connections.\n\n\n4. Unexpected Connections & The Importance of Structure\nHighly connected nodes such as “Men,” “Women,” and “Christ” dominate the network, but less connected nodes can still reveal meaningful insights. A prime example is “Noah”, a relatively isolated node that still exhibits a high clustering coefficient (0.857), indicating that it forms tight local connections despite its limited reach within the broader network.\n\n\nExamining Noah’s connections reveals both expected and surprising relationships. Unsurprisingly, Noah is linked to Moses, another Old Testament figure. However, an unexpected connection emerges between Noah and several Christian themes, including Coronation of the Virgin, Madonna and Child, and Virgin Mary.\nThis suggests that, within this dataset, Noah is not solely categorized within Old Testament narratives but also appears in contexts related to Marian iconography. This is surprising because Noah is typically associated with Genesis and the Flood rather than Christian representations of Mary. Why does he appear in relation to the Coronation of the Virgin? Could this be due to specific artistic traditions, metadata decisions, or broader iconographic patterns? This observation serves as a potential starting point for further research, demonstrating how network analysis can highlight overlooked connections and generate new research questions.",
    "crumbs": [
      "Data Analysis",
      "Analyzing Data"
    ]
  },
  {
    "objectID": "chapters/dataAnalysis/analyzing-data.html#wrapping-up",
    "href": "chapters/dataAnalysis/analyzing-data.html#wrapping-up",
    "title": "Analyzing Data",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThis exploration of the network has revealed key thematic clusters, influential terms, and even unexpected connections within the Medieval Art collection. By structuring metadata as a network, we uncovered both historical patterns and institutional categorization practices, highlighting the interplay between data and interpretation.\nWhile this approach provides a powerful way to visualize and analyze relationships at scale, it also raises important questions: Do these clusters reflect historical realities, or are they shaped by modern cataloging decisions? How might additional data—such as object provenance, artistic schools, or iconographic traditions—further refine our understanding?\nAs we wrap up, we’ll take a step back to reflect on what we’ve learned and consider the broader implications of using APIs and network analysis for cultural heritage research.",
    "crumbs": [
      "Data Analysis",
      "Analyzing Data"
    ]
  },
  {
    "objectID": "chapters/coding/exercise-DLPA.html#learning-objectives",
    "href": "chapters/coding/exercise-DLPA.html#learning-objectives",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSet up and configure a Python environment for API interaction\nMake authenticated requests to the DPLA API\nHandle pagination to collect large datasets\nUse facets to analyze temporal distributions\nExtract and visualize keywords from metadata\n\n\nImport Libraries\nImport all necessary libraries for API interaction, data processing, and visualization.\n\n%pip install httpx yake tqdm matplotlib\n\n\n\nInstall Required Packages\nWe’ll use httpx for HTTP requests, yake for keyword extraction, tqdm for progress bars, and matplotlib for visualization.\n\nimport os\nimport time\nimport json\nfrom getpass import getpass\n\nimport httpx\nfrom urllib.parse import urlsplit, parse_qsl, urlencode, urlunsplit\nimport yake\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA.html#configuration",
    "href": "chapters/coding/exercise-DLPA.html#configuration",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Configuration",
    "text": "Configuration\n\nDefine Constants\nSet up base URLs and configuration variables that will be used throughout the notebook.\n\nAPI_BASE_URL = \"https://api.dp.la/v2/\" \nENV_VAR_NAME = \"DPLA_API_KEY\" \nFALLBACK_DATA_URL = \"https://raw.githubusercontent.com/UCSB-Library-Research-Data-Services/intro2APIs/main/data/\" \n\n\n\nSecure API Key Setup\nStore your API key securely using environment variables. This approach avoids hardcoding sensitive credentials in the notebook.\n\nkey= os.getenv(ENV_VAR_NAME)\n\n# This method avoids hardcoding the API key in the script\n# The variable is persistent during the session\nif not key:\n    key = getpass(f\"Enter your DPLA API key: \").strip()\n    if not key:\n        raise ValueError(\"No API key provided.\")\n    os.environ[ENV_VAR_NAME] = key\n    \nprint(f\"API key set in environment variable {ENV_VAR_NAME}.\")"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA.html#helper-functions",
    "href": "chapters/coding/exercise-DLPA.html#helper-functions",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Helper Functions",
    "text": "Helper Functions\nThese utility functions help keep our code clean and reusable.\n\ndef _join_list(x, sep=\"; \", keep_first_only=False):\n    \"\"\"\n    Helper function to join a list of values into a single string. If the input is not a list, it will return the string representation of the input. If the input is None, it will return an empty string.\n    \"\"\"\n    if isinstance(x, list):\n        if keep_first_only and len(x) &gt; 0:\n            return str(x[0])\n        return sep.join(str(v) for v in x if v is not None)\n    return \"\" if x is None else str(x)\n\ndef _top_n(d, n=10):\n    \"\"\"Helper function to return the top n items from a dictionary, sorted by value in descending order.\"\"\"\n    return dict(sorted(d.items(), key=lambda x: x[1], reverse=True)[:n])\n\ndef _redact_request_url(url):\n    \"\"\"Remove the api_key parameter from the URL for display purposes.\"\"\"\n    parsed_url = urlsplit(str(url))  # Convert httpx.URL to string\n    query_params = parse_qsl(parsed_url.query)\n    filtered_params = [(name, value) for name, value in query_params if name != \"api_key\"]\n    redacted_query = urlencode(filtered_params)\n    redacted_url = parsed_url._replace(query=redacted_query)\n    return urlunsplit(redacted_url)\n\n\nURL Redaction Helper\nRemoves API keys from URLs before printing (security best practice).\n\ndef search_items(query, resource_type='items', verbose=False, timeout=30.0, **parameters):\n    \"\"\"\n    Search DPLA items with given query and parameters.\n    \n    Args:\n        query (str): The search query string. It's possible to use logical operators (AND, OR, NOT). Additionally, you can use wildcards (*) for partial matches.\n        resource_type (str): The type of resource to search for. Default is 'items'.\n        verbose (bool): If True, prints the request URL. Default is False.\n        timeout (float): The timeout for the HTTP request in seconds. Default is 30.0.\n        **parameters: Facets and filter parameters from the DPLA API documentation: https://pro.dp.la/developers/requests\n                    Dotted keywords and values can be passed using dictionary unpacking. For example, to filter by sourceResource.title, you can pass:\n                    **{\"sourceResource.title\": \"example title\"}\n    Returns:\n        dict: The JSON response from the DPLA API as a Python dictionary.\n    \"\"\"\n    \n    # Build the request URL and minimal parameters\n    base_url = f\"{API_BASE_URL}{resource_type}\"\n    params = {\n        \"q\": query,\n        \"api_key\": os.getenv(ENV_VAR_NAME),\n    }\n    \n    # Add additional parameters if any\n    for key, value in parameters.items():\n        params[key] = value\n        \n    # Make the request\n    with httpx.Client(timeout=timeout) as client:\n        response = client.get(base_url, params=params)\n    \n    if verbose:\n        print(f\"Request URL [redacted]: {_redact_request_url(response.url)}\")\n    \n    response.raise_for_status() \n    return response.json()"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA.html#main-api-functions",
    "href": "chapters/coding/exercise-DLPA.html#main-api-functions",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Main API Functions",
    "text": "Main API Functions\n\nSearch Items Function\nThis function queries the DPLA API and returns a single page of results. It handles query parameters, facets, and filters.\n\ndef search_all_items(query, resource_type='items', max_items=100, sleep=0.5, verbose=False, timeout=30.0, **parameters):\n    \"\"\"\n    Collect up to max_items across pages.\n    \n    Args:\n        query (str): The search query string. It's possible to use logical operators \n            (AND, OR, NOT). Additionally, you can use wildcards (*) for partial matches.\n        max_items (int): Maximum number of items to retrieve. For number of elements per page, \n            use the page_size parameter in **parameters.\n        sleep (float): Time to wait between requests to avoid hitting rate limits.\n        **parameters: Facets and filter parameters from the DPLA API documentation: https://pro.dp.la/developers/requests\n    \"\"\"\n    all_docs = []\n    page = 1\n    page_size = int(parameters.get(\"page_size\", 100))\n    if page_size &gt; 500:\n        page_size = 500\n        print(\"page_size cannot exceed 500. Setting to 500.\")\n        \n    while len(all_docs) &lt; max_items:\n        parameters['page'] = page\n        data = search_items(\n            query,\n            resource_type=resource_type,\n            verbose=verbose,\n            timeout=timeout,\n            **parameters\n        )\n        docs = data.get('docs', [])\n        if not docs:\n            break  # No more results\n        all_docs.extend(docs)\n        \n        # stop if we've reached max_items\n        if len(all_docs) &gt;= max_items:\n            break\n        \n        page += 1\n        time.sleep(sleep)\n        \n    return all_docs[:max_items]\n\n    \n\n\n\nSearch All Items Function\nThis function handles pagination and collects multiple pages of results up to a specified maximum.\n🎯 Challenge 1: Experimenting with Pagination\nLet’s explore how pagination works! APIs often return results in “pages” to avoid overwhelming the server and your computer with too much data at once.\nIn the cell below, try changing page_size=5 to page_size=10 and page=1 to page=2. Before running it, think about: which items will you see now? Will they be the same as before, or different ones?\nBonus: What happens if you try page_size=1000? (Hint: Check the output message!)\n\n# Defining the fields, facets, and filters outside the function call for better readability\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\"\n}\n\ntry:\n    ai_search = search_items(\n        \"artificial AND intelligence\", # search query\n        fields=_join_list(fields, sep=\",\"), # fields to include in the response\n        facets=\"sourceResource.date.begin\", # facets to include in the response\n        **dotted_fields, # additional parameters (e.g. filters),\n        page_size=5, # items per page\n        sort_by=\"sourceResource.date.begin\", # sort by date\n        sort_order=\"asc\", # oldest to newest\n        page=1, # page number to retrieve\n        verbose=True # print the request URL for debugging purposes\n        )\n    \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n    r = httpx.get(f\"{FALLBACK_DATA_URL}dpla_search_results.json\")\n    r.raise_for_status()\n    ai_search = r.json()\n\n\n# download the preloaded data for the next steps\nprint(f\"{ai_search.get('count')} results found.\") if isinstance(ai_search, dict) else print(f\"{len(ai_search)} results found.\")"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA.html#exploring-the-data",
    "href": "chapters/coding/exercise-DLPA.html#exploring-the-data",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Exploring the Data",
    "text": "Exploring the Data\n\nInitial Search Query\nLet’s make our first request to explore items about artificial intelligence. We’ll use facets to understand the temporal distribution of items.\n\nfacets_entries = ai_search.get(\"facets\", {}).get(\"sourceResource.date.begin\", {}).get(\"entries\", [])\n\n# Print a sample (we use 'time' because that's the label for date facets)\nfor entry in facets_entries[:5]:\n    print(f\"Year: {entry.get('time')}, Count: {entry.get('count')}\")\n\n\n\nVisualizing Temporal Distribution\nFacets provide aggregated counts by specific fields. Let’s visualize how AI-related items are distributed across time.\n\n# Extract the year and count information from the facet entries\nyears = [entry.get(\"time\") for entry in facets_entries][::-1] # We use [::-1] to reverse the order\ncounts = [entry.get(\"count\") for entry in facets_entries][::-1]\n\n# Create a bar chart using seaborn\nsns.barplot(x=years, y=counts)\nplt.xlabel('Year')\nplt.ylabel('Number of Items')\nplt.title('Items about Artificial Intelligence by Year')\nplt.xticks(range(0, len(years), 5), [years[i] for i in range(0, len(years), 5)], rotation=45)\nplt.tight_layout()\nplt.show()\n\n\nperiods = [\n    (\"preCovid\", 1844, 2018),\n    (\"Covid\", 2019, 2021),\n    (\"postCovid\", 2022, 2026),\n]"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA.html#comparative-analysis-across-time-periods",
    "href": "chapters/coding/exercise-DLPA.html#comparative-analysis-across-time-periods",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Comparative Analysis Across Time Periods",
    "text": "Comparative Analysis Across Time Periods\n\nDefine Time Periods\nBased on the temporal distribution, we can identify three distinct periods of interest.\n🎯 Challenge 2: Testing Date Filters\nBefore we collect data for all three periods, let’s practice using date filters with a single year! Date filters help you narrow down API results to specific time ranges.\nTry creating a new cell below this one and write a search_items() call that retrieves items from 2020 only.\nHint: Use these parameters:\n**{\"sourceResource.date.after\": \"2020\", \"sourceResource.date.before\": \"2020\"}\nBefore running your query, predict: How many results do you expect compared to the full Covid period (2019-2021)? Will it be more, less, or about the same?\n\n# create a pool of results for each period\nai_results = {}\n\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\",\n}\n\ntry:\n    for period_name, start_date, end_date in tqdm(periods):\n        ai_results[period_name] = search_all_items(\n            \"artificial AND intelligence\", # search query\n            max_items=400, # maximum number of items to retrieve for each period\n            fields=_join_list(fields, sep=\",\"), # fields to include in the response\n            facets=\"sourceResource.date.begin\", # Retrieve facets for date ranges\n            page_size=100, # items per page\n            **dotted_fields, # filter to ensure results are about AI, not just using AI in metadata\n            **{\"sourceResource.date.after\": str(start_date)}, # Between year\n            **{\"sourceResource.date.before\": str(end_date)}, # and Year\n            sort_by=\"sourceResource.date.begin\", # sort by date\n            sort_order=\"asc\", # oldest to newest\n            verbose=False \n        )\n        \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n    r = httpx.get(f\"{FALLBACK_DATA_URL}ai_results_by_wave.json\")\n    r.raise_for_status()\n    ai_results = r.json()\n\n\n\nCollect Items for Each Period\nFetch up to 3000 items for each time period using date filters and pagination.\n\nai_results_summary = {period: len(items) for period, items in ai_results.items()}\nprint(\"AI Results Summary by Period:\")\nfor period, count in ai_results_summary.items():\n    print(f\"{period}: {count} items\")\n\n\n\nSummary of Results by Period\nCheck how many items we retrieved for each time period.\n\nai_results.get(\"postCovid\")[:5]\n\n\n\nExploring Item Structure\nLet’s examine the structure of a sample item to understand what metadata is available.\n\ndef extract_keywords(items, skip=None, ngram=2, max_keywords=5, language=\"en\"):\n    \"\"\"Extract keywords from a list of items using YAKE.\"\"\"\n\n    ai_keywords = {}\n\n    kw_extractor = yake.KeywordExtractor(lan=language, n=ngram, top=max_keywords)\n\n    if skip:\n        skip_keywords = set(skip)\n\n    for period, items in tqdm(items.items(), desc=\"Extracting keywords\"):\n        period_keywords = {}\n        for item in items:\n            title = _join_list(item.get(\"sourceResource.title\", \"\"))\n            description = _join_list(item.get(\"sourceResource.description\", \"\"))\n            text = f\"{title} {description}\".lower()\n\n            keywords = kw_extractor.extract_keywords(text)\n            for kw, score in keywords:\n                if skip and kw in skip_keywords:\n                    continue\n                period_keywords[kw] = period_keywords.get(kw, 0) + 1\n                \n        ai_keywords[period] = period_keywords\n    \n    return ai_keywords"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA.html#keyword-extraction",
    "href": "chapters/coding/exercise-DLPA.html#keyword-extraction",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Keyword Extraction",
    "text": "Keyword Extraction\n\nExtract Keywords Using YAKE\nUse YAKE (Yet Another Keyword Extractor) to identify important terms in titles and descriptions for each period.\n\n# Define a list of common words to skip (optional)\nskip_words = [\"artificial intelligence\", \"ai\", \"intelligence\", \"artificial\"]\n\ntop = 10\n\nai_keywords = extract_keywords(ai_results, skip=skip_words, ngram=2, max_keywords=top)\n\nfor period, keywords in ai_keywords.items():\n    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:top]\n    print(f\"Top {top} keywords for {period}:\")\n    for kw, count in sorted_keywords:\n        print(f\"  {kw}: {count}\")\n    print()\n\n\n\nDisplay Top Keywords\nPrint the most frequent keywords for each time period.\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 5), sharex=False)\n\nfor ax, (period_name, start_year, end_year) in zip(axes, periods):\n    data = _top_n(ai_keywords.get(period_name, {}), 10)\n    terms = list(data.keys())\n    counts = list(data.values())\n\n    sns.barplot(x=counts, y=terms, ax=ax, palette=\"viridis\", hue=counts)\n    ax.set_title(f\"{period_name} ({start_year}–{end_year})\")\n    ax.set_xlabel(\"Frequency\")\n\nplt.suptitle(\"How 'Artificial Intelligence' appears across time in DPLA\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA.html#conclusion",
    "href": "chapters/coding/exercise-DLPA.html#conclusion",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve successfully: - ✅ Configured secure API authentication - ✅ Queried the DPLA API with complex filters and facets - ✅ Handled pagination to collect large datasets - ✅ Analyzed temporal patterns in cultural heritage data - ✅ Extracted and visualized keywords across different time periods\nThese techniques are transferable to many other APIs. The patterns you’ve learned—building requests programmatically, handling responses, and combining API data with text analysis—can be applied to museum collections, scientific datasets, social media archives, and more."
  },
  {
    "objectID": "chapters/coding/python-setting-up.html",
    "href": "chapters/coding/python-setting-up.html",
    "title": "Setting Up Your Python Environment for API Interaction",
    "section": "",
    "text": "You can run the Python parts of this workshop either in your browser (recommended) or on your own computer. This page helps you get set up so you can focus on the API concepts, not the tooling.\nThis workshop uses a starter notebook that’s already filled in with the code you’ll need. That way you can run cells immediately, follow along in a live workshop, and edit small parts without having to type everything from scratch.",
    "crumbs": [
      "Getting Data Programmatically",
      "Python Setup"
    ]
  },
  {
    "objectID": "chapters/coding/python-setting-up.html#libraries-used-in-this-workshop",
    "href": "chapters/coding/python-setting-up.html#libraries-used-in-this-workshop",
    "title": "Setting Up Your Python Environment for API Interaction",
    "section": "Libraries Used in This Workshop",
    "text": "Libraries Used in This Workshop\nLibraries are pre-built code packages that provide specific functionality. In this workshop, we’ll use a few libraries: one to make HTTP requests, one to keep our API key out of the notebook output, and one to extract keywords from text. The first cells in our notebook will be dedicated to installing and importing these libraries.\n\nInstalling Libraries\n\n# If you're running in Colab/Binder, you can install everything from inside the notebook.\n%pip install httpx tqdm yake matplotlib\n\n\n\nImporting Libraries\n\n# Import built-in libraries\nimport os # For environment variable management\nimport time # For adding delays between API requests\nimport json # For working with JSON data\nfrom getpass import getpass # For securely inputting API keys\n\n# Import third-party libraries\nimport httpx # For making HTTP requests\nfrom urllib.parse import urlsplit, parse_qsl, urlencode, urlunsplit # For URL manipulation\nimport yake # For keyword extraction\nfrom tqdm import tqdm # For displaying progress bars\nimport matplotlib.pyplot as plt # For data visualization",
    "crumbs": [
      "Getting Data Programmatically",
      "Python Setup"
    ]
  },
  {
    "objectID": "chapters/coding/python-setting-up.html#constants",
    "href": "chapters/coding/python-setting-up.html#constants",
    "title": "Setting Up Your Python Environment for API Interaction",
    "section": "Constants",
    "text": "Constants\nTechnically speaking, a constant is a value that shouldn’t change once it’s set. Python doesn’t have a built-in mechanism to strictly enforce immutability, but it does rely on a strong community-adopted naming convention.\nThe “Pythonic” way to define a constant is to use all uppercase letters with underscores separating words. This signals to other developers that the variable’s value should not be modified after it’s set.\nFor our exercise, we will define the following constants. You can copy and paste this into your code:\n\nAPI_BASE_URL = \"https://api.dp.la/v2/\" \nENV_VAR_NAME = \"DPLA_API_KEY\" \nFALLBACK_DATA_URL = \"https://raw.githubusercontent.com/UCSB-Library-Research-Data-Services/intro2APIs/main/data/\"",
    "crumbs": [
      "Getting Data Programmatically",
      "Python Setup"
    ]
  },
  {
    "objectID": "chapters/coding/python-setting-up.html#storing-your-api-key",
    "href": "chapters/coding/python-setting-up.html#storing-your-api-key",
    "title": "Setting Up Your Python Environment for API Interaction",
    "section": "Storing Your API Key",
    "text": "Storing Your API Key\nEven though this isn’t a public application, it’s still good practice to keep your API key secure and avoid hardcoding it directly. We are going to store the API key as an environment variable (a variable that lives in your system’s environment rather than inside your notebook). This way, you can access it in your code without exposing it.\ngetpass is a built-in Python module that provides a secure way to handle sensitive information, such as passwords or any other sensitive credentials.\nThe method we will use is as follows:\n\n# Check if the API key is already set as an environment variable\napi_key = os.getenv(ENV_VAR_NAME)\n\n# If the API key is not set, a prompt will ask to enter the API key\nif not api_key:\n    api_key = getpass(f\"Enter your DPLA API key: \").strip()\n    if not api_key:\n        raise ValueError(\"No API key provided\")\n    # Store the API key as an environment variable for the current session\n    os.environ[ENV_VAR_NAME] = api_key\n\nprint(f\"API key set in environment variable '{ENV_VAR_NAME}'\")\n\n\n\nAPI key is set in environment variable 'DPLA_API_KEY'\n\n\n\n\n\n\n\n\nImportantNever print your API key\n\n\n\n\n\nNote that we printed a message confirming that the API key is set (and the name of the environment variable where it’s stored), but we did not print the API key itself. This matters in notebooks because cell output can be saved and accidentally shared.\n\n\n\n\nTest the API Key\nTo test that your API key is working, you can make a simple request to the DPLA API. You can copy and paste this code into a cell in your notebook:\n\n# Make a test request to the DPLA API\nif not api_key:\n    print(\n        \"No API key found yet. Set the 'DPLA_API_KEY' environment variable (or run the getpass cell above) and re-run this cell.\"\n    )\nelse:\n    test_url = f\"{API_BASE_URL}items?api_key={api_key}&page_size=1\"\n    response = httpx.get(test_url)\n    if response.status_code == 200:\n        print(\"API key is valid! Here's a sample response:\")\n        print(json.dumps(response.json(), indent=2))\n    else:\n        print(f\"Failed to connect to the API. Status code: {response.status_code}\")\n        print(f\"Response: {response.text}\")\n\nAPI key is valid! Here's a sample response:\n{\n  \"count\": 53194696,\n  \"docs\": [\n    {\n      \"@context\": \"http://dp.la/api/items/context\",\n      \"@id\": \"http://dp.la/api/items/61730f4fd818bd0626a419ef82a88d40\",\n      \"@type\": \"ore:Aggregation\",\n      \"aggregatedCHO\": \"#sourceResource\",\n      \"dataProvider\": {\n        \"@id\": \"http://dp.la/api/contributor/center-for-sacramento-history\",\n        \"name\": \"Center for Sacramento History\"\n      },\n      \"id\": \"61730f4fd818bd0626a419ef82a88d40\",\n      \"ingestDate\": \"2024-02-01T16:27:52.667Z\",\n      \"ingestType\": \"item\",\n      \"isShownAt\": \"https://sacramento.pastperfectonline.com/photo/3F9DAD12-BD08-4B41-B6E1-024530408380\",\n      \"object\": \"https://thumbnails.calisphere.org/clip/150x150/eda2796f3877e7cfe971e54e0dcd7d5f\",\n      \"originalRecord\": {\n        \"stringValue\": \"{\\n  \\\"collection_url\\\" : [ \\\"https://registry.cdlib.org/api/v1/collection/26935/\\\" ],\\n  \\\"repository_name\\\" : [ \\\"Center for Sacramento History\\\" ],\\n  \\\"url_item\\\" : \\\"https://sacramento.pastperfectonline.com/photo/3F9DAD12-BD08-4B41-B6E1-024530408380\\\",\\n  \\\"repository_data\\\" : [ \\\"https://registry.cdlib.org/api/v1/repository/174/::Center for Sacramento History\\\" ],\\n  \\\"rights\\\" : [ \\\"Please contact the contributing institution for more information regarding the copyright status of this object.\\\" ],\\n  \\\"rights_ss\\\" : [ \\\"Please contact the contributing institution for more information regarding the copyright status of this object.\\\" ],\\n  \\\"reference_image_dimensions\\\" : \\\"211:210\\\",\\n  \\\"collection_name\\\" : [ \\\"Center for Sacramento History Photo Collection\\\" ],\\n  \\\"collection_data\\\" : [ \\\"https://registry.cdlib.org/api/v1/collection/26935/::Center for Sacramento History Photo Collection\\\" ],\\n  \\\"sort_title\\\" : \\\"print photographic color\\\",\\n  \\\"reference_image_md5\\\" : \\\"eda2796f3877e7cfe971e54e0dcd7d5f\\\",\\n  \\\"facet_decade\\\" : [ \\\"unknown\\\" ],\\n  \\\"sort_collection_data\\\" : [ \\\"center for sacramento history photo collection:Center for Sacramento History Photo Collection:https://registry.cdlib.org/api/v1/collection/26935/\\\" ],\\n  \\\"harvest_id_s\\\" : \\\"26935--3F9DAD12-BD08-4B41-B6E1-024530408380\\\",\\n  \\\"repository_url\\\" : [ \\\"https://registry.cdlib.org/api/v1/repository/174/\\\" ],\\n  \\\"title\\\" : [ \\\"Print, Photographic, Color\\\" ],\\n  \\\"title_ss\\\" : [ \\\"Print, Photographic, Color\\\" ],\\n  \\\"identifier\\\" : [ \\\"3F9DAD12-BD08-4B41-B6E1-024530408380\\\", \\\"1983/001/SBPM08387\\\" ],\\n  \\\"identifier_ss\\\" : [ \\\"3F9DAD12-BD08-4B41-B6E1-024530408380\\\", \\\"1983/001/SBPM08387\\\" ],\\n  \\\"type\\\" : [ \\\"Image\\\" ],\\n  \\\"type_ss\\\" : [ \\\"Image\\\" ],\\n  \\\"id\\\" : \\\"d6e9e727a3b58ae727960f7d74d0dd38\\\",\\n  \\\"subject\\\" : [ \\\"McClellan Air Force Base\\\" ],\\n  \\\"subject_ss\\\" : [ \\\"McClellan Air Force Base\\\" ],\\n  \\\"_version_\\\" : 1768714413655719936,\\n  \\\"timestamp\\\" : \\\"2023-06-14T21:18:29.361Z\\\"\\n}\"\n      },\n      \"provider\": {\n        \"@id\": \"http://dp.la/api/contributor/cdl\",\n        \"exactMatch\": [\n          \"http://www.wikidata.org/entity/Q5020447\"\n        ],\n        \"name\": \"California Digital Library\"\n      },\n      \"rightsCategory\": \"Unspecified Rights Status\",\n      \"sourceResource\": {\n        \"@id\": \"http://dp.la/api/items/61730f4fd818bd0626a419ef82a88d40#SourceResource\",\n        \"collection\": [\n          {\n            \"title\": \"Center for Sacramento History Photo Collection\"\n          }\n        ],\n        \"identifier\": [\n          \"3F9DAD12-BD08-4B41-B6E1-024530408380\",\n          \"1983/001/SBPM08387\"\n        ],\n        \"rights\": [\n          \"Please contact the contributing institution for more information regarding the copyright status of this object.\"\n        ],\n        \"subject\": [\n          {\n            \"name\": \"McClellan Air Force Base\"\n          }\n        ],\n        \"title\": [\n          \"Print, Photographic, Color\"\n        ],\n        \"type\": [\n          \"image\"\n        ]\n      }\n    }\n  ],\n  \"facets\": [],\n  \"limit\": 1,\n  \"start\": 1\n}\n\n\nHaving all the libraries in place and the API key securely stored, now it’s time to start querying.",
    "crumbs": [
      "Getting Data Programmatically",
      "Python Setup"
    ]
  },
  {
    "objectID": "chapters/coding/exercise-DLPA-resolved.html#learning-objectives",
    "href": "chapters/coding/exercise-DLPA-resolved.html#learning-objectives",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nSet up and configure a Python environment for API interaction\nMake authenticated requests to the DPLA API\nHandle pagination to collect large datasets\nUse facets to analyze temporal distributions\nExtract and visualize keywords from metadata\n\n\nImport Libraries\nImport all necessary libraries for API interaction, data processing, and visualization.\n\n%pip install httpx yake tqdm matplotlib\n\n\n\nInstall Required Packages\nWe’ll use httpx for HTTP requests, yake for keyword extraction, tqdm for progress bars, and matplotlib for visualization.\n\nimport os\nimport time\nimport json\nfrom getpass import getpass\n\nimport httpx\nfrom urllib.parse import urlsplit, parse_qsl, urlencode, urlunsplit\nimport yake\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA-resolved.html#configuration",
    "href": "chapters/coding/exercise-DLPA-resolved.html#configuration",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Configuration",
    "text": "Configuration\n\nDefine Constants\nSet up base URLs and configuration variables that will be used throughout the notebook.\n\nAPI_BASE_URL = \"https://api.dp.la/v2/\" \nENV_VAR_NAME = \"DPLA_API_KEY\" \nFALLBACK_DATA_URL = \"https://raw.githubusercontent.com/UCSB-Library-Research-Data-Services/intro2APIs/main/data/\" \n\n\n\nSecure API Key Setup\nStore your API key securely using environment variables. This approach avoids hardcoding sensitive credentials in the notebook.\n\nkey= os.getenv(ENV_VAR_NAME)\n\n# This method avoids hardcoding the API key in the script\n# The variable is persistent during the session\nif not key:\n    key = getpass(f\"Enter your DPLA API key: \").strip()\n    if not key:\n        raise ValueError(\"No API key provided.\")\n    os.environ[ENV_VAR_NAME] = key\n    \nprint(f\"API key set in environment variable {ENV_VAR_NAME}.\")"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA-resolved.html#helper-functions",
    "href": "chapters/coding/exercise-DLPA-resolved.html#helper-functions",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Helper Functions",
    "text": "Helper Functions\nThese utility functions help keep our code clean and reusable.\n\ndef _join_list(x, sep=\"; \", keep_first_only=False):\n    \"\"\"\n    Helper function to join a list of values into a single string. If the input is not a list, it will return the string representation of the input. If the input is None, it will return an empty string.\n    \"\"\"\n    if isinstance(x, list):\n        if keep_first_only and len(x) &gt; 0:\n            return str(x[0])\n        return sep.join(str(v) for v in x if v is not None)\n    return \"\" if x is None else str(x)\n\ndef _top_n(d, n=10):\n    \"\"\"Helper function to return the top n items from a dictionary, sorted by value in descending order.\"\"\"\n    return dict(sorted(d.items(), key=lambda x: x[1], reverse=True)[:n])\n\ndef _redact_request_url(url):\n    \"\"\"Remove the api_key parameter from the URL for display purposes.\"\"\"\n    parsed_url = urlsplit(str(url))  # Convert httpx.URL to string\n    query_params = parse_qsl(parsed_url.query)\n    filtered_params = [(name, value) for name, value in query_params if name != \"api_key\"]\n    redacted_query = urlencode(filtered_params)\n    redacted_url = parsed_url._replace(query=redacted_query)\n    return urlunsplit(redacted_url)"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA-resolved.html#main-api-functions",
    "href": "chapters/coding/exercise-DLPA-resolved.html#main-api-functions",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Main API Functions",
    "text": "Main API Functions\n\nSearch Items Function\nThis function queries the DPLA API and returns a single page of results. It handles query parameters, facets, and filters.\n\ndef search_items(query, resource_type='items', verbose=False, timeout=30.0, **parameters):\n    \"\"\"\n    Search DPLA items with given query and parameters.\n    \n    Args:\n        query (str): The search query string. It's possible to use logical operators (AND, OR, NOT). Additionally, you can use wildcards (*) for partial matches.\n        resource_type (str): The type of resource to search for. Default is 'items'.\n        verbose (bool): If True, prints the request URL. Default is False.\n        timeout (float): The timeout for the HTTP request in seconds. Default is 30.0.\n        **parameters: Facets and filter parameters from the DPLA API documentation: https://pro.dp.la/developers/requests\n                    Dotted keywords and values can be passed using dictionary unpacking. For example, to filter by sourceResource.title, you can pass:\n                    **{\"sourceResource.title\": \"example title\"}\n    Returns:\n        dict: The JSON response from the DPLA API as a Python dictionary.\n    \"\"\"\n    \n    # Build the request URL and minimal parameters\n    base_url = f\"{API_BASE_URL}{resource_type}\"\n    params = {\n        \"q\": query,\n        \"api_key\": os.getenv(ENV_VAR_NAME),\n    }\n    \n    # Add additional parameters if any\n    for key, value in parameters.items():\n        params[key] = value\n        \n    # Make the request\n    with httpx.Client(timeout=timeout) as client:\n        response = client.get(base_url, params=params)\n    \n    if verbose:\n        print(f\"Request URL [redacted]: {_redact_request_url(response.url)}\")\n    \n    response.raise_for_status() \n    return response.json()\n\n\n\nSearch All Items Function\nThis function handles pagination and collects multiple pages of results up to a specified maximum.\n\ndef search_all_items(query, resource_type='items', max_items=100, sleep=0.5, verbose=False, timeout=30.0, **parameters):\n    \"\"\"\n    Collect up to max_items across pages.\n    \n    Args:\n        query (str): The search query string. It's possible to use logical operators \n            (AND, OR, NOT). Additionally, you can use wildcards (*) for partial matches.\n        max_items (int): Maximum number of items to retrieve. For number of elements per page, \n            use the page_size parameter in **parameters.\n        sleep (float): Time to wait between requests to avoid hitting rate limits.\n        **parameters: Facets and filter parameters from the DPLA API documentation: https://pro.dp.la/developers/requests\n    \"\"\"\n    all_docs = []\n    page = 1\n    page_size = int(parameters.get(\"page_size\", 100))\n    if page_size &gt; 500:\n        page_size = 500\n        print(\"page_size cannot exceed 500. Setting to 500.\")\n        \n    while len(all_docs) &lt; max_items:\n        parameters['page'] = page\n        data = search_items(\n            query,\n            resource_type=resource_type,\n            verbose=verbose,\n            timeout=timeout,\n            **parameters\n        )\n        docs = data.get('docs', [])\n        if not docs:\n            break  # No more results\n        all_docs.extend(docs)\n        \n        # stop if we've reached max_items\n        if len(all_docs) &gt;= max_items:\n            break\n        \n        page += 1\n        time.sleep(sleep)\n        \n    return all_docs[:max_items]"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA-resolved.html#exploring-the-data",
    "href": "chapters/coding/exercise-DLPA-resolved.html#exploring-the-data",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Exploring the Data",
    "text": "Exploring the Data\n\nInitial Search Query\nLet’s make our first request to explore items about artificial intelligence. We’ll use facets to understand the temporal distribution of items.\n🎯 Challenge 1: Experimenting with Pagination\nLet’s explore how pagination works! APIs often return results in “pages” to avoid overwhelming the server and your computer with too much data at once.\nIn the cell below, try changing page_size=5 to page_size=10 and page=1 to page=2. Before running it, think about: which items will you see now? Will they be the same as before, or different ones?\nBonus: What happens if you try page_size=1000? (Hint: Check the output message!)\n\n\nSolution to Challenge 1\nHere’s how pagination affects what you see:\n\n# Challenge 1 Solution: Modified pagination parameters\n# page_size=10 means we get 10 items per page instead of 5\n# page=2 means we skip the first 10 items and get items 11-20\n\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\"\n}\n\ntry:\n    ai_search_pagination = search_items(\n        \"artificial AND intelligence\",\n        fields=_join_list(fields, sep=\",\"),\n        facets=\"sourceResource.date.begin\",\n        **dotted_fields,\n        page_size=10,  # Changed from 5 to 10\n        sort_by=\"sourceResource.date.begin\",\n        sort_order=\"asc\",\n        page=2,  # Changed from 1 to 2 - this skips the first page\n        verbose=True\n    )\n    \n    print(f\"\\n{ai_search_pagination.get('count')} total results found.\")\n    print(f\"Showing items from page 2 (items 11-20):\\n\")\n    \n    # Display the first 3 items to see the different results\n    for i, doc in enumerate(ai_search_pagination.get('docs', [])[:3], start=11):\n        title = _join_list(doc.get(\"sourceResource.title\", \"No title\"))\n        date = _join_list(doc.get(\"sourceResource.date.begin\", \"No date\"))\n        print(f\"{i}. {title} ({date})\")\n        \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n    r = httpx.get(f\"{FALLBACK_DATA_URL}dpla_search_results.json\")\n    r.raise_for_status()\n    ai_search_pagination = r.json()\n\n# Bonus: Try page_size=1000 - the API will limit it to 500 max!\n\n\n# Defining the fields, facets, and filters outside the function call for better readability\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\"\n}\n\ntry:\n    ai_search = search_items(\n        \"artificial AND intelligence\", # search query\n        fields=_join_list(fields, sep=\",\"), # fields to include in the response\n        facets=\"sourceResource.date.begin\", # facets to include in the response\n        **dotted_fields, # additional parameters (e.g. filters),\n        page_size=5, # items per page\n        sort_by=\"sourceResource.date.begin\", # sort by date\n        sort_order=\"asc\", # oldest to newest\n        page=1, # page number to retrieve\n        verbose=True # print the request URL for debugging purposes\n        )\n    \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n    r = httpx.get(f\"{FALLBACK_DATA_URL}dpla_search_results.json\")\n    r.raise_for_status()\n    ai_search = r.json()\n\n\n# download the preloaded data for the next steps\nprint(f\"{ai_search.get('count')} results found.\") if isinstance(ai_search, dict) else print(f\"{len(ai_search)} results found.\")\n\n\n\nExplore Temporal Distribution\nPrint a sample of the temporal facet results to see how items are distributed across different time periods.\n\nfacets_entries = ai_search.get(\"facets\", {}).get(\"sourceResource.date.begin\", {}).get(\"entries\", [])\n\n# Print a sample (we use 'time' because that's the label for date facets)\nfor entry in facets_entries[:5]:\n    print(f\"Year: {entry.get('time')}, Count: {entry.get('count')}\")\n\n\n\nVisualizing Temporal Distribution\nFacets provide aggregated counts by specific fields. Let’s visualize how AI-related items are distributed across time.\n\n# Extract the year and count information from the facet entries\nyears = [entry.get(\"time\") for entry in facets_entries][::-1] # We use [::-1] to reverse the order\ncounts = [entry.get(\"count\") for entry in facets_entries][::-1]\n\n# Create a bar chart using seaborn\nsns.barplot(x=years, y=counts)\nplt.xlabel('Year')\nplt.ylabel('Number of Items')\nplt.title('Items about Artificial Intelligence by Year')\nplt.xticks(range(0, len(years), 5), [years[i] for i in range(0, len(years), 5)], rotation=45)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA-resolved.html#comparative-analysis-across-time-periods",
    "href": "chapters/coding/exercise-DLPA-resolved.html#comparative-analysis-across-time-periods",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Comparative Analysis Across Time Periods",
    "text": "Comparative Analysis Across Time Periods\n\nDefine Time Periods\nBased on the temporal distribution, we can identify three distinct periods of interest.\n🎯 Challenge 2: Testing Date Filters\nBefore we collect data for all three periods, let’s practice using date filters with a single year! Date filters help you narrow down API results to specific time ranges.\nTry creating a new cell below this one and write a search_items() call that retrieves items from 2020 only.\nHint: Use these parameters:\n**{\"sourceResource.date.after\": \"2020\", \"sourceResource.date.before\": \"2020\"}\nBefore running your query, predict: How many results do you expect compared to the full Covid period (2019-2021)? Will it be more, less, or about the same?\n\n\nSolution to Challenge 2\nLet’s test a single year (2020) before collecting data for all periods:\n\n# Challenge 2 Solution: Testing date filters for a single year (2020)\n# Date filters help narrow results to specific time ranges\n\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\",\n}\n\ntry:\n    # Query for 2020 only\n    test_2020 = search_items(\n        \"artificial AND intelligence\",\n        fields=_join_list(fields, sep=\",\"),\n        facets=\"sourceResource.date.begin\",\n        **dotted_fields,\n        **{\"sourceResource.date.after\": \"2020\"},  # Items from 2020...\n        **{\"sourceResource.date.before\": \"2020\"}, # ...to 2020 (inclusive)\n        page_size=100,\n        sort_by=\"sourceResource.date.begin\",\n        sort_order=\"asc\",\n        verbose=True\n    )\n    \n    count_2020 = test_2020.get('count', 0)\n    print(f\"\\n✅ Items from 2020 only: {count_2020}\")\n    \n    # For comparison, let's query the full Covid period (2019-2021)\n    test_covid_full = search_items(\n        \"artificial AND intelligence\",\n        fields=_join_list(fields, sep=\",\"),\n        **dotted_fields,\n        **{\"sourceResource.date.after\": \"2019\"},\n        **{\"sourceResource.date.before\": \"2021\"},\n        page_size=100,\n        verbose=False\n    )\n    \n    count_covid = test_covid_full.get('count', 0)\n    print(f\"📊 Items from Covid period (2019-2021): {count_covid}\")\n    print(f\"\\n💡 Insight: 2020 represents {count_2020/count_covid*100:.1f}% of the Covid period items\")\n    \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}\")\n    print(\"This demonstrates how date filters narrow your results to specific time ranges!\")\n\n\nperiods = [\n    (\"preCovid\", 1844, 2018),\n    (\"Covid\", 2019, 2021),\n    (\"postCovid\", 2022, 2026),\n]\n\n\n\nCollect Items for Each Period\nFetch up to 3000 items for each time period using date filters and pagination.\n\n# create a pool of results for each period\nai_results = {}\n\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\",\n}\n\ntry:\n    for period_name, start_date, end_date in tqdm(periods):\n        ai_results[period_name] = search_all_items(\n            \"artificial AND intelligence\", # search query\n            max_items=400, # maximum number of items to retrieve for each period\n            fields=_join_list(fields, sep=\",\"), # fields to include in the response\n            facets=\"sourceResource.date.begin\", # Retrieve facets for date ranges\n            page_size=100, # items per page\n            **dotted_fields, # filter to ensure results are about AI, not just using AI in metadata\n            **{\"sourceResource.date.after\": str(start_date)}, # Between year\n            **{\"sourceResource.date.before\": str(end_date)}, # and Year\n            sort_by=\"sourceResource.date.begin\", # sort by date\n            sort_order=\"asc\", # oldest to newest\n            verbose=False \n        )\n        \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n    r = httpx.get(f\"{FALLBACK_DATA_URL}ai_results_by_wave.json\")\n    r.raise_for_status()\n    ai_results = r.json()\n\n\n\nSummary of Results by Period\nCheck how many items we retrieved for each time period.\n\nai_results_summary = {period: len(items) for period, items in ai_results.items()}\nprint(\"AI Results Summary by Period:\")\nfor period, count in ai_results_summary.items():\n    print(f\"{period}: {count} items\")\n\n\n\nExploring Item Structure\nLet’s examine the structure of a sample item to understand what metadata is available.\n\nai_results.get(\"postCovid\")[:5]"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA-resolved.html#keyword-extraction",
    "href": "chapters/coding/exercise-DLPA-resolved.html#keyword-extraction",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Keyword Extraction",
    "text": "Keyword Extraction\n\nExtract Keywords Using YAKE\nUse YAKE (Yet Another Keyword Extractor) to identify important terms in titles and descriptions for each period.\n\ndef extract_keywords(items, skip=None, ngram=2, max_keywords=5, language=\"en\"):\n    \"\"\"Extract keywords from a list of items using YAKE.\"\"\"\n\n    ai_keywords = {}\n\n    kw_extractor = yake.KeywordExtractor(lan=language, n=ngram, top=max_keywords)\n\n    if skip:\n        skip_keywords = set(skip)\n\n    for period, items in tqdm(items.items(), desc=\"Extracting keywords\"):\n        period_keywords = {}\n        for item in items:\n            text = _join_list(item.get(\"sourceResource.title\", \"\")).lower()\n\n            keywords = kw_extractor.extract_keywords(text)\n            for kw, score in keywords:\n                if skip and kw in skip_keywords:\n                    continue\n                period_keywords[kw] = period_keywords.get(kw, 0) + 1\n                \n        ai_keywords[period] = period_keywords\n    \n    return ai_keywords\n\n\n\nDisplay Top Keywords\nPrint the most frequent keywords for each time period.\n\n# Define a list of common words to skip (optional)\nskip_words = [\"artificial intelligence\", \"ai\", \"intelligence\", \"artificial\"]\n\ntop = 10\n\nai_keywords = extract_keywords(ai_results, skip=skip_words, ngram=2, max_keywords=top)\n\nfor period, keywords in ai_keywords.items():\n    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:top]\n    print(f\"Top {top} keywords for {period}:\")\n    for kw, count in sorted_keywords:\n        print(f\"  {kw}: {count}\")\n    print()\n\n\n\nVisualize Top Keywords\nCreate bar charts to visualize the top keywords for each time period.\n\nfig, axes = plt.subplots(1, 3, figsize=(14, 5), sharex=False)\n\nfor ax, (period_name, start_year, end_year) in zip(axes, periods):\n    data = _top_n(ai_keywords.get(period_name, {}), 10)\n    terms = list(data.keys())\n    counts = list(data.values())\n\n    sns.barplot(x=counts, y=terms, ax=ax, palette=\"viridis\", hue=counts)\n    ax.set_title(f\"{period_name} ({start_year}–{end_year})\")\n    ax.set_xlabel(\"Frequency\")\n\nplt.suptitle(\"How 'Artificial Intelligence' appears across time in DPLA\", fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "chapters/coding/exercise-DLPA-resolved.html#conclusion",
    "href": "chapters/coding/exercise-DLPA-resolved.html#conclusion",
    "title": "DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage",
    "section": "Conclusion",
    "text": "Conclusion\nYou’ve successfully: - ✅ Configured secure API authentication - ✅ Queried the DPLA API with complex filters and facets - ✅ Handled pagination to collect large datasets - ✅ Analyzed temporal patterns in cultural heritage data - ✅ Extracted and visualized keywords across different time periods\nThese techniques are transferable to many other APIs. The patterns you’ve learned—building requests programmatically, handling responses, and combining API data with text analysis—can be applied to museum collections, scientific datasets, social media archives, and more."
  },
  {
    "objectID": "chapters/what-is-an-api.html",
    "href": "chapters/what-is-an-api.html",
    "title": "What is an API?",
    "section": "",
    "text": "&lt;script&gt;\nwindow.getRandomCat = async function () {\n    \n    const response = await fetch(\"https://api.thecatapi.com/v1/images/search?size=med&mime_types=jpg&format=json&has_breeds=true&order=RANDOM&page=0&limit=1\");\n    if (!response.ok) throw new Error(\"Failed to fetch random cat.\");\n    const data = await response.json();\n\n    const cat = data[0];\n    const cat_id = cat.id;\n\n    const cat_detailed_info = await fetch(`https://api.thecatapi.com/v1/images/${cat_id}`);\n    const cat_detailed_info_json = await cat_detailed_info.json();\n\n    // Safely access items that might be undefined or empty\n    const breed = cat_detailed_info_json.breeds && cat_detailed_info_json.breeds.length &gt; 0 ? cat_detailed_info_json.breeds[0].name : \"Unknown\";\n    const description = cat_detailed_info_json.breeds && cat_detailed_info_json.breeds.length &gt; 0 ? cat_detailed_info_json.breeds[0].description : \"Unknown\";\n\n    document.getElementById(\"cat-image\").innerHTML = `\n        &lt;div class=\"card\" style=\"max-width: 300px; margin: 15px auto;\"&gt;\n            &lt;img src=\"${cat.url}\" class=\"card-img-top\" alt=\"Random Cat\"&gt;\n            &lt;div class=\"card-body\"&gt;\n                &lt;h5 class=\"card-title\"&gt;Breed: ${breed}&lt;/h5&gt;\n                &lt;p class=\"card-text\"&gt;${description}&lt;/p&gt;\n            &lt;/div&gt;\n        &lt;/div&gt;\n    `;\n};\n&lt;/script&gt;\n&lt;button onclick=\"getRandomCat()\" class=\"btn btn-primary\" style=\"padding: 10px 20px; font-size: 16px; display: block; margin: 0 auto;\"&gt;Click here to get a random cat :)&lt;/button&gt;\n&lt;div id=\"cat-image\" style=\"margin-top: 15px;\"&gt;&lt;/div&gt;\nAn API (Application Programming Interface) is a set of rules and protocols enabling software programs to communicate and share information. While the term is often associated with “web APIs,” it encompasses a broader concept. For instance, an API might allow a library to share its collection data with a museum or enable a weather service to provide updates to a news organization. APIs operate based on predefined rules set by developers, specifying how data can be accessed and used. While some APIs are public, most are internal, facilitating communication between systems within an organization.",
    "crumbs": [
      "What are APIs and why do they exists"
    ]
  },
  {
    "objectID": "chapters/what-is-an-api.html#using-an-api",
    "href": "chapters/what-is-an-api.html#using-an-api",
    "title": "What is an API?",
    "section": "Using an API",
    "text": "Using an API\nFor those unfamiliar with computer science, APIs can be a bit abstract, so let’s try to explain it better with this example using the The Cat API. This API allows you to get information about different cat and dog breeds, with images and descriptions. The information is not available through a public interface, so if you want to see a cat image, you must use the API.\nIn this example, let’s get the information about the Ragdoll cat breed. To do this, we require to use this API endpoint: https://api.thecatapi.com/v1/images/XFhRpYS_D.\nYou can click on that link to view the information about the cat in a format similar to this:\n\n\nCode\nasync function getCatData(catId) {\n    const url = `https://api.thecatapi.com/v1/images/${catId}`;\n    const response = await fetch(url);\n    if (!response.ok) throw new Error(`HTTP error! status: ${response.status}`);\n    const data = await response.json();\n    return data;\n}\n\ncatData = await getCatData(\"XFhRpYS_D\");\n\nviewof catDataString = {\n  const pre = html`&lt;pre style=\"background-color: #f8f9fa; padding: 15px; border-radius: 5px; overflow-x: auto;\"&gt;`;\n  const code = html`&lt;code style=\"color: #333;\"&gt;`;\n  \n  // Formatted JSON with syntax highlighting\n  const formatted = JSON.stringify(catData, null, 2)\n    .replace(/&/g, '&amp;')\n    .replace(/&lt;/g, '&lt;')\n    .replace(/&gt;/g, '&gt;')\n    .replace(/\"([^\"]+)\":/g, '&lt;span style=\"color: #a31515;\"&gt;\"$1\"&lt;/span&gt;:') \n    .replace(/: \"([^\"]+)\"/g, ': &lt;span style=\"color: #008000;\"&gt;\"$1\"&lt;/span&gt;')  \n    .replace(/: ([0-9]+)/g, ': &lt;span style=\"color: #0000FF;\"&gt;$1&lt;/span&gt;')  \n    .replace(/\\b(true|false|null)\\b/g, '&lt;span style=\"color: #FF0000;\"&gt;$1&lt;/span&gt;');\n    \n  code.innerHTML = formatted;\n  pre.appendChild(code);\n  return pre;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis format is called JSON, and its widely used in APIs to return data. We will see more about this format later, but for now, we can say that this output is called a response, and it contains keys and values. Keys are the names of the information we requested and values are the information we received. For instance, in this response, the url to the image of a Ragdoll cat can be found under the url key, and its value is https://cdn2.thecatapi.com/images/XFhRpYS_D.jpg.\n\n\n\nJSON syntax\n\n\nNow, what we want to do is to get the image of the Ragdoll, not just the abstract information. To do this, we have to access the values associated with each key. The url to the image is under the url key, the name of the cat is under the breeds key and the description is under the description key. With a bit of coding, we can display the image of the cat:\n\n\nCode\nfunction displayCat(data) {\n    const container = html`&lt;div class=\"card\" style=\"max-width: 300px; margin: 15px auto;\"&gt;\n        &lt;img src=\"${data.url}\" class=\"card-img-top\" alt=\"Ragdoll cat\"&gt;\n        &lt;div class=\"card-body\"&gt;\n            &lt;h5 class=\"card-title\"&gt;Breed: ${data.breeds[0].name}&lt;/h5&gt;\n            &lt;p class=\"card-text\"&gt;${data.breeds[0].description}&lt;/p&gt;\n        &lt;/div&gt;\n    &lt;/div&gt;`;\n    return container;\n}\n\ndisplayCat(catData);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCute, isn’t it?\nWhat the API is doing, is to get the information from the server database, return it to the client in a format that can be easily parsed by a computer, and then the browser can use this information to display the cat image, or to get additional information about the cat.\n\n\n\nA simple representation of an API (click to zoom)\n\n\nWith that information, we can display the information in a more readable format, and reutilize the method to get the image of other cats, for instance, a Bengal cat:\n\n\nCode\nbengalCatData = await getCatData(\"LSaDk6OjY\");\ndisplayCat(bengalCatData);\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYou can even get a random cat image!\n  Code \n\n\n\nClick here to get a random cat :)\n\n\n\n\n\n\nNow that we’ve seen how APIs work, let’s dive into why they’re so essential.",
    "crumbs": [
      "What are APIs and why do they exists"
    ]
  },
  {
    "objectID": "chapters/what-is-an-api.html#why-do-we-need-apis",
    "href": "chapters/what-is-an-api.html#why-do-we-need-apis",
    "title": "What is an API?",
    "section": "Why Do We Need APIs?",
    "text": "Why Do We Need APIs?\nA common question might be: why use an API instead of simply sharing the data directly?\n\nSimplifying Access to Data\nFor researchers, APIs simplify access to complex datasets. Imagine working with a public archive or a library catalog where data is stored in various relational tables. Instead of manually extracting and organizing the data, an API allows you to query for specific subsets—like texts published in a particular year or metadata about historical photographs—without worrying about the database structure.\nAn API simplifies this process by predefining these complex queries and providing endpoints to access the data you need with minimal effort. Instead of interacting directly with the database, users make requests to the API and receive only the required data in a user-friendly format, like JSON.\n\n\nEnhancing Security and Performance\nDirect access to a database can compromise its security and integrity. Opening a database to the public exposes it to risks, such as unauthorized modifications or data breaches. APIs mitigate these risks by controlling what data is accessible and to whom. For instance APIs can restrict access to specific users or IP addresses. APIs can also limit the amount of data retrieved, improving performance.\nFor collaborative research, APIs ensure secure and consistent access to data across teams and institutions. They also enhance reproducibility by allowing other researchers to replicate data retrieval processes through well-documented endpoints.\n\n\nWhy Not Share Data as a Downloadable File?\nSharing data in formats like Excel or CSV files can sometimes be a practical solution. However, this approach has significant limitations, for instance, downloadable files do not automatically update when the source data changes. Users must re-download the file every time updates are made, which can lead to outdated information. Another limitation is associated with the size of the file. For large datasets, downloading and processing the entire file can be inefficient and cumbersome.\nUltimately, whether to use an API or share data as a downloadable file depends on the project’s specific needs, balancing dynamic access with ease of use. When you need to retrieve specific data subsets or interact with the data dynamically, an API is ideal. It provides granularity and ensures users access the most up-to-date information. When users need the entire dataset for analysis or offline use, offering a downloadable file might be more appropriate.\n\n\n\n\n\n\nCautionAPIs for bulk data retrieval\n\n\n\n\n\nAPIs are not designed for bulk data retrieval but for accessing data in a more focused and controlled way. It bridges the gap between complex databases and the users who need specific, timely information.",
    "crumbs": [
      "What are APIs and why do they exists"
    ]
  },
  {
    "objectID": "chapters/what-is-an-api.html#apis-in-research",
    "href": "chapters/what-is-an-api.html#apis-in-research",
    "title": "What is an API?",
    "section": "APIs in Research",
    "text": "APIs in Research\nIn research, APIs bridge the gap between data and analysis by automating data collection and streamlining workflows. A historian studying digitized newspapers, for instance, can use APIs to query articles that mention specific events, dates, or people. Social scientists can analyze real-time conversations and trends through APIs from platforms like X or YouTube, while environmental researchers can leverage satellite data APIs to monitor deforestation patterns. By enabling seamless integration across datasets, APIs foster interdisciplinary collaboration and empower scholars to tackle complex questions with innovative approaches.\n\n\n\n\n\n\nNoteIn summary\n\n\n\n\n\nAPIs are not just tools for developers—they hold immense potential for researchers, too. By providing structured, dynamic access to datasets, APIs enable scholars to automate data collection, access real-time information, and integrate diverse data sources into their workflows. Whether you’re studying digital humanities, analyzing climate data, or investigating social media trends, APIs allow you to retrieve exactly the data you need, at scale and with precision. Embracing APIs as part of your research toolkit opens doors to innovative methodologies and insights that might otherwise remain out of reach.",
    "crumbs": [
      "What are APIs and why do they exists"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/endpoints.html",
    "href": "chapters/interactingAPIs/endpoints.html",
    "title": "Endpoints",
    "section": "",
    "text": "In the terminology of APIs, an endpoint is a URL that specifies the location of a resource on a server. As we saw with the cat API, most of the time you can access the data from an endpoint without requiring any particular software besides a web browser.\nHowever, an API doesn’t just retrieve data, it could also create, update, or delete data. For that reason, we could say that an endpoint is a specific type of URL that allows you to perform a specific action on a resource.\nThen, let’s break the pieces of an endpoint into four parts:",
    "crumbs": [
      "Interacting with APIs",
      "Endpoints"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/endpoints.html#the-protocol",
    "href": "chapters/interactingAPIs/endpoints.html#the-protocol",
    "title": "Endpoints",
    "section": "The protocol",
    "text": "The protocol\nThe protocol is the first part of an endpoint, specifying whether access to the resource is done through HTTP or HTTPS. In simple terms, HTTP is the same protocol used by the web browser to retrieve web pages. The “S” in HTTPS stands for “secure” and it means that the communication between the client and the server is encrypted.\n\n\n\n\n\n\nCurrently, HTTPS is the de facto standard for APIs, due to enhanced security.",
    "crumbs": [
      "Interacting with APIs",
      "Endpoints"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/endpoints.html#the-domain",
    "href": "chapters/interactingAPIs/endpoints.html#the-domain",
    "title": "Endpoints",
    "section": "The domain",
    "text": "The domain\nThe second part of an endpoint is the domain name. This means the unique name of the host that provides the resource. In our example, the domain of the cat API is api.thecatapi.com.\n\n\n\n\n\n\nThe domain name of the API is not necessarily the same as the name of the main website of the API provider. For instance, the domain name of the cat API is api.thecatapi.com, but the main website of the cat API is thecatapi.com.",
    "crumbs": [
      "Interacting with APIs",
      "Endpoints"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/endpoints.html#the-path",
    "href": "chapters/interactingAPIs/endpoints.html#the-path",
    "title": "Endpoints",
    "section": "The path",
    "text": "The path\nThe third part of an endpoint is the path. This piece of the endpoint specifies the resource we want to access. This could be a specific set of data, for instance, a list of cat breeds, but it could also be an action to be performed, for instance, searching. The path can also specify the version of the API that is being used, for instance, “v1” or “v2”.\nIn the cat API, the path can be something like /v1/breeds.\nSome endpoints can include a resource identifier to retrieve a specific resource, for instance, the path /v1/breeds/ gives as response a list of all cat breeds, and it’s possible to retrieve a specific breed, let’s say “Korat”, by adding the identifier /kora to the path, so the full path would be /v1/breeds/kora.",
    "crumbs": [
      "Interacting with APIs",
      "Endpoints"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/endpoints.html#the-query-parameters",
    "href": "chapters/interactingAPIs/endpoints.html#the-query-parameters",
    "title": "Endpoints",
    "section": "The query parameters",
    "text": "The query parameters\nAn endpoint can retrieve lists of data and individual items. It can also perform actions, for instance, searching. The query parameters are the part of endpoints that allows us to explore the data available in the API. This can include multiple parts used to specify the search. Let’s explore this with more detail.\nThe Cat API has an endpoint to search for cat images. The path of this endpoint is /v1/images/search. If we do a request to this endpoint we will get a random cat image. However, the developers have included a set of parameters that can delimit the search like the size of the image, the type of the image, the format of the response, if the image has a breed, etc. In that case, instead of just having a random cat image, we can retrieve a list of cat images with a size, media type, and with a specific limit. Then, the part of the endpoint will be like this:\nsearch?size=small&mime_types=gif&limit=10\nNote that the query parameters are separated from the path by a question mark (?) and they are separated by ampersands (&). Order of the parameters is not important, but using the correct name of the parameter is crucial.",
    "crumbs": [
      "Interacting with APIs",
      "Endpoints"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/endpoints.html#the-full-endpoint",
    "href": "chapters/interactingAPIs/endpoints.html#the-full-endpoint",
    "title": "Endpoints",
    "section": "The full endpoint",
    "text": "The full endpoint\nNow, having all the parts of an endpoint, we can write the full endpoint as follows:\n\nThis can be read as follows: “I want to retrieve 5 cat gif images with a small size”.\nWith this result:\n\n\nCode\ncats = {\n  try {\n  return await FileAttachment(\"../../_data/cats.json\").json();\n  } catch (e) {\n  console.error(\"Error loading cats data. This is normal if you're running the book locally.\", e);\n  return await FileAttachment(\"../../_dev/cats-fallback.json\").json()\n  }\n}\n\nviewof carousel = {\n  const container = html`\n  &lt;div class=\"carousel-container\"&gt;\n  &lt;div class=\"carousel-wrapper\"&gt;&lt;/div&gt;\n  &lt;button class=\"carousel-button prev\"&gt;←&lt;/button&gt;\n  &lt;button class=\"carousel-button next\"&gt;→&lt;/button&gt;\n  &lt;/div&gt;\n  `;\n  \n  const wrapper = container.querySelector('.carousel-wrapper');\n  \n  cats.forEach(cat =&gt; {\n  const img = document.createElement('img');\n  img.src = cat.url;\n  img.alt = 'Cat image';\n  wrapper.appendChild(img);\n  });\n\n  let currentIndex = 0;\n  const totalImages = cats.length;\n  \n  function updateCarousel() {\n  wrapper.style.transform = `translateX(-${currentIndex * 100}%)`;\n  }\n\n  container.querySelector('.next').addEventListener('click', () =&gt; {\n  currentIndex = (currentIndex + 1) % totalImages;\n  updateCarousel();\n  });\n\n  container.querySelector('.prev').addEventListener('click', () =&gt; {\n  currentIndex = (currentIndex - 1 + totalImages) % totalImages;\n  updateCarousel();\n  });\n  \n  return container;\n}",
    "crumbs": [
      "Interacting with APIs",
      "Endpoints"
    ]
  },
  {
    "objectID": "chapters/interactingAPIs/interact-apis.html",
    "href": "chapters/interactingAPIs/interact-apis.html",
    "title": "Interacting with APIs",
    "section": "",
    "text": "Now that we understand what an API endpoint is, let’s try interacting with the Cat API directly. Enter an endpoint path below (like /v1/images/search?limit=1) to see the API response.\nTry these examples:\n\n/v1/images/search?limit=1 - Get one random cat image\n/v1/images/search?mime_types=gif - Get a random cat GIF\n/v1/breeds - Get a list of cat breeds\n/v1/breeds/siam - Get information about Siamese cats\n\n\n\n\n\n\n\nThe response will be shown in JSON format, which is a common data format used by APIs. JSON (JavaScript Object Notation) is a lightweight data-interchange format that is easy for humans to read and write and easy for machines to parse and generate.\n\n\n\n\n\nCode\nviewof method = Inputs.select([\"GET\"], {\n  label: \"HTTP Method\",\n  attributes: {\n  class: \"form-select mb-3\"\n  }\n})\n\nviewof endpoint = Inputs.text({\n  label: \"Endpoint path\", \n  placeholder: \"/v1/images/search?limit=1\",\n  value: \"/v1/images/search?limit=1\",\n  attributes: {\n  class: \"form-control mb-3\"\n  }\n})\n\n// Function to make the API request\nasync function fetchFromApi(method, path) {\n  const baseUrl = \"https://api.thecatapi.com\";\n  try {\n  const response = await fetch(`${baseUrl}${path}`);\n  const status = {\n    code: response.status,\n    ok: response.ok,\n    text: response.statusText\n  };\n  if (!response.ok) {\n    throw new Error(`HTTP error! status: ${response.status}`);\n  }\n  const data = await response.json();\n  return { data, status };\n  } catch (error) {\n  return {\n    data: { \"Message\": `Error: ${error.message}` },\n    status: {\n    code: 400,\n    ok: false,\n    text: \"Bad Request\"\n    }\n  };\n  }\n}\n\nresponse = {\n  const result = await fetchFromApi(method, endpoint);\n  return result;\n}\n\nviewof prettyResponse = {\n  let content;\n\n  if (response.data.Message) {\n    content = html`&lt;div class=\"alert alert-warning m-0\"&gt;${response.data.Message}&lt;/div&gt;`;\n  } else {\n    content = html`&lt;pre class=\"card-body m-0\" style=\"background-color: #f8f9fa; max-height: 400px; overflow-y: auto;\"&gt;${JSON.stringify(response.data, null, 2)}&lt;/pre&gt;`;\n  }\n\n  const badgeClass = response.status.ok ? \"bg-success\" : \"bg-danger\";\n\n  const container = html`&lt;div class=\"card\"&gt;\n  &lt;div class=\"card-header d-flex justify-content-between align-items-center\"&gt;\n  &lt;span&gt;Response&lt;/span&gt;\n  &lt;span class=\"badge ${badgeClass}\"&gt;${response.status.code} ${response.status.text}&lt;/span&gt;\n  &lt;/div&gt;\n  ${content}\n  &lt;/div&gt;`;\n  return container;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn fact, with the same structure, we can interact with multiple APIs. Let’s try interacting with The Metropolitan Museum of Art Collection API to get the a list of objects ids from the collection.\n\n\nCode\nviewof methodParts = Inputs.select([\"GET\"], {\n  label: \"HTTP Method\",\n  attributes: {\n    class: \"form-select mb-3\"\n  }\n})\n\nviewof domain = Inputs.text({\n  label: \"Domain\",\n  placeholder: \"collectionapi.metmuseum.org\",\n  value: \"collectionapi.metmuseum.org\",\n  attributes: {\n  class: \"form-control mb-3\"\n  }\n})\n\nviewof path = Inputs.text({\n  label: \"Path\",\n  placeholder: \"/public/collection/v1/search\",\n  value: \"/public/collection/v1/search\",\n  attributes: {\n    class: \"form-control mb-3\"\n  }\n})\n\nviewof query = Inputs.text({\n  label: \"Query parameters\",\n  placeholder: \"?q=cat\",\n  value: \"q=cat\",\n  attributes: {\n    class: \"form-control mb-3\"\n  }\n})\n\nasync function fetchFromApiParts(method, domain, path, query) {\n  try {\n    const baseUrl = `https://${domain}`;\n    const url = `${baseUrl}${path}?${query}`;\n    const response = await fetch(url);\n    const status = {\n    code: response.status,\n    ok: response.ok,\n    text: response.statusText\n    };\n    if (!response.ok) {\n    throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const data = await response.json();\n    return { data, status };\n  } catch (error) {\n    return {\n    data: { \"Message\": `Error: ${error.message}` },\n    status: {\n      code: 400,\n      ok: false,\n      text: \"Bad Request\"\n    }\n    };\n  }\n}\n\nresponseParts = {\n  const result = await fetchFromApiParts(methodParts, domain, path, query);\n  return result;\n}\n\nviewof prettyResponseParts = {\n  let content;\n  if (responseParts.data.Message) {\n    content = html`&lt;div class=\"alert alert-warning m-0\"&gt;${responseParts.data.Message}&lt;/div&gt;`;\n  } else {\n    content = html`&lt;pre class=\"card-body m-0\" style=\"background-color: #f8f9fa; max-height: 400px; overflow-y: auto;\"&gt;${JSON.stringify(responseParts.data, null, 2)}&lt;/pre&gt;`;\n  }\n  \n  const badgeClass = responseParts.status.ok ? \"bg-success\" : \"bg-danger\";\n  \n  const container = html`&lt;div class=\"card\"&gt;\n    &lt;div class=\"card-header d-flex justify-content-between align-items-center\"&gt;\n    &lt;span&gt;Response&lt;/span&gt;\n    &lt;span class=\"badge ${badgeClass}\"&gt;${responseParts.status.code} ${responseParts.status.text}&lt;/span&gt;\n    &lt;/div&gt;\n    ${content}\n  &lt;/div&gt;`;\n  return container;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, take any ID from the result and use it to get the object details from the API.\n\n\nCode\nviewof methodDetails = Inputs.select([\"GET\"], {\n  label: \"HTTP Method\",\n  attributes: {\n    class: \"form-select mb-3\"\n  }\n})\n\nviewof domainDetails = Inputs.text({\n  label: \"Domain\",\n  placeholder: \"collectionapi.metmuseum.org\",\n  value: \"collectionapi.metmuseum.org\",\n  attributes: {\n    class: \"form-control mb-3\"\n  }\n})\n\nviewof pathDetails = Inputs.text({\n  label: \"Path\",\n  placeholder: \"/public/collection/v1/objects/\",\n  value: \"/public/collection/v1/objects/\",\n  attributes: {\n    class: \"form-control mb-3\"\n  }\n})\n\nviewof parameterDetails = Inputs.text({\n  label: \"Parameter\",\n  placeholder: \"Write the object id here\",\n  value: \"570744\",\n  attributes: {\n    class: \"form-control mb-3\"\n  }\n})\n\nasync function fetchFromApiDetails(method, domain, path, parameter) {\n  try {\n    const baseUrl = `https://${domain}`;\n    const url = `${baseUrl}${path}${parameter}`;\n    const response = await fetch(url);\n    const status = {\n    code: response.status,\n    ok: response.ok,\n    text: response.statusText\n    };\n    if (!response.ok) {\n    throw new Error(`HTTP error! status: ${response.status}`);\n    }\n    const data = await response.json();\n    return { data, status };\n  } catch (error) {\n    return { error: error.message };\n  }\n}\n\nresponseDetails = {\n  const result = await fetchFromApiDetails(methodDetails, domainDetails, pathDetails, parameterDetails);\n  return result;\n}\n\nviewof prettyResponseDetailsContainer = {\n  let content;\n  if (responseDetails.data.Message) {\n    content = html`&lt;div class=\"alert alert-warning m-0\"&gt;${responseDetails.data.Message}&lt;/div&gt;`;\n  } else {\n    content = html`&lt;pre class=\"card-body m-0\" style=\"background-color: #f8f9fa; max-height: 400px; overflow-y: auto;\"&gt;${JSON.stringify(responseDetails.data, null, 2)}&lt;/pre&gt;`;\n  }\n\n  const badgeClass = responseDetails.status.ok ? \"bg-success\" : \"bg-danger\";\n\n  const container = html`&lt;div class=\"card\"&gt;\n    &lt;div class=\"card-header d-flex justify-content-between align-items-center\"&gt;\n    &lt;span&gt;Response&lt;/span&gt;\n    &lt;span class=\"badge ${badgeClass}\"&gt;${responseDetails.status.code} ${responseDetails.status.text}&lt;/span&gt;\n    &lt;/div&gt;\n    ${content}\n  &lt;/div&gt;`;\n  return container;\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd that allows us to retrieve, for instance, the image of the object, that is stored in the primaryImage field.\n\n\nCode\nviewof primaryImage = {\n  const primaryImage = responseDetails.status.ok ? responseDetails.data.primaryImageSmall : \"https://placehold.co/600x400\";\n  if (primaryImage) {\n    const img = html`&lt;img src=\"${primaryImage}\" alt=\"Primary Image\"&gt;`;\n    return img;\n  } else {\n    return html`&lt;img src=\"https://placehold.co/600x400\" alt=\"Placeholder\"&gt;`;\n  }\n\n}\n\n\n\n\n\n\n\nNice! Now, let’s do an exercise to practice what we have learned.",
    "crumbs": [
      "Interacting with APIs",
      "Interacting with APIs"
    ]
  }
]