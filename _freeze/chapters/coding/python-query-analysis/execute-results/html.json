{
  "hash": "49ad648359b203644ab1915af256cf1e",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Query and Analyze API Data with Python\"\nexecute:\n  echo: true\nformat:\n  html:\n    code-overflow: wrap\nlightbox: true\n---\n\n\n\nAll right, enough setup! Now we can start making requests and exploring the data.\n\n:::{.callout-note appearance=\"simple\" collapse=\"false\"}\n## Working with APIs is an iterative process\nWhat we're showing here is the result of several rounds of testing and refining. Don't feel discouraged if your first experiments don't work as expected—it's normal to encounter errors and surprising results when working with APIs. The important thing is to keep experimenting, learning from the errors, and refining your approach.\n:::\n\n## The shape of data\n\nThe first thing we want to do is get a sense of the shape of the data we're getting from the API. One interesting question is: how are items distributed across time? There are multiple ways to explore this, but perhaps the most straightforward is to use [`facets`](https://pro.dp.la/developers/requests#faceting){target=\"_blank\"} to get the distribution of items by year.\n\nAdditionally, we'll filter the results to only include titles, descriptions, and dates. This makes the data easier to work with and reduces the amount we need to process.\n\n::: {#4f33fad3 .cell execution_count=2}\n``` {.python .cell-code}\n# Defining the fields, facets, and filters outside the function call for better readability\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\"\n}\n\ntry:\n    ai_search = search_items(\n        \"artificial AND intelligence\", # search query\n        fields=_join_list(fields, sep=\",\"), # fields to include in the response\n        facets=\"sourceResource.date.begin\", # facets to include in the response\n        **dotted_fields, # additional parameters (e.g. filters),\n        page_size=5, # items per page\n        sort_by=\"sourceResource.date.begin\", # sort by date\n        sort_order=\"asc\", # oldest to newest\n        page=1, # page number to retrieve\n        verbose=True # print the request URL for debugging purposes\n        )\n    \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n    r = httpx.get(f\"{FALLBACK_DATA_URL}dpla_search_results.json\")\n    r.raise_for_status()\n    ai_search = r.json()\n\n\n# download the preloaded data for the next steps\nprint(f\"{ai_search.get('count')} results found.\") if isinstance(ai_search, dict) else print(f\"{len(ai_search)} results found.\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nRequest URL [redacted]: https://api.dp.la/v2/items?q=artificial+AND+intelligence&fields=sourceResource.title%2CsourceResource.description%2CsourceResource.date.begin%2CsourceResource.date.end&facets=sourceResource.date.begin&sourceResource.subject.name=artificial+intelligence&page_size=5&sort_by=sourceResource.date.begin&sort_order=asc&page=1\n728 results found.\n```\n:::\n:::\n\n\nIf the request was successful, you should see a redacted version of the request URL and the number of results found. If there was an error (e.g. invalid API key, network error, etc.), you should see an error message and the preloaded data will be used instead.\n\n:::{.callout-tip appearance=\"simple\" collapse=\"false\"}\nNote how the result count is significantly higher than the result from the web interface. The exact reason isn't fully documented, but one possibility is that the web interface and API handle field matching differently (perhaps with different case sensitivity or tokenization). This is a good example of how different interfaces to the same data can yield different results, and why it's important to understand the underlying data and how it's indexed.\n:::\n\n### Let's play with the facets!\n\nSomething that stands out in the response is the facet counts. Facets can be understood as a way to get aggregated counts of items based on specific fields. In this case, we're getting the count of items by year (based on the `sourceResource.date.begin` field).\n\nTo explore the facet counts, we can extract the facet information from the response and plot it using a bar chart. This will let us visualize the distribution of items across time.\n\nThe first thing we need to do is reach the facet information nested in the response. To know how, we need to explore the [documentation](https://pro.dp.la/developers/object-structure){target=\"_blank\"} and understand the object structure.\n\nAccording to the documentation, the way to reach our facet information is following this structure:\n\n```\nai_search\n├── facets (dict)\n│   └── sourceResource.date.begin (dict)\n│       └── entries (list)\n│           ├── count (int)\n│           └── label (str)\n```\n\nTherefore, if we want to get the entries of the `sourceResource.date.begin` facet, we can do it with the following code:\n\n::: {#5c2fde52 .cell execution_count=3}\n``` {.python .cell-code}\nfacets_entries = ai_search.get(\"facets\", {}).get(\"sourceResource.date.begin\", {}).get(\"entries\", [])\n\n# Print a sample (we use 'time' because that's the label for date facets)\nfor entry in facets_entries[:5]:\n    print(f\"Year: {entry.get('time')}, Count: {entry.get('count')}\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nYear: 2026, Count: 4\nYear: 2025, Count: 119\nYear: 2024, Count: 95\nYear: 2023, Count: 77\nYear: 2022, Count: 23\n```\n:::\n:::\n\n\nNow that we understand better the structure of the facet information, we can visualize it using a bar chart. We will use the `seaborn` library to create the bar chart, but you can use any other library you prefer (e.g. `matplotlib`, `plotly`, etc.).\n\n::: {#e1bc5951 .cell execution_count=4}\n``` {.python .cell-code}\n# Extract the year and count information from the facet entries\nyears = [entry.get(\"time\") for entry in facets_entries][::-1] # We use [::-1] to reverse the order\ncounts = [entry.get(\"count\") for entry in facets_entries][::-1]\n\n# Create a bar chart using seaborn\nsns.barplot(x=years, y=counts)\nplt.xlabel('Year')\nplt.ylabel('Number of Items')\nplt.title('Items about Artificial Intelligence by Year')\nplt.xticks(range(0, len(years), 5), [years[i] for i in range(0, len(years), 5)], rotation=45)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](python-query-analysis_files/figure-html/cell-5-output-1.png){width=662 height=470}\n:::\n:::\n\n\nThis graph helps us understand the distribution of items about artificial intelligence across different years. We can see that between 2018 and 2022 there is a peak in the number of items, and a significant increase after 2023. Before that, the number of items per year is relatively low (fewer than 10 per year).\n\n## Further exploration\n\nWith this information, we can establish three periods of interest: before 2018, between 2018 and 2022, and after 2023. We'll set up a list with these periods and use it to filter items for further analysis:\n\n::: {#55686632 .cell execution_count=5}\n``` {.python .cell-code}\nperiods = [\n    (\"preCovid\", 1844, 2018),\n    (\"Covid\", 2019, 2021),\n    (\"postCovid\", 2022, 2026),\n]\n```\n:::\n\n\nLet's create a pool of items for each period using the `search_all_items` function we defined earlier. We'll use the `sourceResource.date.begin` field to filter the items by year:\n\n::: {#6b61a767 .cell execution_count=6}\n``` {.python .cell-code}\nai_results = {}\n\nfields = [\n    \"sourceResource.title\",\n    \"sourceResource.description\",\n    \"sourceResource.date.begin\",\n    \"sourceResource.date.end\"\n]\n\ndotted_fields = {\n    \"sourceResource.subject.name\": \"artificial intelligence\",\n}\n\ntry:\n    for period_name, start_date, end_date in tqdm(periods):\n        ai_results[period_name] = search_all_items(\n            \"artificial AND intelligence\", # search query\n            max_items=5, # maximum number of items to retrieve for each period\n            fields=_join_list(fields, sep=\",\"), # fields to include in the response\n            facets=\"sourceResource.date.begin\", # Retrieve facets for date ranges\n            page_size=10, # items per page\n            **dotted_fields, # filter to ensure results are about AI, not just using AI in metadata\n            **{\"sourceResource.date.after\": str(start_date)}, # Between year\n            **{\"sourceResource.date.before\": str(end_date)}, # and Year\n            sort_by=\"sourceResource.date.begin\", # sort by date\n            sort_order=\"asc\", # oldest to newest\n            verbose=False \n        )\n        \nexcept httpx.HTTPStatusError as e:\n    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n    r = httpx.get(f\"{FALLBACK_DATA_URL}ai_results_by_wave.json\")\n    r.raise_for_status()\n    ai_results = r.json()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\r  0%|          | 0/3 [00:00<?, ?it/s]\r 33%|███▎      | 1/3 [00:00<00:00,  2.83it/s]\r 67%|██████▋   | 2/3 [00:00<00:00,  3.46it/s]\r100%|██████████| 3/3 [00:00<00:00,  3.82it/s]\r100%|██████████| 3/3 [00:00<00:00,  3.63it/s]\n```\n:::\n:::\n\n\nThis code will create a dictionary called `ai_results` where the keys are the period names (e.g. \"preCovid\", \"Covid\", \"postCovid\") and the values are lists of items that match the search query and date filters for each period. Let's print a summary of the results to see how many items we got for each period:\n\n::: {#7ba058c1 .cell execution_count=7}\n``` {.python .cell-code}\nai_results_summary = {period: len(items) for period, items in ai_results.items()}\nprint(\"AI Results Summary by Period:\")\nfor period, count in ai_results_summary.items():\n    print(f\"{period}: {count} items\")\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nAI Results Summary by Period:\npreCovid: 5 items\nCovid: 5 items\npostCovid: 5 items\n```\n:::\n:::\n\n\nWe can see a significant increase in the number of items in the \"postCovid\" period compared to the earlier periods. This reflects the AI boom that has been happening in recent years—which was likely accelerated (though not solely caused) by the COVID-19 pandemic.\n\n## Extracting keywords with YAKE\n\nOur last step is to extract keywords from the titles and descriptions of items in each period using the YAKE library. YAKE (Yet Another Keyword Extractor) is a simple and effective unsupervised keyword extraction method.\n\nBut first, let's explore the structure of the `sourceResource` field in our items:\n\n::: {#dc1bc535 .cell execution_count=8}\n``` {.python .cell-code}\n# Get a sample item to explore the structure of the sourceResource field\nai_results.get(\"postCovid\")[:5]\n```\n\n::: {.cell-output .cell-output-display execution_count=19}\n```\n[{'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['In scope of the U.S. Government Publishing Office Cataloging and Indexing Program (C&I) and Federal Depository Library Program (FDLP).',\n   'Description based on online resource; title from publication (Agency website, viewed September 14, 2022).'],\n  'sourceResource.title': 'Artificial intelligence strategic plan: fiscal years 2023-2027, draft report for comment'},\n {'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['\"Published Fall 2022.\"',\n   '\"Ann Caracristi Institute for Intelligence Research\"--Cover.',\n   'In scope of the U.S. Government Publishing Office Cataloging and Indexing Program (C&I) and Federal Depository Library Program (FDLP).',\n   'Includes bibliographical references (pages 69-72).',\n   'Description based on online resource; title from PDF title page (NIU, viewed July 18, 2023).'],\n  'sourceResource.title': 'Perceptions of artificial intelligence / machine learning in the intelligence community : a systematic review of the literature'},\n {'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['In scope of the U.S. Government Publishing Office Cataloging and Indexing Program (C&I) and Federal Depository Library Program (FDLP).',\n   '\"FHWA-HRT-21-081.\"',\n   'Description based on online resource; title from publication (Agency website, viewed December 22, 2023).'],\n  'sourceResource.title': 'Effectiveness of TMC AI applications in case studies'},\n {'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['Access ID (govinfo): CHRG-117hhrg46195.',\n   '\"Serial no. 117-52.\"',\n   'Includes bibliographical references.',\n   'Hearing witnesses: Broussard, Meredith, Associate Professor, Arthur L. Carter Journalism Institute of New York University; Cooper, Aaron, Vice President, Global Policy, BSA--The Software Alliance; King, Meg, Director, Science and Technology Innovation Program, The Wilson Center; Vogel, Miriam, President and CEO, EqualAI; Yong, Jeffery, Principal Advisor, Financial Stability Institute, Bank for International Settlements.',\n   'Date of hearing: 2021-10-13.',\n   'Description based on online resource; title from PDF title page (govinfo, viewed Jan. 19, 2022).'],\n  'sourceResource.title': 'Beyond I, robot : ethics, artificial intelligence, and the digital age : viritual hearing before the Task Force on Artificial Intelligence of the Committee on Financial Services, U.S. House of Representatives, One Hundred Seventeenth Congress, first session, October 13, 2021'},\n {'sourceResource.date.begin': '2022',\n  'sourceResource.date.end': '2022',\n  'sourceResource.description': ['Access ID (govinfo): CHRG-117hhrg47880.',\n   'Hearing witnesses: Greenfield, Kevin, Deputy Comptroller for Operational Risk Policy, Office of the Comptroller of the Currency (OCC); Hall, Melanie, Commissioner, Division of Banking and Financial Institutions, State of Montana; and Chair, Board of Directors, Conference of State Bank Supervisors (CSBS); Lay, Kelly J., Director, Office of Examination and Insurance, National Credit Union Administration (NCUA); Rusu, Jessica, Chief Data, Information and Intelligence Officer, Financial Conduct Authority (FCA), United Kingdom.',\n   'Date of hearing: 2022-05-13.',\n   'Includes bibliographical references.',\n   '\"Serial no. 117-85.\"',\n   'Description based on online resource; title from PDF title page (GovInfo, viewed Aug. 2, 2022).'],\n  'sourceResource.title': 'Keeping up with the codes : using AI for effective RegTech : hybrid hearing before the Task Force on Artificial Intelligence of the Committee on Financial Services, U.S. House of Representatives, One Hundred Seventeenth Congress, second session, May 13, 2022'}]\n```\n:::\n:::\n\n\nGreat! Each item is a dictionary with the following keys:\n\n- `sourceResource.date.begin` (str): The start date of the item (e.g. \"2015-01-01\")\n- `sourceResource.date.end` (str): The end date of the item (e.g. \"2015-12-31\")\n- `sourceResource.description` (str | list): A description of the item\n- `sourceResource.title` (str | list): The title of the item\n\nNow we can use YAKE to extract keywords from the titles and descriptions of the items in each period. We'll create a function that takes a list of items and returns a dictionary with keyword counts for each period:\n\n::: {#7091e20c .cell execution_count=9}\n``` {.python .cell-code}\ndef extract_keywords(items, skip=None, ngram=2, max_keywords=5, language=\"en\"):\n    \"\"\"Extract keywords from a list of items using YAKE.\"\"\"\n\n    ai_keywords = {}\n\n    kw_extractor = yake.KeywordExtractor(lan=language, n=ngram, top=max_keywords)\n\n    if skip:\n        skip_keywords = set(skip)\n\n    for period, items in tqdm(items.items(), desc=\"Extracting keywords\"):\n        period_keywords = {}\n        for item in items:\n            title = _join_list(item.get(\"sourceResource.title\", \"\"))\n            description = _join_list(item.get(\"sourceResource.description\", \"\"))\n            text = f\"{title} {description}\".lower()\n\n            keywords = kw_extractor.extract_keywords(text)\n            for kw, score in keywords:\n                if skip and kw in skip_keywords:\n                    continue\n                period_keywords[kw] = period_keywords.get(kw, 0) + 1\n                \n        ai_keywords[period] = period_keywords\n    \n    return ai_keywords\n```\n:::\n\n\nNow, we can iterate over the loaded items and extract the keywords for each period:\n\n::: {#ba7ef49a .cell execution_count=10}\n``` {.python .cell-code}\n# Define a list of common words to skip (optional)\nskip_words = [\"artificial intelligence\", \"ai\", \"intelligence\", \"artificial\"]\n\ntop = 10\n\nai_keywords = extract_keywords(ai_results, skip=skip_words, ngram=2, max_keywords=top)\n\nfor period, keywords in ai_keywords.items():\n    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:top]\n    print(f\"Top {top} keywords for {period}:\")\n    for kw, count in sorted_keywords:\n        print(f\"  {kw}: {count}\")\n    print()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\rExtracting keywords:   0%|          | 0/3 [00:00<?, ?it/s]\rExtracting keywords: 100%|██████████| 3/3 [00:00<00:00, 79.24it/s]\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nTop 10 keywords for preCovid:\n  computer: 2\n  des combinaisons: 1\n  nouvelle application: 1\n  vapeur pour: 1\n  pour remplacer: 1\n  syste: 1\n  nie: 1\n  noiaphanisme: 1\n  combinaisons: 1\n  nouvelle: 1\n\nTop 10 keywords for Covid:\n  description based: 4\n  online resource: 4\n  includes bibliographical: 3\n  bibliographical references.: 3\n  national security: 3\n  web site: 3\n  title screen: 3\n  nscai web: 2\n  security commission: 2\n  report description: 2\n\nTop 10 keywords for postCovid:\n  government publishing: 3\n  description based: 3\n  online resource: 3\n  agency website: 2\n  indexing program: 2\n  financial services: 2\n  viewed september: 1\n  strategic plan: 1\n  fiscal years: 1\n  years 2023-2027: 1\n\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n\n```\n:::\n:::\n\n\nFinally, we can visualize the top keywords for each period using a bar chart:\n\n::: {#4b617b89 .cell fig-height='5' fig-width='14' message='false' execution_count=11}\n``` {.python .cell-code}\nfig, axes = plt.subplots(1, 3, figsize=(14, 5), sharex=False)\n\nfor ax, (period_name, start_year, end_year) in zip(axes, periods):\n    data = top_n(ai_keywords.get(period_name, {}), 10)\n    terms = list(data.keys())\n    counts = list(data.values())\n\n    sns.barplot(x=counts, y=terms, ax=ax, palette=\"viridis\", hue=counts)\n    ax.set_title(f\"{period_name} ({start_year}–{end_year})\")\n    ax.set_xlabel(\"Frequency\")\n\nplt.suptitle(\"How 'Artificial Intelligence' appears across time in DPLA\", fontsize=14)\nplt.tight_layout()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](python-query-analysis_files/figure-html/cell-12-output-1.png){width=1333 height=475}\n:::\n:::\n\n\n## Wrapping up\n\nAnd that's it! You've successfully queried the DPLA API, filtered and paginated through results, visualized temporal distributions using facets, and extracted keywords from metadata across different time periods. Along the way, you've seen how APIs let you access much richer datasets than web interfaces alone—and how a little code can unlock powerful ways to explore and analyze cultural heritage collections.\n\nThe techniques you've learned here—building requests programmatically, handling pagination, working with nested JSON structures, and combining API data with text analysis—are transferable to many other APIs. Whether you're exploring museum collections, scientific datasets, or social media archives, the same patterns apply: understand the documentation, experiment with parameters, and iterate on your queries.\n\nKeep exploring, and remember: every API is a doorway to data that's waiting to be discovered.\n\n",
    "supporting": [
      "python-query-analysis_files"
    ],
    "filters": [],
    "includes": {}
  }
}