{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fde013d",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/UCSB-Library-Research-Data-Services/intro2APIs/blob/main/chapters/coding/exercise-DLPA.ipynb\" style='text-decoration: none;'>\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a> \n",
    "\n",
    "[![](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/UCSB-Library-Research-Data-Services/intro2APIs/main?urlpath=%2Fdoc%2Ftree%2Fchapters%2Fcoding%2Fexercise-DLPA.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b08c21",
   "metadata": {},
   "source": [
    "# DPLA API Exercise: Exploring Artificial Intelligence in Cultural Heritage\n",
    "\n",
    "This notebook demonstrates how to interact with the Digital Public Library of America (DPLA) API to query, analyze, and visualize cultural heritage data about artificial intelligence across different time periods.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Set up and configure a Python environment for API interaction\n",
    "- Make authenticated requests to the DPLA API\n",
    "- Handle pagination to collect large datasets\n",
    "- Use facets to analyze temporal distributions\n",
    "- Extract and visualize keywords from metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9167aa26",
   "metadata": {},
   "source": [
    "### Import Libraries\n",
    "\n",
    "Import all necessary libraries for API interaction, data processing, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c1b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install httpx yake tqdm matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340d07f",
   "metadata": {},
   "source": [
    "### Install Required Packages\n",
    "\n",
    "We'll use `httpx` for HTTP requests, `yake` for keyword extraction, `tqdm` for progress bars, and `matplotlib` for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133b1fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "from getpass import getpass\n",
    "\n",
    "import httpx\n",
    "from urllib.parse import urlsplit, parse_qsl, urlencode, urlunsplit\n",
    "import yake\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb90f76",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "### Define Constants\n",
    "\n",
    "Set up base URLs and configuration variables that will be used throughout the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c6dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_BASE_URL = \"https://api.dp.la/v2/\" \n",
    "ENV_VAR_NAME = \"DPLA_API_KEY\" \n",
    "FALLBACK_DATA_URL = \"https://raw.githubusercontent.com/UCSB-Library-Research-Data-Services/intro2APIs/main/data/\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f33bc99c",
   "metadata": {},
   "source": [
    "### Secure API Key Setup\n",
    "\n",
    "Store your API key securely using environment variables. This approach avoids hardcoding sensitive credentials in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc393e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "key= os.getenv(ENV_VAR_NAME)\n",
    "\n",
    "# This method avoids hardcoding the API key in the script\n",
    "# The variable is persistent during the session\n",
    "if not key:\n",
    "    key = getpass(f\"Enter your DPLA API key: \").strip()\n",
    "    if not key:\n",
    "        raise ValueError(\"No API key provided.\")\n",
    "    os.environ[ENV_VAR_NAME] = key\n",
    "    \n",
    "print(f\"API key set in environment variable {ENV_VAR_NAME}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61724a14",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "These utility functions help keep our code clean and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fd690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _join_list(x, sep=\"; \", keep_first_only=False):\n",
    "    \"\"\"\n",
    "    Helper function to join a list of values into a single string. If the input is not a list, it will return the string representation of the input. If the input is None, it will return an empty string.\n",
    "    \"\"\"\n",
    "    if isinstance(x, list):\n",
    "        if keep_first_only and len(x) > 0:\n",
    "            return str(x[0])\n",
    "        return sep.join(str(v) for v in x if v is not None)\n",
    "    return \"\" if x is None else str(x)\n",
    "\n",
    "def _top_n(d, n=10):\n",
    "    \"\"\"Helper function to return the top n items from a dictionary, sorted by value in descending order.\"\"\"\n",
    "    return dict(sorted(d.items(), key=lambda x: x[1], reverse=True)[:n])\n",
    "\n",
    "def _redact_request_url(url):\n",
    "    \"\"\"Remove the api_key parameter from the URL for display purposes.\"\"\"\n",
    "    parsed_url = urlsplit(str(url))  # Convert httpx.URL to string\n",
    "    query_params = parse_qsl(parsed_url.query)\n",
    "    filtered_params = [(name, value) for name, value in query_params if name != \"api_key\"]\n",
    "    redacted_query = urlencode(filtered_params)\n",
    "    redacted_url = parsed_url._replace(query=redacted_query)\n",
    "    return urlunsplit(redacted_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cb4148",
   "metadata": {},
   "source": [
    "### URL Redaction Helper\n",
    "\n",
    "Removes API keys from URLs before printing (security best practice)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a0cdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_items(query, resource_type='items', verbose=False, timeout=30.0, **parameters):\n",
    "    \"\"\"\n",
    "    Search DPLA items with given query and parameters.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query string. It's possible to use logical operators (AND, OR, NOT). Additionally, you can use wildcards (*) for partial matches.\n",
    "        resource_type (str): The type of resource to search for. Default is 'items'.\n",
    "        verbose (bool): If True, prints the request URL. Default is False.\n",
    "        timeout (float): The timeout for the HTTP request in seconds. Default is 30.0.\n",
    "        **parameters: Facets and filter parameters from the DPLA API documentation: https://pro.dp.la/developers/requests\n",
    "                    Dotted keywords and values can be passed using dictionary unpacking. For example, to filter by sourceResource.title, you can pass:\n",
    "                    **{\"sourceResource.title\": \"example title\"}\n",
    "    Returns:\n",
    "        dict: The JSON response from the DPLA API as a Python dictionary.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build the request URL and minimal parameters\n",
    "    base_url = f\"{API_BASE_URL}{resource_type}\"\n",
    "    params = {\n",
    "        \"q\": query,\n",
    "        \"api_key\": os.getenv(ENV_VAR_NAME),\n",
    "    }\n",
    "    \n",
    "    # Add additional parameters if any\n",
    "    for key, value in parameters.items():\n",
    "        params[key] = value\n",
    "        \n",
    "    # Make the request\n",
    "    with httpx.Client(timeout=timeout) as client:\n",
    "        response = client.get(base_url, params=params)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Request URL [redacted]: {_redact_request_url(response.url)}\")\n",
    "    \n",
    "    response.raise_for_status() \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba55b78c",
   "metadata": {},
   "source": [
    "## Main API Functions\n",
    "\n",
    "### Search Items Function\n",
    "\n",
    "This function queries the DPLA API and returns a single page of results. It handles query parameters, facets, and filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487aedcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_all_items(query, resource_type='items', max_items=100, sleep=0.5, verbose=False, timeout=30.0, **parameters):\n",
    "    \"\"\"\n",
    "    Collect up to max_items across pages.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The search query string. It's possible to use logical operators \n",
    "            (AND, OR, NOT). Additionally, you can use wildcards (*) for partial matches.\n",
    "        max_items (int): Maximum number of items to retrieve. For number of elements per page, \n",
    "            use the page_size parameter in **parameters.\n",
    "        sleep (float): Time to wait between requests to avoid hitting rate limits.\n",
    "        **parameters: Facets and filter parameters from the DPLA API documentation: https://pro.dp.la/developers/requests\n",
    "    \"\"\"\n",
    "    all_docs = []\n",
    "    page = 1\n",
    "    page_size = int(parameters.get(\"page_size\", 100))\n",
    "    if page_size > 500:\n",
    "        page_size = 500\n",
    "        print(\"page_size cannot exceed 500. Setting to 500.\")\n",
    "        \n",
    "    while len(all_docs) < max_items:\n",
    "        parameters['page'] = page\n",
    "        data = search_items(\n",
    "            query,\n",
    "            resource_type=resource_type,\n",
    "            verbose=verbose,\n",
    "            timeout=timeout,\n",
    "            **parameters\n",
    "        )\n",
    "        docs = data.get('docs', [])\n",
    "        if not docs:\n",
    "            break  # No more results\n",
    "        all_docs.extend(docs)\n",
    "        \n",
    "        # stop if we've reached max_items\n",
    "        if len(all_docs) >= max_items:\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(sleep)\n",
    "        \n",
    "    return all_docs[:max_items]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017a5581",
   "metadata": {},
   "source": [
    "### Search All Items Function\n",
    "\n",
    "This function handles pagination and collects multiple pages of results up to a specified maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf424050",
   "metadata": {},
   "source": [
    "ðŸŽ¯ **Challenge 1: Experimenting with Pagination**\n",
    "\n",
    "Let's explore how pagination works! APIs often return results in \"pages\" to avoid overwhelming the server and your computer with too much data at once.\n",
    "\n",
    "In the cell below, try changing `page_size=5` to `page_size=10` and `page=1` to `page=2`. Before running it, think about: which items will you see now? Will they be the same as before, or different ones?\n",
    "\n",
    "**Bonus:** What happens if you try `page_size=1000`? (Hint: Check the output message!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4998341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the fields, facets, and filters outside the function call for better readability\n",
    "fields = [\n",
    "    \"sourceResource.title\",\n",
    "    \"sourceResource.description\",\n",
    "    \"sourceResource.date.begin\",\n",
    "    \"sourceResource.date.end\"\n",
    "]\n",
    "\n",
    "dotted_fields = {\n",
    "    \"sourceResource.subject.name\": \"artificial intelligence\"\n",
    "}\n",
    "\n",
    "try:\n",
    "    ai_search = search_items(\n",
    "        \"artificial AND intelligence\", # search query\n",
    "        fields=_join_list(fields, sep=\",\"), # fields to include in the response\n",
    "        facets=\"sourceResource.date.begin\", # facets to include in the response\n",
    "        **dotted_fields, # additional parameters (e.g. filters),\n",
    "        page_size=5, # items per page\n",
    "        sort_by=\"sourceResource.date.begin\", # sort by date\n",
    "        sort_order=\"asc\", # oldest to newest\n",
    "        page=1, # page number to retrieve\n",
    "        verbose=True # print the request URL for debugging purposes\n",
    "        )\n",
    "    \n",
    "except httpx.HTTPStatusError as e:\n",
    "    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n",
    "    r = httpx.get(f\"{FALLBACK_DATA_URL}dpla_search_results.json\")\n",
    "    r.raise_for_status()\n",
    "    ai_search = r.json()\n",
    "\n",
    "\n",
    "# download the preloaded data for the next steps\n",
    "print(f\"{ai_search.get('count')} results found.\") if isinstance(ai_search, dict) else print(f\"{len(ai_search)} results found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc486acc",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "### Initial Search Query\n",
    "\n",
    "Let's make our first request to explore items about artificial intelligence. We'll use facets to understand the temporal distribution of items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a71dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "facets_entries = ai_search.get(\"facets\", {}).get(\"sourceResource.date.begin\", {}).get(\"entries\", [])\n",
    "\n",
    "# Print a sample (we use 'time' because that's the label for date facets)\n",
    "for entry in facets_entries[:5]:\n",
    "    print(f\"Year: {entry.get('time')}, Count: {entry.get('count')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3571b1",
   "metadata": {},
   "source": [
    "### Visualizing Temporal Distribution\n",
    "\n",
    "Facets provide aggregated counts by specific fields. Let's visualize how AI-related items are distributed across time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e380a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year and count information from the facet entries\n",
    "years = [entry.get(\"time\") for entry in facets_entries][::-1] # We use [::-1] to reverse the order\n",
    "counts = [entry.get(\"count\") for entry in facets_entries][::-1]\n",
    "\n",
    "# Create a bar chart using seaborn\n",
    "sns.barplot(x=years, y=counts)\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Items')\n",
    "plt.title('Items about Artificial Intelligence by Year')\n",
    "plt.xticks(range(0, len(years), 5), [years[i] for i in range(0, len(years), 5)], rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8321ea24",
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = [\n",
    "    (\"preCovid\", 1844, 2018),\n",
    "    (\"Covid\", 2019, 2021),\n",
    "    (\"postCovid\", 2022, 2026),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b7c40",
   "metadata": {},
   "source": [
    "## Comparative Analysis Across Time Periods\n",
    "\n",
    "### Define Time Periods\n",
    "\n",
    "Based on the temporal distribution, we can identify three distinct periods of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3be4bf",
   "metadata": {},
   "source": [
    "ðŸŽ¯ **Challenge 2: Testing Date Filters**\n",
    "\n",
    "Before we collect data for all three periods, let's practice using date filters with a single year! Date filters help you narrow down API results to specific time ranges.\n",
    "\n",
    "Try creating a new cell below this one and write a `search_items()` call that retrieves items from **2020 only**. \n",
    "\n",
    "**Hint:** Use these parameters:\n",
    "```python\n",
    "**{\"sourceResource.date.after\": \"2020\", \"sourceResource.date.before\": \"2020\"}\n",
    "```\n",
    "\n",
    "Before running your query, predict: How many results do you expect compared to the full Covid period (2019-2021)? Will it be more, less, or about the same?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460afff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a pool of results for each period\n",
    "ai_results = {}\n",
    "\n",
    "fields = [\n",
    "    \"sourceResource.title\",\n",
    "    \"sourceResource.description\",\n",
    "    \"sourceResource.date.begin\",\n",
    "    \"sourceResource.date.end\"\n",
    "]\n",
    "\n",
    "dotted_fields = {\n",
    "    \"sourceResource.subject.name\": \"artificial intelligence\",\n",
    "}\n",
    "\n",
    "try:\n",
    "    for period_name, start_date, end_date in tqdm(periods):\n",
    "        ai_results[period_name] = search_all_items(\n",
    "            \"artificial AND intelligence\", # search query\n",
    "            max_items=400, # maximum number of items to retrieve for each period\n",
    "            fields=_join_list(fields, sep=\",\"), # fields to include in the response\n",
    "            facets=\"sourceResource.date.begin\", # Retrieve facets for date ranges\n",
    "            page_size=100, # items per page\n",
    "            **dotted_fields, # filter to ensure results are about AI, not just using AI in metadata\n",
    "            **{\"sourceResource.date.after\": str(start_date)}, # Between year\n",
    "            **{\"sourceResource.date.before\": str(end_date)}, # and Year\n",
    "            sort_by=\"sourceResource.date.begin\", # sort by date\n",
    "            sort_order=\"asc\", # oldest to newest\n",
    "            verbose=False \n",
    "        )\n",
    "        \n",
    "except httpx.HTTPStatusError as e:\n",
    "    print(f\"HTTP error occurred: {e}. Using preloaded data instead.\")\n",
    "    r = httpx.get(f\"{FALLBACK_DATA_URL}ai_results_by_wave.json\")\n",
    "    r.raise_for_status()\n",
    "    ai_results = r.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62100e04",
   "metadata": {},
   "source": [
    "### Collect Items for Each Period\n",
    "\n",
    "Fetch up to 3000 items for each time period using date filters and pagination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c0491",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_results_summary = {period: len(items) for period, items in ai_results.items()}\n",
    "print(\"AI Results Summary by Period:\")\n",
    "for period, count in ai_results_summary.items():\n",
    "    print(f\"{period}: {count} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0bd3ba",
   "metadata": {},
   "source": [
    "### Summary of Results by Period\n",
    "\n",
    "Check how many items we retrieved for each time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfb54d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ai_results.get(\"postCovid\")[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e1471",
   "metadata": {},
   "source": [
    "### Exploring Item Structure\n",
    "\n",
    "Let's examine the structure of a sample item to understand what metadata is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41b71b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(items, skip=None, ngram=2, max_keywords=5, language=\"en\"):\n",
    "    \"\"\"Extract keywords from a list of items using YAKE.\"\"\"\n",
    "\n",
    "    ai_keywords = {}\n",
    "\n",
    "    kw_extractor = yake.KeywordExtractor(lan=language, n=ngram, top=max_keywords)\n",
    "\n",
    "    if skip:\n",
    "        skip_keywords = set(skip)\n",
    "\n",
    "    for period, items in tqdm(items.items(), desc=\"Extracting keywords\"):\n",
    "        period_keywords = {}\n",
    "        for item in items:\n",
    "            title = _join_list(item.get(\"sourceResource.title\", \"\"))\n",
    "            description = _join_list(item.get(\"sourceResource.description\", \"\"))\n",
    "            text = f\"{title} {description}\".lower()\n",
    "\n",
    "            keywords = kw_extractor.extract_keywords(text)\n",
    "            for kw, score in keywords:\n",
    "                if skip and kw in skip_keywords:\n",
    "                    continue\n",
    "                period_keywords[kw] = period_keywords.get(kw, 0) + 1\n",
    "                \n",
    "        ai_keywords[period] = period_keywords\n",
    "    \n",
    "    return ai_keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af44f367",
   "metadata": {},
   "source": [
    "## Keyword Extraction\n",
    "\n",
    "### Extract Keywords Using YAKE\n",
    "\n",
    "Use YAKE (Yet Another Keyword Extractor) to identify important terms in titles and descriptions for each period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101f65d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of common words to skip (optional)\n",
    "skip_words = [\"artificial intelligence\", \"ai\", \"intelligence\", \"artificial\"]\n",
    "\n",
    "top = 10\n",
    "\n",
    "ai_keywords = extract_keywords(ai_results, skip=skip_words, ngram=2, max_keywords=top)\n",
    "\n",
    "for period, keywords in ai_keywords.items():\n",
    "    sorted_keywords = sorted(keywords.items(), key=lambda x: x[1], reverse=True)[:top]\n",
    "    print(f\"Top {top} keywords for {period}:\")\n",
    "    for kw, count in sorted_keywords:\n",
    "        print(f\"  {kw}: {count}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ec418",
   "metadata": {},
   "source": [
    "### Display Top Keywords\n",
    "\n",
    "Print the most frequent keywords for each time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e3cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(14, 5), sharex=False)\n",
    "\n",
    "for ax, (period_name, start_year, end_year) in zip(axes, periods):\n",
    "    data = _top_n(ai_keywords.get(period_name, {}), 10)\n",
    "    terms = list(data.keys())\n",
    "    counts = list(data.values())\n",
    "\n",
    "    sns.barplot(x=counts, y=terms, ax=ax, palette=\"viridis\", hue=counts)\n",
    "    ax.set_title(f\"{period_name} ({start_year}â€“{end_year})\")\n",
    "    ax.set_xlabel(\"Frequency\")\n",
    "\n",
    "plt.suptitle(\"How 'Artificial Intelligence' appears across time in DPLA\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcefec4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "You've successfully:\n",
    "- âœ… Configured secure API authentication\n",
    "- âœ… Queried the DPLA API with complex filters and facets\n",
    "- âœ… Handled pagination to collect large datasets\n",
    "- âœ… Analyzed temporal patterns in cultural heritage data\n",
    "- âœ… Extracted and visualized keywords across different time periods\n",
    "\n",
    "These techniques are transferable to many other APIs. The patterns you've learnedâ€”building requests programmatically, handling responses, and combining API data with text analysisâ€”can be applied to museum collections, scientific datasets, social media archives, and more."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
